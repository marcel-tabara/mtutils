{
  "service_name": "databrew",
  "service_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/index.html",
  "service_commands": [
    {
      "command_name": "batch-delete-recipe-version",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/batch-delete-recipe-version.html",
      "command_description": "Description\n\nDeletes one or more versions of a recipe at a time.\n\nThe entire request will be rejected if:\n\nThe recipe does not exist.\n\nThere is an invalid version identifier in the list of versions.\n\nThe version list is empty.\n\nThe version list size exceeds 50.\n\nThe version list contains duplicate entries.\n\nThe request will complete successfully, but with partial failures, if:\n\nA version does not exist.\n\nA version is being used by a job.\n\nYou specify LATEST_WORKING , but it’s being used by a project.\n\nThe version fails to be deleted.\n\nThe LATEST_WORKING version will only be deleted if the recipe has no other versions. If you try to delete LATEST_WORKING while other versions exist (or if they can’t be deleted), then LATEST_WORKING will be listed as partial failure in the response.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  batch-delete-recipe-version\n--name <value>\n--recipe-versions <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "--recipe-versions <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the recipe whose versions are to be deleted.\n\n--recipe-versions (list)\n\nAn array of version identifiers, for the recipe versions to be deleted. You can specify numeric versions (X.Y ) or LATEST_WORKING . LATEST_PUBLISHED is not supported.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the recipe that was modified.\n\nErrors -> (list)\n\nErrors, if any, that occurred while attempting to delete the recipe versions.\n\n(structure)\n\nRepresents any errors encountered when attempting to delete multiple recipe versions.\n\nErrorCode -> (string)\n\nThe HTTP status code for the error.\n\nErrorMessage -> (string)\n\nThe text of the error message.\n\nRecipeVersion -> (string)\n\nThe identifier for the recipe version associated with this error."
    },
    {
      "command_name": "create-dataset",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/create-dataset.html",
      "command_description": "Description\n\nCreates a new DataBrew dataset.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  create-dataset\n--name <value>\n[--format <value>]\n[--format-options <value>]\n--input <value>\n[--path-options <value>]\n[--tags <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--format <value>]",
        "[--format-options <value>]",
        "--input <value>",
        "[--path-options <value>]",
        "[--tags <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the dataset to be created. Valid characters are alphanumeric (A-Z, a-z, 0-9), hyphen (-), period (.), and space.\n\n--format (string)\n\nThe file format of a dataset that is created from an Amazon S3 file or folder.\n\nPossible values:\n\nCSV\n\nJSON\n\nPARQUET\n\nEXCEL\n\n--format-options (structure)\n\nRepresents a set of options that define the structure of either comma-separated value (CSV), Excel, or JSON input.\n\nJson -> (structure)\n\nOptions that define how JSON input is to be interpreted by DataBrew.\n\nMultiLine -> (boolean)\n\nA value that specifies whether JSON input contains embedded new line characters.\n\nExcel -> (structure)\n\nOptions that define how Excel input is to be interpreted by DataBrew.\n\nSheetNames -> (list)\n\nOne or more named sheets in the Excel file that will be included in the dataset.\n\n(string)\n\nSheetIndexes -> (list)\n\nOne or more sheet numbers in the Excel file that will be included in the dataset.\n\n(integer)\n\nHeaderRow -> (boolean)\n\nA variable that specifies whether the first row in the file is parsed as the header. If this value is false, column names are auto-generated.\n\nCsv -> (structure)\n\nOptions that define how CSV input is to be interpreted by DataBrew.\n\nDelimiter -> (string)\n\nA single character that specifies the delimiter being used in the CSV file.\n\nHeaderRow -> (boolean)\n\nA variable that specifies whether the first row in the file is parsed as the header. If this value is false, column names are auto-generated.\n\nShorthand Syntax:\n\nJson={MultiLine=boolean},Excel={SheetNames=[string,string],SheetIndexes=[integer,integer],HeaderRow=boolean},Csv={Delimiter=string,HeaderRow=boolean}\n\n\nJSON Syntax:\n\n{\n  \"Json\": {\n    \"MultiLine\": true|false\n  },\n  \"Excel\": {\n    \"SheetNames\": [\"string\", ...],\n    \"SheetIndexes\": [integer, ...],\n    \"HeaderRow\": true|false\n  },\n  \"Csv\": {\n    \"Delimiter\": \"string\",\n    \"HeaderRow\": true|false\n  }\n}\n\n\n--input (structure)\n\nRepresents information on how DataBrew can find data, in either the Glue Data Catalog or Amazon S3.\n\nS3InputDefinition -> (structure)\n\nThe Amazon S3 location where the data is stored.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDataCatalogInputDefinition -> (structure)\n\nThe Glue Data Catalog parameters for the data.\n\nCatalogId -> (string)\n\nThe unique identifier of the Amazon Web Services account that holds the Data Catalog that stores the data.\n\nDatabaseName -> (string)\n\nThe name of a database in the Data Catalog.\n\nTableName -> (string)\n\nThe name of a database table in the Data Catalog. This table corresponds to a DataBrew dataset.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon location where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDatabaseInputDefinition -> (structure)\n\nConnection information for dataset input files stored in a database.\n\nGlueConnectionName -> (string)\n\nThe Glue Connection that stores the connection information for the target database.\n\nDatabaseTableName -> (string)\n\nThe table within the target database.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can read input data, or write output from a job.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nShorthand Syntax:\n\nS3InputDefinition={Bucket=string,Key=string},DataCatalogInputDefinition={CatalogId=string,DatabaseName=string,TableName=string,TempDirectory={Bucket=string,Key=string}},DatabaseInputDefinition={GlueConnectionName=string,DatabaseTableName=string,TempDirectory={Bucket=string,Key=string}}\n\n\nJSON Syntax:\n\n{\n  \"S3InputDefinition\": {\n    \"Bucket\": \"string\",\n    \"Key\": \"string\"\n  },\n  \"DataCatalogInputDefinition\": {\n    \"CatalogId\": \"string\",\n    \"DatabaseName\": \"string\",\n    \"TableName\": \"string\",\n    \"TempDirectory\": {\n      \"Bucket\": \"string\",\n      \"Key\": \"string\"\n    }\n  },\n  \"DatabaseInputDefinition\": {\n    \"GlueConnectionName\": \"string\",\n    \"DatabaseTableName\": \"string\",\n    \"TempDirectory\": {\n      \"Bucket\": \"string\",\n      \"Key\": \"string\"\n    }\n  }\n}\n\n\n--path-options (structure)\n\nA set of options that defines how DataBrew interprets an Amazon S3 path of the dataset.\n\nLastModifiedDateCondition -> (structure)\n\nIf provided, this structure defines a date range for matching Amazon S3 objects based on their LastModifiedDate attribute in Amazon S3.\n\nExpression -> (string)\n\nThe expression which includes condition names followed by substitution variables, possibly grouped and combined with other conditions. For example, “(starts_with :prefix1 or starts_with :prefix2) and (ends_with :suffix1 or ends_with :suffix2)”. Substitution variables should start with ‘:’ symbol.\n\nValuesMap -> (map)\n\nThe map of substitution variable names to their values used in this filter expression.\n\nkey -> (string)\n\nvalue -> (string)\n\nFilesLimit -> (structure)\n\nIf provided, this structure imposes a limit on a number of files that should be selected.\n\nMaxFiles -> (integer)\n\nThe number of Amazon S3 files to select.\n\nOrderedBy -> (string)\n\nA criteria to use for Amazon S3 files sorting before their selection. By default uses LAST_MODIFIED_DATE as a sorting criteria. Currently it’s the only allowed value.\n\nOrder -> (string)\n\nA criteria to use for Amazon S3 files sorting before their selection. By default uses DESCENDING order, i.e. most recent files are selected first. Anotherpossible value is ASCENDING.\n\nParameters -> (map)\n\nA structure that maps names of parameters used in the Amazon S3 path of a dataset to their definitions.\n\nkey -> (string)\n\nvalue -> (structure)\n\nRepresents a dataset paramater that defines type and conditions for a parameter in the Amazon S3 path of the dataset.\n\nName -> (string)\n\nThe name of the parameter that is used in the dataset’s Amazon S3 path.\n\nType -> (string)\n\nThe type of the dataset parameter, can be one of a ‘String’, ‘Number’ or ‘Datetime’.\n\nDatetimeOptions -> (structure)\n\nAdditional parameter options such as a format and a timezone. Required for datetime parameters.\n\nFormat -> (string)\n\nRequired option, that defines the datetime format used for a date parameter in the Amazon S3 path. Should use only supported datetime specifiers and separation characters, all literal a-z or A-Z characters should be escaped with single quotes. E.g. “MM.dd.yyyy-‘at’-HH:mm”.\n\nTimezoneOffset -> (string)\n\nOptional value for a timezone offset of the datetime parameter value in the Amazon S3 path. Shouldn’t be used if Format for this parameter includes timezone fields. If no offset specified, UTC is assumed.\n\nLocaleCode -> (string)\n\nOptional value for a non-US locale code, needed for correct interpretation of some date formats.\n\nCreateColumn -> (boolean)\n\nOptional boolean value that defines whether the captured value of this parameter should be used to create a new column in a dataset.\n\nFilter -> (structure)\n\nThe optional filter expression structure to apply additional matching criteria to the parameter.\n\nExpression -> (string)\n\nThe expression which includes condition names followed by substitution variables, possibly grouped and combined with other conditions. For example, “(starts_with :prefix1 or starts_with :prefix2) and (ends_with :suffix1 or ends_with :suffix2)”. Substitution variables should start with ‘:’ symbol.\n\nValuesMap -> (map)\n\nThe map of substitution variable names to their values used in this filter expression.\n\nkey -> (string)\n\nvalue -> (string)\n\nShorthand Syntax:\n\nLastModifiedDateCondition={Expression=string,ValuesMap={KeyName1=string,KeyName2=string}},FilesLimit={MaxFiles=integer,OrderedBy=string,Order=string},Parameters={KeyName1={Name=string,Type=string,DatetimeOptions={Format=string,TimezoneOffset=string,LocaleCode=string},CreateColumn=boolean,Filter={Expression=string,ValuesMap={KeyName1=string,KeyName2=string}}},KeyName2={Name=string,Type=string,DatetimeOptions={Format=string,TimezoneOffset=string,LocaleCode=string},CreateColumn=boolean,Filter={Expression=string,ValuesMap={KeyName1=string,KeyName2=string}}}}\n\n\nJSON Syntax:\n\n{\n  \"LastModifiedDateCondition\": {\n    \"Expression\": \"string\",\n    \"ValuesMap\": {\"string\": \"string\"\n      ...}\n  },\n  \"FilesLimit\": {\n    \"MaxFiles\": integer,\n    \"OrderedBy\": \"LAST_MODIFIED_DATE\",\n    \"Order\": \"DESCENDING\"|\"ASCENDING\"\n  },\n  \"Parameters\": {\"string\": {\n        \"Name\": \"string\",\n        \"Type\": \"Datetime\"|\"Number\"|\"String\",\n        \"DatetimeOptions\": {\n          \"Format\": \"string\",\n          \"TimezoneOffset\": \"string\",\n          \"LocaleCode\": \"string\"\n        },\n        \"CreateColumn\": true|false,\n        \"Filter\": {\n          \"Expression\": \"string\",\n          \"ValuesMap\": {\"string\": \"string\"\n            ...}\n        }\n      }\n    ...}\n}\n\n\n--tags (map)\n\nMetadata tags to apply to this dataset.\n\nkey -> (string)\n\nvalue -> (string)\n\nShorthand Syntax:\n\nKeyName1=string,KeyName2=string\n\n\nJSON Syntax:\n\n{\"string\": \"string\"\n  ...}\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the dataset that you created."
    },
    {
      "command_name": "create-profile-job",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/create-profile-job.html",
      "command_description": "Description\n\nCreates a new job to analyze a dataset and create its data profile.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  create-profile-job\n--dataset-name <value>\n[--encryption-key-arn <value>]\n[--encryption-mode <value>]\n--name <value>\n[--log-subscription <value>]\n[--max-capacity <value>]\n[--max-retries <value>]\n--output-location <value>\n[--configuration <value>]\n--role-arn <value>\n[--tags <value>]\n[--timeout <value>]\n[--job-sample <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--dataset-name <value>",
        "[--encryption-key-arn <value>]",
        "[--encryption-mode <value>]",
        "--name <value>",
        "[--log-subscription <value>]",
        "[--max-capacity <value>]",
        "[--max-retries <value>]",
        "--output-location <value>",
        "[--configuration <value>]",
        "--role-arn <value>",
        "[--tags <value>]",
        "[--timeout <value>]",
        "[--job-sample <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--dataset-name (string)\n\nThe name of the dataset that this job is to act upon.\n\n--encryption-key-arn (string)\n\nThe Amazon Resource Name (ARN) of an encryption key that is used to protect the job.\n\n--encryption-mode (string)\n\nThe encryption mode for the job, which can be one of the following:\n\nSSE-KMS - SSE-KMS - Server-side encryption with KMS-managed keys.\n\nSSE-S3 - Server-side encryption with keys managed by Amazon S3.\n\nPossible values:\n\nSSE-KMS\n\nSSE-S3\n\n--name (string)\n\nThe name of the job to be created. Valid characters are alphanumeric (A-Z, a-z, 0-9), hyphen (-), period (.), and space.\n\n--log-subscription (string)\n\nEnables or disables Amazon CloudWatch logging for the job. If logging is enabled, CloudWatch writes one log stream for each job run.\n\nPossible values:\n\nENABLE\n\nDISABLE\n\n--max-capacity (integer)\n\nThe maximum number of nodes that DataBrew can use when the job processes data.\n\n--max-retries (integer)\n\nThe maximum number of times to retry the job after a job run fails.\n\n--output-location (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can read input data, or write output from a job.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nShorthand Syntax:\n\nBucket=string,Key=string\n\n\nJSON Syntax:\n\n{\n  \"Bucket\": \"string\",\n  \"Key\": \"string\"\n}\n\n\n--configuration (structure)\n\nConfiguration for profile jobs. Used to select columns, do evaluations, and override default parameters of evaluations. When configuration is null, the profile job will run with default settings.\n\nDatasetStatisticsConfiguration -> (structure)\n\nConfiguration for inter-column evaluations. Configuration can be used to select evaluations and override parameters of evaluations. When configuration is undefined, the profile job will run all supported inter-column evaluations.\n\nIncludedStatistics -> (list)\n\nList of included evaluations. When the list is undefined, all supported evaluations will be included.\n\n(string)\n\nOverrides -> (list)\n\nList of overrides for evaluations.\n\n(structure)\n\nOverride of a particular evaluation for a profile job.\n\nStatistic -> (string)\n\nThe name of an evaluation\n\nParameters -> (map)\n\nA map that includes overrides of an evaluation’s parameters.\n\nkey -> (string)\n\nvalue -> (string)\n\nProfileColumns -> (list)\n\nList of column selectors. ProfileColumns can be used to select columns from the dataset. When ProfileColumns is undefined, the profile job will profile all supported columns.\n\n(structure)\n\nSelector of a column from a dataset for profile job configuration. One selector includes either a column name or a regular expression.\n\nRegex -> (string)\n\nA regular expression for selecting a column from a dataset.\n\nName -> (string)\n\nThe name of a column from a dataset.\n\nColumnStatisticsConfigurations -> (list)\n\nList of configurations for column evaluations. ColumnStatisticsConfigurations are used to select evaluations and override parameters of evaluations for particular columns. When ColumnStatisticsConfigurations is undefined, the profile job will profile all supported columns and run all supported evaluations.\n\n(structure)\n\nConfiguration for column evaluations for a profile job. ColumnStatisticsConfiguration can be used to select evaluations and override parameters of evaluations for particular columns.\n\nSelectors -> (list)\n\nList of column selectors. Selectors can be used to select columns from the dataset. When selectors are undefined, configuration will be applied to all supported columns.\n\n(structure)\n\nSelector of a column from a dataset for profile job configuration. One selector includes either a column name or a regular expression.\n\nRegex -> (string)\n\nA regular expression for selecting a column from a dataset.\n\nName -> (string)\n\nThe name of a column from a dataset.\n\nStatistics -> (structure)\n\nConfiguration for evaluations. Statistics can be used to select evaluations and override parameters of evaluations.\n\nIncludedStatistics -> (list)\n\nList of included evaluations. When the list is undefined, all supported evaluations will be included.\n\n(string)\n\nOverrides -> (list)\n\nList of overrides for evaluations.\n\n(structure)\n\nOverride of a particular evaluation for a profile job.\n\nStatistic -> (string)\n\nThe name of an evaluation\n\nParameters -> (map)\n\nA map that includes overrides of an evaluation’s parameters.\n\nkey -> (string)\n\nvalue -> (string)\n\nJSON Syntax:\n\n{\n  \"DatasetStatisticsConfiguration\": {\n    \"IncludedStatistics\": [\"string\", ...],\n    \"Overrides\": [\n      {\n        \"Statistic\": \"string\",\n        \"Parameters\": {\"string\": \"string\"\n          ...}\n      }\n      ...\n    ]\n  },\n  \"ProfileColumns\": [\n    {\n      \"Regex\": \"string\",\n      \"Name\": \"string\"\n    }\n    ...\n  ],\n  \"ColumnStatisticsConfigurations\": [\n    {\n      \"Selectors\": [\n        {\n          \"Regex\": \"string\",\n          \"Name\": \"string\"\n        }\n        ...\n      ],\n      \"Statistics\": {\n        \"IncludedStatistics\": [\"string\", ...],\n        \"Overrides\": [\n          {\n            \"Statistic\": \"string\",\n            \"Parameters\": {\"string\": \"string\"\n              ...}\n          }\n          ...\n        ]\n      }\n    }\n    ...\n  ]\n}\n\n\n--role-arn (string)\n\nThe Amazon Resource Name (ARN) of the Identity and Access Management (IAM) role to be assumed when DataBrew runs the job.\n\n--tags (map)\n\nMetadata tags to apply to this job.\n\nkey -> (string)\n\nvalue -> (string)\n\nShorthand Syntax:\n\nKeyName1=string,KeyName2=string\n\n\nJSON Syntax:\n\n{\"string\": \"string\"\n  ...}\n\n\n--timeout (integer)\n\nThe job’s timeout in minutes. A job that attempts to run longer than this timeout period ends with a status of TIMEOUT .\n\n--job-sample (structure)\n\nSample configuration for profile jobs only. Determines the number of rows on which the profile job will be executed. If a JobSample value is not provided, the default value will be used. The default value is CUSTOM_ROWS for the mode parameter and 20000 for the size parameter.\n\nMode -> (string)\n\nA value that determines whether the profile job is run on the entire dataset or a specified number of rows. This value must be one of the following:\n\nFULL_DATASET - The profile job is run on the entire dataset.\n\nCUSTOM_ROWS - The profile job is run on the number of rows specified in the Size parameter.\n\nSize -> (long)\n\nThe Size parameter is only required when the mode is CUSTOM_ROWS. The profile job is run on the specified number of rows. The maximum value for size is Long.MAX_VALUE.\n\nLong.MAX_VALUE = 9223372036854775807\n\nShorthand Syntax:\n\nMode=string,Size=long\n\n\nJSON Syntax:\n\n{\n  \"Mode\": \"FULL_DATASET\"|\"CUSTOM_ROWS\",\n  \"Size\": long\n}\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the job that was created."
    },
    {
      "command_name": "create-project",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/create-project.html",
      "command_description": "Description\n\nCreates a new DataBrew project.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  create-project\n--dataset-name <value>\n--name <value>\n--recipe-name <value>\n[--sample <value>]\n--role-arn <value>\n[--tags <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--dataset-name <value>",
        "--name <value>",
        "--recipe-name <value>",
        "[--sample <value>]",
        "--role-arn <value>",
        "[--tags <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--dataset-name (string)\n\nThe name of an existing dataset to associate this project with.\n\n--name (string)\n\nA unique name for the new project. Valid characters are alphanumeric (A-Z, a-z, 0-9), hyphen (-), period (.), and space.\n\n--recipe-name (string)\n\nThe name of an existing recipe to associate with the project.\n\n--sample (structure)\n\nRepresents the sample size and sampling type for DataBrew to use for interactive data analysis.\n\nSize -> (integer)\n\nThe number of rows in the sample.\n\nType -> (string)\n\nThe way in which DataBrew obtains rows from a dataset.\n\nShorthand Syntax:\n\nSize=integer,Type=string\n\n\nJSON Syntax:\n\n{\n  \"Size\": integer,\n  \"Type\": \"FIRST_N\"|\"LAST_N\"|\"RANDOM\"\n}\n\n\n--role-arn (string)\n\nThe Amazon Resource Name (ARN) of the Identity and Access Management (IAM) role to be assumed for this request.\n\n--tags (map)\n\nMetadata tags to apply to this project.\n\nkey -> (string)\n\nvalue -> (string)\n\nShorthand Syntax:\n\nKeyName1=string,KeyName2=string\n\n\nJSON Syntax:\n\n{\"string\": \"string\"\n  ...}\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the project that you created."
    },
    {
      "command_name": "create-recipe",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/create-recipe.html",
      "command_description": "Description\n\nCreates a new DataBrew recipe.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  create-recipe\n[--description <value>]\n--name <value>\n--steps <value>\n[--tags <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--description <value>]",
        "--name <value>",
        "--steps <value>",
        "[--tags <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--description (string)\n\nA description for the recipe.\n\n--name (string)\n\nA unique name for the recipe. Valid characters are alphanumeric (A-Z, a-z, 0-9), hyphen (-), period (.), and space.\n\n--steps (list)\n\nAn array containing the steps to be performed by the recipe. Each recipe step consists of one recipe action and (optionally) an array of condition expressions.\n\n(structure)\n\nRepresents a single step from a DataBrew recipe to be performed.\n\nAction -> (structure)\n\nThe particular action to be performed in the recipe step.\n\nOperation -> (string)\n\nThe name of a valid DataBrew transformation to be performed on the data.\n\nParameters -> (map)\n\nContextual parameters for the transformation.\n\nkey -> (string)\n\nvalue -> (string)\n\nConditionExpressions -> (list)\n\nOne or more conditions that must be met for the recipe step to succeed.\n\nNote\n\nAll of the conditions in the array must be met. In other words, all of the conditions must be combined using a logical AND operation.\n\n(structure)\n\nRepresents an individual condition that evaluates to true or false.\n\nConditions are used with recipe actions. The action is only performed for column values where the condition evaluates to true.\n\nIf a recipe requires more than one condition, then the recipe must specify multiple ConditionExpression elements. Each condition is applied to the rows in a dataset first, before the recipe action is performed.\n\nCondition -> (string)\n\nA specific condition to apply to a recipe action. For more information, see Recipe structure in the Glue DataBrew Developer Guide .\n\nValue -> (string)\n\nA value that the condition must evaluate to for the condition to succeed.\n\nTargetColumn -> (string)\n\nA column to apply this condition to.\n\nShorthand Syntax:\n\nAction={Operation=string,Parameters={KeyName1=string,KeyName2=string}},ConditionExpressions=[{Condition=string,Value=string,TargetColumn=string},{Condition=string,Value=string,TargetColumn=string}] ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Action\": {\n      \"Operation\": \"string\",\n      \"Parameters\": {\"string\": \"string\"\n        ...}\n    },\n    \"ConditionExpressions\": [\n      {\n        \"Condition\": \"string\",\n        \"Value\": \"string\",\n        \"TargetColumn\": \"string\"\n      }\n      ...\n    ]\n  }\n  ...\n]\n\n\n--tags (map)\n\nMetadata tags to apply to this recipe.\n\nkey -> (string)\n\nvalue -> (string)\n\nShorthand Syntax:\n\nKeyName1=string,KeyName2=string\n\n\nJSON Syntax:\n\n{\"string\": \"string\"\n  ...}\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the recipe that you created."
    },
    {
      "command_name": "create-recipe-job",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/create-recipe-job.html",
      "command_description": "Description\n\nCreates a new job to transform input data, using steps defined in an existing Glue DataBrew recipe\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  create-recipe-job\n[--dataset-name <value>]\n[--encryption-key-arn <value>]\n[--encryption-mode <value>]\n--name <value>\n[--log-subscription <value>]\n[--max-capacity <value>]\n[--max-retries <value>]\n[--outputs <value>]\n[--data-catalog-outputs <value>]\n[--database-outputs <value>]\n[--project-name <value>]\n[--recipe-reference <value>]\n--role-arn <value>\n[--tags <value>]\n[--timeout <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--dataset-name <value>]",
        "[--encryption-key-arn <value>]",
        "[--encryption-mode <value>]",
        "--name <value>",
        "[--log-subscription <value>]",
        "[--max-capacity <value>]",
        "[--max-retries <value>]",
        "[--outputs <value>]",
        "[--data-catalog-outputs <value>]",
        "[--database-outputs <value>]",
        "[--project-name <value>]",
        "[--recipe-reference <value>]",
        "--role-arn <value>",
        "[--tags <value>]",
        "[--timeout <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--dataset-name (string)\n\nThe name of the dataset that this job processes.\n\n--encryption-key-arn (string)\n\nThe Amazon Resource Name (ARN) of an encryption key that is used to protect the job.\n\n--encryption-mode (string)\n\nThe encryption mode for the job, which can be one of the following:\n\nSSE-KMS - Server-side encryption with keys managed by KMS.\n\nSSE-S3 - Server-side encryption with keys managed by Amazon S3.\n\nPossible values:\n\nSSE-KMS\n\nSSE-S3\n\n--name (string)\n\nA unique name for the job. Valid characters are alphanumeric (A-Z, a-z, 0-9), hyphen (-), period (.), and space.\n\n--log-subscription (string)\n\nEnables or disables Amazon CloudWatch logging for the job. If logging is enabled, CloudWatch writes one log stream for each job run.\n\nPossible values:\n\nENABLE\n\nDISABLE\n\n--max-capacity (integer)\n\nThe maximum number of nodes that DataBrew can consume when the job processes data.\n\n--max-retries (integer)\n\nThe maximum number of times to retry the job after a job run fails.\n\n--outputs (list)\n\nOne or more artifacts that represent the output from running the job.\n\n(structure)\n\nRepresents options that specify how and where in Amazon S3 DataBrew writes the output generated by recipe jobs or profile jobs.\n\nCompressionFormat -> (string)\n\nThe compression algorithm used to compress the output text of the job.\n\nFormat -> (string)\n\nThe data format of the output of the job.\n\nPartitionColumns -> (list)\n\nThe names of one or more partition columns for the output of the job.\n\n(string)\n\nLocation -> (structure)\n\nThe location in Amazon S3 where the job writes its output.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nOverwrite -> (boolean)\n\nA value that, if true, means that any data in the location specified for output is overwritten with new output.\n\nFormatOptions -> (structure)\n\nRepresents options that define how DataBrew formats job output files.\n\nCsv -> (structure)\n\nRepresents a set of options that define the structure of comma-separated value (CSV) job output.\n\nDelimiter -> (string)\n\nA single character that specifies the delimiter used to create CSV job output.\n\nShorthand Syntax:\n\nCompressionFormat=string,Format=string,PartitionColumns=string,string,Location={Bucket=string,Key=string},Overwrite=boolean,FormatOptions={Csv={Delimiter=string}} ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"CompressionFormat\": \"GZIP\"|\"LZ4\"|\"SNAPPY\"|\"BZIP2\"|\"DEFLATE\"|\"LZO\"|\"BROTLI\"|\"ZSTD\"|\"ZLIB\",\n    \"Format\": \"CSV\"|\"JSON\"|\"PARQUET\"|\"GLUEPARQUET\"|\"AVRO\"|\"ORC\"|\"XML\"|\"TABLEAUHYPER\",\n    \"PartitionColumns\": [\"string\", ...],\n    \"Location\": {\n      \"Bucket\": \"string\",\n      \"Key\": \"string\"\n    },\n    \"Overwrite\": true|false,\n    \"FormatOptions\": {\n      \"Csv\": {\n        \"Delimiter\": \"string\"\n      }\n    }\n  }\n  ...\n]\n\n\n--data-catalog-outputs (list)\n\nOne or more artifacts that represent the Glue Data Catalog output from running the job.\n\n(structure)\n\nRepresents options that specify how and where in the Glue Data Catalog DataBrew writes the output generated by recipe jobs.\n\nCatalogId -> (string)\n\nThe unique identifier of the Amazon Web Services account that holds the Data Catalog that stores the data.\n\nDatabaseName -> (string)\n\nThe name of a database in the Data Catalog.\n\nTableName -> (string)\n\nThe name of a table in the Data Catalog.\n\nS3Options -> (structure)\n\nRepresents options that specify how and where DataBrew writes the Amazon S3 output generated by recipe jobs.\n\nLocation -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can write output from a job.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDatabaseOptions -> (structure)\n\nRepresents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nTableName -> (string)\n\nA prefix for the name of a table DataBrew will create in the database.\n\nOverwrite -> (boolean)\n\nA value that, if true, means that any data in the location specified for output is overwritten with new output. Not supported with DatabaseOptions.\n\nShorthand Syntax:\n\nCatalogId=string,DatabaseName=string,TableName=string,S3Options={Location={Bucket=string,Key=string}},DatabaseOptions={TempDirectory={Bucket=string,Key=string},TableName=string},Overwrite=boolean ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"CatalogId\": \"string\",\n    \"DatabaseName\": \"string\",\n    \"TableName\": \"string\",\n    \"S3Options\": {\n      \"Location\": {\n        \"Bucket\": \"string\",\n        \"Key\": \"string\"\n      }\n    },\n    \"DatabaseOptions\": {\n      \"TempDirectory\": {\n        \"Bucket\": \"string\",\n        \"Key\": \"string\"\n      },\n      \"TableName\": \"string\"\n    },\n    \"Overwrite\": true|false\n  }\n  ...\n]\n\n\n--database-outputs (list)\n\nRepresents a list of JDBC database output objects which defines the output destination for a DataBrew recipe job to write to.\n\n(structure)\n\nRepresents a JDBC database output object which defines the output destination for a DataBrew recipe job to write into.\n\nGlueConnectionName -> (string)\n\nThe Glue connection that stores the connection information for the target database.\n\nDatabaseOptions -> (structure)\n\nRepresents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nTableName -> (string)\n\nA prefix for the name of a table DataBrew will create in the database.\n\nDatabaseOutputMode -> (string)\n\nThe output mode to write into the database. Currently supported option: NEW_TABLE.\n\nShorthand Syntax:\n\nGlueConnectionName=string,DatabaseOptions={TempDirectory={Bucket=string,Key=string},TableName=string},DatabaseOutputMode=string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"GlueConnectionName\": \"string\",\n    \"DatabaseOptions\": {\n      \"TempDirectory\": {\n        \"Bucket\": \"string\",\n        \"Key\": \"string\"\n      },\n      \"TableName\": \"string\"\n    },\n    \"DatabaseOutputMode\": \"NEW_TABLE\"\n  }\n  ...\n]\n\n\n--project-name (string)\n\nEither the name of an existing project, or a combination of a recipe and a dataset to associate with the recipe.\n\n--recipe-reference (structure)\n\nRepresents the name and version of a DataBrew recipe.\n\nName -> (string)\n\nThe name of the recipe.\n\nRecipeVersion -> (string)\n\nThe identifier for the version for the recipe.\n\nShorthand Syntax:\n\nName=string,RecipeVersion=string\n\n\nJSON Syntax:\n\n{\n  \"Name\": \"string\",\n  \"RecipeVersion\": \"string\"\n}\n\n\n--role-arn (string)\n\nThe Amazon Resource Name (ARN) of the Identity and Access Management (IAM) role to be assumed when DataBrew runs the job.\n\n--tags (map)\n\nMetadata tags to apply to this job.\n\nkey -> (string)\n\nvalue -> (string)\n\nShorthand Syntax:\n\nKeyName1=string,KeyName2=string\n\n\nJSON Syntax:\n\n{\"string\": \"string\"\n  ...}\n\n\n--timeout (integer)\n\nThe job’s timeout in minutes. A job that attempts to run longer than this timeout period ends with a status of TIMEOUT .\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the job that you created."
    },
    {
      "command_name": "create-schedule",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/create-schedule.html",
      "command_description": "Description\n\nCreates a new schedule for one or more DataBrew jobs. Jobs can be run at a specific date and time, or at regular intervals.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  create-schedule\n[--job-names <value>]\n--cron-expression <value>\n[--tags <value>]\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--job-names <value>]",
        "--cron-expression <value>",
        "[--tags <value>]",
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--job-names (list)\n\nThe name or names of one or more jobs to be run.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--cron-expression (string)\n\nThe date or dates and time or times when the jobs are to be run. For more information, see Cron expressions in the Glue DataBrew Developer Guide .\n\n--tags (map)\n\nMetadata tags to apply to this schedule.\n\nkey -> (string)\n\nvalue -> (string)\n\nShorthand Syntax:\n\nKeyName1=string,KeyName2=string\n\n\nJSON Syntax:\n\n{\"string\": \"string\"\n  ...}\n\n\n--name (string)\n\nA unique name for the schedule. Valid characters are alphanumeric (A-Z, a-z, 0-9), hyphen (-), period (.), and space.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the schedule that was created."
    },
    {
      "command_name": "delete-dataset",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/delete-dataset.html",
      "command_description": "Description\n\nDeletes a dataset from DataBrew.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-dataset\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the dataset to be deleted.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the dataset that you deleted."
    },
    {
      "command_name": "delete-job",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/delete-job.html",
      "command_description": "Description\n\nDeletes the specified DataBrew job.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-job\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the job to be deleted.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the job that you deleted."
    },
    {
      "command_name": "delete-project",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/delete-project.html",
      "command_description": "Description\n\nDeletes an existing DataBrew project.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-project\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the project to be deleted.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the project that you deleted."
    },
    {
      "command_name": "delete-recipe-version",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/delete-recipe-version.html",
      "command_description": "Description\n\nDeletes a single version of a DataBrew recipe.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-recipe-version\n--name <value>\n--recipe-version <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "--recipe-version <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the recipe.\n\n--recipe-version (string)\n\nThe version of the recipe to be deleted. You can specify a numeric versions (X.Y ) or LATEST_WORKING . LATEST_PUBLISHED is not supported.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the recipe that was deleted.\n\nRecipeVersion -> (string)\n\nThe version of the recipe that was deleted."
    },
    {
      "command_name": "delete-schedule",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/delete-schedule.html",
      "command_description": "Description\n\nDeletes the specified DataBrew schedule.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-schedule\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the schedule to be deleted.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the schedule that was deleted."
    },
    {
      "command_name": "describe-dataset",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/describe-dataset.html",
      "command_description": "Description\n\nReturns the definition of a specific DataBrew dataset.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-dataset\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the dataset to be described.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nCreatedBy -> (string)\n\nThe identifier (user name) of the user who created the dataset.\n\nCreateDate -> (timestamp)\n\nThe date and time that the dataset was created.\n\nName -> (string)\n\nThe name of the dataset.\n\nFormat -> (string)\n\nThe file format of a dataset that is created from an Amazon S3 file or folder.\n\nFormatOptions -> (structure)\n\nRepresents a set of options that define the structure of either comma-separated value (CSV), Excel, or JSON input.\n\nJson -> (structure)\n\nOptions that define how JSON input is to be interpreted by DataBrew.\n\nMultiLine -> (boolean)\n\nA value that specifies whether JSON input contains embedded new line characters.\n\nExcel -> (structure)\n\nOptions that define how Excel input is to be interpreted by DataBrew.\n\nSheetNames -> (list)\n\nOne or more named sheets in the Excel file that will be included in the dataset.\n\n(string)\n\nSheetIndexes -> (list)\n\nOne or more sheet numbers in the Excel file that will be included in the dataset.\n\n(integer)\n\nHeaderRow -> (boolean)\n\nA variable that specifies whether the first row in the file is parsed as the header. If this value is false, column names are auto-generated.\n\nCsv -> (structure)\n\nOptions that define how CSV input is to be interpreted by DataBrew.\n\nDelimiter -> (string)\n\nA single character that specifies the delimiter being used in the CSV file.\n\nHeaderRow -> (boolean)\n\nA variable that specifies whether the first row in the file is parsed as the header. If this value is false, column names are auto-generated.\n\nInput -> (structure)\n\nRepresents information on how DataBrew can find data, in either the Glue Data Catalog or Amazon S3.\n\nS3InputDefinition -> (structure)\n\nThe Amazon S3 location where the data is stored.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDataCatalogInputDefinition -> (structure)\n\nThe Glue Data Catalog parameters for the data.\n\nCatalogId -> (string)\n\nThe unique identifier of the Amazon Web Services account that holds the Data Catalog that stores the data.\n\nDatabaseName -> (string)\n\nThe name of a database in the Data Catalog.\n\nTableName -> (string)\n\nThe name of a database table in the Data Catalog. This table corresponds to a DataBrew dataset.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon location where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDatabaseInputDefinition -> (structure)\n\nConnection information for dataset input files stored in a database.\n\nGlueConnectionName -> (string)\n\nThe Glue Connection that stores the connection information for the target database.\n\nDatabaseTableName -> (string)\n\nThe table within the target database.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can read input data, or write output from a job.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nLastModifiedDate -> (timestamp)\n\nThe date and time that the dataset was last modified.\n\nLastModifiedBy -> (string)\n\nThe identifier (user name) of the user who last modified the dataset.\n\nSource -> (string)\n\nThe location of the data for this dataset, Amazon S3 or the Glue Data Catalog.\n\nPathOptions -> (structure)\n\nA set of options that defines how DataBrew interprets an Amazon S3 path of the dataset.\n\nLastModifiedDateCondition -> (structure)\n\nIf provided, this structure defines a date range for matching Amazon S3 objects based on their LastModifiedDate attribute in Amazon S3.\n\nExpression -> (string)\n\nThe expression which includes condition names followed by substitution variables, possibly grouped and combined with other conditions. For example, “(starts_with :prefix1 or starts_with :prefix2) and (ends_with :suffix1 or ends_with :suffix2)”. Substitution variables should start with ‘:’ symbol.\n\nValuesMap -> (map)\n\nThe map of substitution variable names to their values used in this filter expression.\n\nkey -> (string)\n\nvalue -> (string)\n\nFilesLimit -> (structure)\n\nIf provided, this structure imposes a limit on a number of files that should be selected.\n\nMaxFiles -> (integer)\n\nThe number of Amazon S3 files to select.\n\nOrderedBy -> (string)\n\nA criteria to use for Amazon S3 files sorting before their selection. By default uses LAST_MODIFIED_DATE as a sorting criteria. Currently it’s the only allowed value.\n\nOrder -> (string)\n\nA criteria to use for Amazon S3 files sorting before their selection. By default uses DESCENDING order, i.e. most recent files are selected first. Anotherpossible value is ASCENDING.\n\nParameters -> (map)\n\nA structure that maps names of parameters used in the Amazon S3 path of a dataset to their definitions.\n\nkey -> (string)\n\nvalue -> (structure)\n\nRepresents a dataset paramater that defines type and conditions for a parameter in the Amazon S3 path of the dataset.\n\nName -> (string)\n\nThe name of the parameter that is used in the dataset’s Amazon S3 path.\n\nType -> (string)\n\nThe type of the dataset parameter, can be one of a ‘String’, ‘Number’ or ‘Datetime’.\n\nDatetimeOptions -> (structure)\n\nAdditional parameter options such as a format and a timezone. Required for datetime parameters.\n\nFormat -> (string)\n\nRequired option, that defines the datetime format used for a date parameter in the Amazon S3 path. Should use only supported datetime specifiers and separation characters, all literal a-z or A-Z characters should be escaped with single quotes. E.g. “MM.dd.yyyy-‘at’-HH:mm”.\n\nTimezoneOffset -> (string)\n\nOptional value for a timezone offset of the datetime parameter value in the Amazon S3 path. Shouldn’t be used if Format for this parameter includes timezone fields. If no offset specified, UTC is assumed.\n\nLocaleCode -> (string)\n\nOptional value for a non-US locale code, needed for correct interpretation of some date formats.\n\nCreateColumn -> (boolean)\n\nOptional boolean value that defines whether the captured value of this parameter should be used to create a new column in a dataset.\n\nFilter -> (structure)\n\nThe optional filter expression structure to apply additional matching criteria to the parameter.\n\nExpression -> (string)\n\nThe expression which includes condition names followed by substitution variables, possibly grouped and combined with other conditions. For example, “(starts_with :prefix1 or starts_with :prefix2) and (ends_with :suffix1 or ends_with :suffix2)”. Substitution variables should start with ‘:’ symbol.\n\nValuesMap -> (map)\n\nThe map of substitution variable names to their values used in this filter expression.\n\nkey -> (string)\n\nvalue -> (string)\n\nTags -> (map)\n\nMetadata tags associated with this dataset.\n\nkey -> (string)\n\nvalue -> (string)\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the dataset."
    },
    {
      "command_name": "describe-job",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/describe-job.html",
      "command_description": "Description\n\nReturns the definition of a specific DataBrew job.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-job\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the job to be described.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nCreateDate -> (timestamp)\n\nThe date and time that the job was created.\n\nCreatedBy -> (string)\n\nThe identifier (user name) of the user associated with the creation of the job.\n\nDatasetName -> (string)\n\nThe dataset that the job acts upon.\n\nEncryptionKeyArn -> (string)\n\nThe Amazon Resource Name (ARN) of an encryption key that is used to protect the job.\n\nEncryptionMode -> (string)\n\nThe encryption mode for the job, which can be one of the following:\n\nSSE-KMS - Server-side encryption with keys managed by KMS.\n\nSSE-S3 - Server-side encryption with keys managed by Amazon S3.\n\nName -> (string)\n\nThe name of the job.\n\nType -> (string)\n\nThe job type, which must be one of the following:\n\nPROFILE - The job analyzes the dataset to determine its size, data types, data distribution, and more.\n\nRECIPE - The job applies one or more transformations to a dataset.\n\nLastModifiedBy -> (string)\n\nThe identifier (user name) of the user who last modified the job.\n\nLastModifiedDate -> (timestamp)\n\nThe date and time that the job was last modified.\n\nLogSubscription -> (string)\n\nIndicates whether Amazon CloudWatch logging is enabled for this job.\n\nMaxCapacity -> (integer)\n\nThe maximum number of compute nodes that DataBrew can consume when the job processes data.\n\nMaxRetries -> (integer)\n\nThe maximum number of times to retry the job after a job run fails.\n\nOutputs -> (list)\n\nOne or more artifacts that represent the output from running the job.\n\n(structure)\n\nRepresents options that specify how and where in Amazon S3 DataBrew writes the output generated by recipe jobs or profile jobs.\n\nCompressionFormat -> (string)\n\nThe compression algorithm used to compress the output text of the job.\n\nFormat -> (string)\n\nThe data format of the output of the job.\n\nPartitionColumns -> (list)\n\nThe names of one or more partition columns for the output of the job.\n\n(string)\n\nLocation -> (structure)\n\nThe location in Amazon S3 where the job writes its output.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nOverwrite -> (boolean)\n\nA value that, if true, means that any data in the location specified for output is overwritten with new output.\n\nFormatOptions -> (structure)\n\nRepresents options that define how DataBrew formats job output files.\n\nCsv -> (structure)\n\nRepresents a set of options that define the structure of comma-separated value (CSV) job output.\n\nDelimiter -> (string)\n\nA single character that specifies the delimiter used to create CSV job output.\n\nDataCatalogOutputs -> (list)\n\nOne or more artifacts that represent the Glue Data Catalog output from running the job.\n\n(structure)\n\nRepresents options that specify how and where in the Glue Data Catalog DataBrew writes the output generated by recipe jobs.\n\nCatalogId -> (string)\n\nThe unique identifier of the Amazon Web Services account that holds the Data Catalog that stores the data.\n\nDatabaseName -> (string)\n\nThe name of a database in the Data Catalog.\n\nTableName -> (string)\n\nThe name of a table in the Data Catalog.\n\nS3Options -> (structure)\n\nRepresents options that specify how and where DataBrew writes the Amazon S3 output generated by recipe jobs.\n\nLocation -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can write output from a job.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDatabaseOptions -> (structure)\n\nRepresents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nTableName -> (string)\n\nA prefix for the name of a table DataBrew will create in the database.\n\nOverwrite -> (boolean)\n\nA value that, if true, means that any data in the location specified for output is overwritten with new output. Not supported with DatabaseOptions.\n\nDatabaseOutputs -> (list)\n\nRepresents a list of JDBC database output objects which defines the output destination for a DataBrew recipe job to write into.\n\n(structure)\n\nRepresents a JDBC database output object which defines the output destination for a DataBrew recipe job to write into.\n\nGlueConnectionName -> (string)\n\nThe Glue connection that stores the connection information for the target database.\n\nDatabaseOptions -> (structure)\n\nRepresents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nTableName -> (string)\n\nA prefix for the name of a table DataBrew will create in the database.\n\nDatabaseOutputMode -> (string)\n\nThe output mode to write into the database. Currently supported option: NEW_TABLE.\n\nProjectName -> (string)\n\nThe DataBrew project associated with this job.\n\nProfileConfiguration -> (structure)\n\nConfiguration for profile jobs. Used to select columns, do evaluations, and override default parameters of evaluations. When configuration is null, the profile job will run with default settings.\n\nDatasetStatisticsConfiguration -> (structure)\n\nConfiguration for inter-column evaluations. Configuration can be used to select evaluations and override parameters of evaluations. When configuration is undefined, the profile job will run all supported inter-column evaluations.\n\nIncludedStatistics -> (list)\n\nList of included evaluations. When the list is undefined, all supported evaluations will be included.\n\n(string)\n\nOverrides -> (list)\n\nList of overrides for evaluations.\n\n(structure)\n\nOverride of a particular evaluation for a profile job.\n\nStatistic -> (string)\n\nThe name of an evaluation\n\nParameters -> (map)\n\nA map that includes overrides of an evaluation’s parameters.\n\nkey -> (string)\n\nvalue -> (string)\n\nProfileColumns -> (list)\n\nList of column selectors. ProfileColumns can be used to select columns from the dataset. When ProfileColumns is undefined, the profile job will profile all supported columns.\n\n(structure)\n\nSelector of a column from a dataset for profile job configuration. One selector includes either a column name or a regular expression.\n\nRegex -> (string)\n\nA regular expression for selecting a column from a dataset.\n\nName -> (string)\n\nThe name of a column from a dataset.\n\nColumnStatisticsConfigurations -> (list)\n\nList of configurations for column evaluations. ColumnStatisticsConfigurations are used to select evaluations and override parameters of evaluations for particular columns. When ColumnStatisticsConfigurations is undefined, the profile job will profile all supported columns and run all supported evaluations.\n\n(structure)\n\nConfiguration for column evaluations for a profile job. ColumnStatisticsConfiguration can be used to select evaluations and override parameters of evaluations for particular columns.\n\nSelectors -> (list)\n\nList of column selectors. Selectors can be used to select columns from the dataset. When selectors are undefined, configuration will be applied to all supported columns.\n\n(structure)\n\nSelector of a column from a dataset for profile job configuration. One selector includes either a column name or a regular expression.\n\nRegex -> (string)\n\nA regular expression for selecting a column from a dataset.\n\nName -> (string)\n\nThe name of a column from a dataset.\n\nStatistics -> (structure)\n\nConfiguration for evaluations. Statistics can be used to select evaluations and override parameters of evaluations.\n\nIncludedStatistics -> (list)\n\nList of included evaluations. When the list is undefined, all supported evaluations will be included.\n\n(string)\n\nOverrides -> (list)\n\nList of overrides for evaluations.\n\n(structure)\n\nOverride of a particular evaluation for a profile job.\n\nStatistic -> (string)\n\nThe name of an evaluation\n\nParameters -> (map)\n\nA map that includes overrides of an evaluation’s parameters.\n\nkey -> (string)\n\nvalue -> (string)\n\nRecipeReference -> (structure)\n\nRepresents the name and version of a DataBrew recipe.\n\nName -> (string)\n\nThe name of the recipe.\n\nRecipeVersion -> (string)\n\nThe identifier for the version for the recipe.\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the job.\n\nRoleArn -> (string)\n\nThe ARN of the Identity and Access Management (IAM) role to be assumed when DataBrew runs the job.\n\nTags -> (map)\n\nMetadata tags associated with this job.\n\nkey -> (string)\n\nvalue -> (string)\n\nTimeout -> (integer)\n\nThe job’s timeout in minutes. A job that attempts to run longer than this timeout period ends with a status of TIMEOUT .\n\nJobSample -> (structure)\n\nSample configuration for profile jobs only. Determines the number of rows on which the profile job will be executed.\n\nMode -> (string)\n\nA value that determines whether the profile job is run on the entire dataset or a specified number of rows. This value must be one of the following:\n\nFULL_DATASET - The profile job is run on the entire dataset.\n\nCUSTOM_ROWS - The profile job is run on the number of rows specified in the Size parameter.\n\nSize -> (long)\n\nThe Size parameter is only required when the mode is CUSTOM_ROWS. The profile job is run on the specified number of rows. The maximum value for size is Long.MAX_VALUE.\n\nLong.MAX_VALUE = 9223372036854775807"
    },
    {
      "command_name": "describe-job-run",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/describe-job-run.html",
      "command_description": "Description\n\nRepresents one run of a DataBrew job.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-job-run\n--name <value>\n--run-id <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "--run-id <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the job being processed during this run.\n\n--run-id (string)\n\nThe unique identifier of the job run.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nAttempt -> (integer)\n\nThe number of times that DataBrew has attempted to run the job.\n\nCompletedOn -> (timestamp)\n\nThe date and time when the job completed processing.\n\nDatasetName -> (string)\n\nThe name of the dataset for the job to process.\n\nErrorMessage -> (string)\n\nA message indicating an error (if any) that was encountered when the job ran.\n\nExecutionTime -> (integer)\n\nThe amount of time, in seconds, during which the job run consumed resources.\n\nJobName -> (string)\n\nThe name of the job being processed during this run.\n\nProfileConfiguration -> (structure)\n\nConfiguration for profile jobs. Used to select columns, do evaluations, and override default parameters of evaluations. When configuration is null, the profile job will run with default settings.\n\nDatasetStatisticsConfiguration -> (structure)\n\nConfiguration for inter-column evaluations. Configuration can be used to select evaluations and override parameters of evaluations. When configuration is undefined, the profile job will run all supported inter-column evaluations.\n\nIncludedStatistics -> (list)\n\nList of included evaluations. When the list is undefined, all supported evaluations will be included.\n\n(string)\n\nOverrides -> (list)\n\nList of overrides for evaluations.\n\n(structure)\n\nOverride of a particular evaluation for a profile job.\n\nStatistic -> (string)\n\nThe name of an evaluation\n\nParameters -> (map)\n\nA map that includes overrides of an evaluation’s parameters.\n\nkey -> (string)\n\nvalue -> (string)\n\nProfileColumns -> (list)\n\nList of column selectors. ProfileColumns can be used to select columns from the dataset. When ProfileColumns is undefined, the profile job will profile all supported columns.\n\n(structure)\n\nSelector of a column from a dataset for profile job configuration. One selector includes either a column name or a regular expression.\n\nRegex -> (string)\n\nA regular expression for selecting a column from a dataset.\n\nName -> (string)\n\nThe name of a column from a dataset.\n\nColumnStatisticsConfigurations -> (list)\n\nList of configurations for column evaluations. ColumnStatisticsConfigurations are used to select evaluations and override parameters of evaluations for particular columns. When ColumnStatisticsConfigurations is undefined, the profile job will profile all supported columns and run all supported evaluations.\n\n(structure)\n\nConfiguration for column evaluations for a profile job. ColumnStatisticsConfiguration can be used to select evaluations and override parameters of evaluations for particular columns.\n\nSelectors -> (list)\n\nList of column selectors. Selectors can be used to select columns from the dataset. When selectors are undefined, configuration will be applied to all supported columns.\n\n(structure)\n\nSelector of a column from a dataset for profile job configuration. One selector includes either a column name or a regular expression.\n\nRegex -> (string)\n\nA regular expression for selecting a column from a dataset.\n\nName -> (string)\n\nThe name of a column from a dataset.\n\nStatistics -> (structure)\n\nConfiguration for evaluations. Statistics can be used to select evaluations and override parameters of evaluations.\n\nIncludedStatistics -> (list)\n\nList of included evaluations. When the list is undefined, all supported evaluations will be included.\n\n(string)\n\nOverrides -> (list)\n\nList of overrides for evaluations.\n\n(structure)\n\nOverride of a particular evaluation for a profile job.\n\nStatistic -> (string)\n\nThe name of an evaluation\n\nParameters -> (map)\n\nA map that includes overrides of an evaluation’s parameters.\n\nkey -> (string)\n\nvalue -> (string)\n\nRunId -> (string)\n\nThe unique identifier of the job run.\n\nState -> (string)\n\nThe current state of the job run entity itself.\n\nLogSubscription -> (string)\n\nThe current status of Amazon CloudWatch logging for the job run.\n\nLogGroupName -> (string)\n\nThe name of an Amazon CloudWatch log group, where the job writes diagnostic messages when it runs.\n\nOutputs -> (list)\n\nOne or more output artifacts from a job run.\n\n(structure)\n\nRepresents options that specify how and where in Amazon S3 DataBrew writes the output generated by recipe jobs or profile jobs.\n\nCompressionFormat -> (string)\n\nThe compression algorithm used to compress the output text of the job.\n\nFormat -> (string)\n\nThe data format of the output of the job.\n\nPartitionColumns -> (list)\n\nThe names of one or more partition columns for the output of the job.\n\n(string)\n\nLocation -> (structure)\n\nThe location in Amazon S3 where the job writes its output.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nOverwrite -> (boolean)\n\nA value that, if true, means that any data in the location specified for output is overwritten with new output.\n\nFormatOptions -> (structure)\n\nRepresents options that define how DataBrew formats job output files.\n\nCsv -> (structure)\n\nRepresents a set of options that define the structure of comma-separated value (CSV) job output.\n\nDelimiter -> (string)\n\nA single character that specifies the delimiter used to create CSV job output.\n\nDataCatalogOutputs -> (list)\n\nOne or more artifacts that represent the Glue Data Catalog output from running the job.\n\n(structure)\n\nRepresents options that specify how and where in the Glue Data Catalog DataBrew writes the output generated by recipe jobs.\n\nCatalogId -> (string)\n\nThe unique identifier of the Amazon Web Services account that holds the Data Catalog that stores the data.\n\nDatabaseName -> (string)\n\nThe name of a database in the Data Catalog.\n\nTableName -> (string)\n\nThe name of a table in the Data Catalog.\n\nS3Options -> (structure)\n\nRepresents options that specify how and where DataBrew writes the Amazon S3 output generated by recipe jobs.\n\nLocation -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can write output from a job.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDatabaseOptions -> (structure)\n\nRepresents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nTableName -> (string)\n\nA prefix for the name of a table DataBrew will create in the database.\n\nOverwrite -> (boolean)\n\nA value that, if true, means that any data in the location specified for output is overwritten with new output. Not supported with DatabaseOptions.\n\nDatabaseOutputs -> (list)\n\nRepresents a list of JDBC database output objects which defines the output destination for a DataBrew recipe job to write into.\n\n(structure)\n\nRepresents a JDBC database output object which defines the output destination for a DataBrew recipe job to write into.\n\nGlueConnectionName -> (string)\n\nThe Glue connection that stores the connection information for the target database.\n\nDatabaseOptions -> (structure)\n\nRepresents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nTableName -> (string)\n\nA prefix for the name of a table DataBrew will create in the database.\n\nDatabaseOutputMode -> (string)\n\nThe output mode to write into the database. Currently supported option: NEW_TABLE.\n\nRecipeReference -> (structure)\n\nRepresents the name and version of a DataBrew recipe.\n\nName -> (string)\n\nThe name of the recipe.\n\nRecipeVersion -> (string)\n\nThe identifier for the version for the recipe.\n\nStartedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who started the job run.\n\nStartedOn -> (timestamp)\n\nThe date and time when the job run began.\n\nJobSample -> (structure)\n\nSample configuration for profile jobs only. Determines the number of rows on which the profile job will be executed. If a JobSample value is not provided, the default value will be used. The default value is CUSTOM_ROWS for the mode parameter and 20000 for the size parameter.\n\nMode -> (string)\n\nA value that determines whether the profile job is run on the entire dataset or a specified number of rows. This value must be one of the following:\n\nFULL_DATASET - The profile job is run on the entire dataset.\n\nCUSTOM_ROWS - The profile job is run on the number of rows specified in the Size parameter.\n\nSize -> (long)\n\nThe Size parameter is only required when the mode is CUSTOM_ROWS. The profile job is run on the specified number of rows. The maximum value for size is Long.MAX_VALUE.\n\nLong.MAX_VALUE = 9223372036854775807"
    },
    {
      "command_name": "describe-project",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/describe-project.html",
      "command_description": "Description\n\nReturns the definition of a specific DataBrew project.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-project\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the project to be described.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nCreateDate -> (timestamp)\n\nThe date and time that the project was created.\n\nCreatedBy -> (string)\n\nThe identifier (user name) of the user who created the project.\n\nDatasetName -> (string)\n\nThe dataset associated with the project.\n\nLastModifiedDate -> (timestamp)\n\nThe date and time that the project was last modified.\n\nLastModifiedBy -> (string)\n\nThe identifier (user name) of the user who last modified the project.\n\nName -> (string)\n\nThe name of the project.\n\nRecipeName -> (string)\n\nThe recipe associated with this job.\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the project.\n\nSample -> (structure)\n\nRepresents the sample size and sampling type for DataBrew to use for interactive data analysis.\n\nSize -> (integer)\n\nThe number of rows in the sample.\n\nType -> (string)\n\nThe way in which DataBrew obtains rows from a dataset.\n\nRoleArn -> (string)\n\nThe ARN of the Identity and Access Management (IAM) role to be assumed when DataBrew runs the job.\n\nTags -> (map)\n\nMetadata tags associated with this project.\n\nkey -> (string)\n\nvalue -> (string)\n\nSessionStatus -> (string)\n\nDescribes the current state of the session:\n\nPROVISIONING - allocating resources for the session.\n\nINITIALIZING - getting the session ready for first use.\n\nASSIGNED - the session is ready for use.\n\nOpenedBy -> (string)\n\nThe identifier (user name) of the user that opened the project for use.\n\nOpenDate -> (timestamp)\n\nThe date and time when the project was opened."
    },
    {
      "command_name": "describe-recipe",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/describe-recipe.html",
      "command_description": "Description\n\nReturns the definition of a specific DataBrew recipe corresponding to a particular version.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-recipe\n--name <value>\n[--recipe-version <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--recipe-version <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the recipe to be described.\n\n--recipe-version (string)\n\nThe recipe version identifier. If this parameter isn’t specified, then the latest published version is returned.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nCreatedBy -> (string)\n\nThe identifier (user name) of the user who created the recipe.\n\nCreateDate -> (timestamp)\n\nThe date and time that the recipe was created.\n\nLastModifiedBy -> (string)\n\nThe identifier (user name) of the user who last modified the recipe.\n\nLastModifiedDate -> (timestamp)\n\nThe date and time that the recipe was last modified.\n\nProjectName -> (string)\n\nThe name of the project associated with this recipe.\n\nPublishedBy -> (string)\n\nThe identifier (user name) of the user who last published the recipe.\n\nPublishedDate -> (timestamp)\n\nThe date and time when the recipe was last published.\n\nDescription -> (string)\n\nThe description of the recipe.\n\nName -> (string)\n\nThe name of the recipe.\n\nSteps -> (list)\n\nOne or more steps to be performed by the recipe. Each step consists of an action, and the conditions under which the action should succeed.\n\n(structure)\n\nRepresents a single step from a DataBrew recipe to be performed.\n\nAction -> (structure)\n\nThe particular action to be performed in the recipe step.\n\nOperation -> (string)\n\nThe name of a valid DataBrew transformation to be performed on the data.\n\nParameters -> (map)\n\nContextual parameters for the transformation.\n\nkey -> (string)\n\nvalue -> (string)\n\nConditionExpressions -> (list)\n\nOne or more conditions that must be met for the recipe step to succeed.\n\nNote\n\nAll of the conditions in the array must be met. In other words, all of the conditions must be combined using a logical AND operation.\n\n(structure)\n\nRepresents an individual condition that evaluates to true or false.\n\nConditions are used with recipe actions. The action is only performed for column values where the condition evaluates to true.\n\nIf a recipe requires more than one condition, then the recipe must specify multiple ConditionExpression elements. Each condition is applied to the rows in a dataset first, before the recipe action is performed.\n\nCondition -> (string)\n\nA specific condition to apply to a recipe action. For more information, see Recipe structure in the Glue DataBrew Developer Guide .\n\nValue -> (string)\n\nA value that the condition must evaluate to for the condition to succeed.\n\nTargetColumn -> (string)\n\nA column to apply this condition to.\n\nTags -> (map)\n\nMetadata tags associated with this project.\n\nkey -> (string)\n\nvalue -> (string)\n\nResourceArn -> (string)\n\nThe ARN of the recipe.\n\nRecipeVersion -> (string)\n\nThe recipe version identifier."
    },
    {
      "command_name": "describe-schedule",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/describe-schedule.html",
      "command_description": "Description\n\nReturns the definition of a specific DataBrew schedule.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-schedule\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the schedule to be described.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nCreateDate -> (timestamp)\n\nThe date and time that the schedule was created.\n\nCreatedBy -> (string)\n\nThe identifier (user name) of the user who created the schedule.\n\nJobNames -> (list)\n\nThe name or names of one or more jobs to be run by using the schedule.\n\n(string)\n\nLastModifiedBy -> (string)\n\nThe identifier (user name) of the user who last modified the schedule.\n\nLastModifiedDate -> (timestamp)\n\nThe date and time that the schedule was last modified.\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the schedule.\n\nCronExpression -> (string)\n\nThe date or dates and time or times when the jobs are to be run for the schedule. For more information, see Cron expressions in the Glue DataBrew Developer Guide .\n\nTags -> (map)\n\nMetadata tags associated with this schedule.\n\nkey -> (string)\n\nvalue -> (string)\n\nName -> (string)\n\nThe name of the schedule."
    },
    {
      "command_name": "list-datasets",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/list-datasets.html",
      "command_description": "Description\n\nLists all of the DataBrew datasets.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\nlist-datasets is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: Datasets",
      "command_synopsis": "Synopsis\n  list-datasets\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nDatasets -> (list)\n\nA list of datasets that are defined.\n\n(structure)\n\nRepresents a dataset that can be processed by DataBrew.\n\nAccountId -> (string)\n\nThe ID of the Amazon Web Services account that owns the dataset.\n\nCreatedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who created the dataset.\n\nCreateDate -> (timestamp)\n\nThe date and time that the dataset was created.\n\nName -> (string)\n\nThe unique name of the dataset.\n\nFormat -> (string)\n\nThe file format of a dataset that is created from an Amazon S3 file or folder.\n\nFormatOptions -> (structure)\n\nA set of options that define how DataBrew interprets the data in the dataset.\n\nJson -> (structure)\n\nOptions that define how JSON input is to be interpreted by DataBrew.\n\nMultiLine -> (boolean)\n\nA value that specifies whether JSON input contains embedded new line characters.\n\nExcel -> (structure)\n\nOptions that define how Excel input is to be interpreted by DataBrew.\n\nSheetNames -> (list)\n\nOne or more named sheets in the Excel file that will be included in the dataset.\n\n(string)\n\nSheetIndexes -> (list)\n\nOne or more sheet numbers in the Excel file that will be included in the dataset.\n\n(integer)\n\nHeaderRow -> (boolean)\n\nA variable that specifies whether the first row in the file is parsed as the header. If this value is false, column names are auto-generated.\n\nCsv -> (structure)\n\nOptions that define how CSV input is to be interpreted by DataBrew.\n\nDelimiter -> (string)\n\nA single character that specifies the delimiter being used in the CSV file.\n\nHeaderRow -> (boolean)\n\nA variable that specifies whether the first row in the file is parsed as the header. If this value is false, column names are auto-generated.\n\nInput -> (structure)\n\nInformation on how DataBrew can find the dataset, in either the Glue Data Catalog or Amazon S3.\n\nS3InputDefinition -> (structure)\n\nThe Amazon S3 location where the data is stored.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDataCatalogInputDefinition -> (structure)\n\nThe Glue Data Catalog parameters for the data.\n\nCatalogId -> (string)\n\nThe unique identifier of the Amazon Web Services account that holds the Data Catalog that stores the data.\n\nDatabaseName -> (string)\n\nThe name of a database in the Data Catalog.\n\nTableName -> (string)\n\nThe name of a database table in the Data Catalog. This table corresponds to a DataBrew dataset.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon location where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDatabaseInputDefinition -> (structure)\n\nConnection information for dataset input files stored in a database.\n\nGlueConnectionName -> (string)\n\nThe Glue Connection that stores the connection information for the target database.\n\nDatabaseTableName -> (string)\n\nThe table within the target database.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can read input data, or write output from a job.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nLastModifiedDate -> (timestamp)\n\nThe last modification date and time of the dataset.\n\nLastModifiedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who last modified the dataset.\n\nSource -> (string)\n\nThe location of the data for the dataset, either Amazon S3 or the Glue Data Catalog.\n\nPathOptions -> (structure)\n\nA set of options that defines how DataBrew interprets an Amazon S3 path of the dataset.\n\nLastModifiedDateCondition -> (structure)\n\nIf provided, this structure defines a date range for matching Amazon S3 objects based on their LastModifiedDate attribute in Amazon S3.\n\nExpression -> (string)\n\nThe expression which includes condition names followed by substitution variables, possibly grouped and combined with other conditions. For example, “(starts_with :prefix1 or starts_with :prefix2) and (ends_with :suffix1 or ends_with :suffix2)”. Substitution variables should start with ‘:’ symbol.\n\nValuesMap -> (map)\n\nThe map of substitution variable names to their values used in this filter expression.\n\nkey -> (string)\n\nvalue -> (string)\n\nFilesLimit -> (structure)\n\nIf provided, this structure imposes a limit on a number of files that should be selected.\n\nMaxFiles -> (integer)\n\nThe number of Amazon S3 files to select.\n\nOrderedBy -> (string)\n\nA criteria to use for Amazon S3 files sorting before their selection. By default uses LAST_MODIFIED_DATE as a sorting criteria. Currently it’s the only allowed value.\n\nOrder -> (string)\n\nA criteria to use for Amazon S3 files sorting before their selection. By default uses DESCENDING order, i.e. most recent files are selected first. Anotherpossible value is ASCENDING.\n\nParameters -> (map)\n\nA structure that maps names of parameters used in the Amazon S3 path of a dataset to their definitions.\n\nkey -> (string)\n\nvalue -> (structure)\n\nRepresents a dataset paramater that defines type and conditions for a parameter in the Amazon S3 path of the dataset.\n\nName -> (string)\n\nThe name of the parameter that is used in the dataset’s Amazon S3 path.\n\nType -> (string)\n\nThe type of the dataset parameter, can be one of a ‘String’, ‘Number’ or ‘Datetime’.\n\nDatetimeOptions -> (structure)\n\nAdditional parameter options such as a format and a timezone. Required for datetime parameters.\n\nFormat -> (string)\n\nRequired option, that defines the datetime format used for a date parameter in the Amazon S3 path. Should use only supported datetime specifiers and separation characters, all literal a-z or A-Z characters should be escaped with single quotes. E.g. “MM.dd.yyyy-‘at’-HH:mm”.\n\nTimezoneOffset -> (string)\n\nOptional value for a timezone offset of the datetime parameter value in the Amazon S3 path. Shouldn’t be used if Format for this parameter includes timezone fields. If no offset specified, UTC is assumed.\n\nLocaleCode -> (string)\n\nOptional value for a non-US locale code, needed for correct interpretation of some date formats.\n\nCreateColumn -> (boolean)\n\nOptional boolean value that defines whether the captured value of this parameter should be used to create a new column in a dataset.\n\nFilter -> (structure)\n\nThe optional filter expression structure to apply additional matching criteria to the parameter.\n\nExpression -> (string)\n\nThe expression which includes condition names followed by substitution variables, possibly grouped and combined with other conditions. For example, “(starts_with :prefix1 or starts_with :prefix2) and (ends_with :suffix1 or ends_with :suffix2)”. Substitution variables should start with ‘:’ symbol.\n\nValuesMap -> (map)\n\nThe map of substitution variable names to their values used in this filter expression.\n\nkey -> (string)\n\nvalue -> (string)\n\nTags -> (map)\n\nMetadata tags that have been applied to the dataset.\n\nkey -> (string)\n\nvalue -> (string)\n\nResourceArn -> (string)\n\nThe unique Amazon Resource Name (ARN) for the dataset.\n\nNextToken -> (string)\n\nA token that you can use in a subsequent call to retrieve the next set of results."
    },
    {
      "command_name": "list-job-runs",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/list-job-runs.html",
      "command_description": "Description\n\nLists all of the previous runs of a particular DataBrew job.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\nlist-job-runs is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: JobRuns",
      "command_synopsis": "Synopsis\n  list-job-runs\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the job.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nJobRuns -> (list)\n\nA list of job runs that have occurred for the specified job.\n\n(structure)\n\nRepresents one run of a DataBrew job.\n\nAttempt -> (integer)\n\nThe number of times that DataBrew has attempted to run the job.\n\nCompletedOn -> (timestamp)\n\nThe date and time when the job completed processing.\n\nDatasetName -> (string)\n\nThe name of the dataset for the job to process.\n\nErrorMessage -> (string)\n\nA message indicating an error (if any) that was encountered when the job ran.\n\nExecutionTime -> (integer)\n\nThe amount of time, in seconds, during which a job run consumed resources.\n\nJobName -> (string)\n\nThe name of the job being processed during this run.\n\nRunId -> (string)\n\nThe unique identifier of the job run.\n\nState -> (string)\n\nThe current state of the job run entity itself.\n\nLogSubscription -> (string)\n\nThe current status of Amazon CloudWatch logging for the job run.\n\nLogGroupName -> (string)\n\nThe name of an Amazon CloudWatch log group, where the job writes diagnostic messages when it runs.\n\nOutputs -> (list)\n\nOne or more output artifacts from a job run.\n\n(structure)\n\nRepresents options that specify how and where in Amazon S3 DataBrew writes the output generated by recipe jobs or profile jobs.\n\nCompressionFormat -> (string)\n\nThe compression algorithm used to compress the output text of the job.\n\nFormat -> (string)\n\nThe data format of the output of the job.\n\nPartitionColumns -> (list)\n\nThe names of one or more partition columns for the output of the job.\n\n(string)\n\nLocation -> (structure)\n\nThe location in Amazon S3 where the job writes its output.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nOverwrite -> (boolean)\n\nA value that, if true, means that any data in the location specified for output is overwritten with new output.\n\nFormatOptions -> (structure)\n\nRepresents options that define how DataBrew formats job output files.\n\nCsv -> (structure)\n\nRepresents a set of options that define the structure of comma-separated value (CSV) job output.\n\nDelimiter -> (string)\n\nA single character that specifies the delimiter used to create CSV job output.\n\nDataCatalogOutputs -> (list)\n\nOne or more artifacts that represent the Glue Data Catalog output from running the job.\n\n(structure)\n\nRepresents options that specify how and where in the Glue Data Catalog DataBrew writes the output generated by recipe jobs.\n\nCatalogId -> (string)\n\nThe unique identifier of the Amazon Web Services account that holds the Data Catalog that stores the data.\n\nDatabaseName -> (string)\n\nThe name of a database in the Data Catalog.\n\nTableName -> (string)\n\nThe name of a table in the Data Catalog.\n\nS3Options -> (structure)\n\nRepresents options that specify how and where DataBrew writes the Amazon S3 output generated by recipe jobs.\n\nLocation -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can write output from a job.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDatabaseOptions -> (structure)\n\nRepresents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nTableName -> (string)\n\nA prefix for the name of a table DataBrew will create in the database.\n\nOverwrite -> (boolean)\n\nA value that, if true, means that any data in the location specified for output is overwritten with new output. Not supported with DatabaseOptions.\n\nDatabaseOutputs -> (list)\n\nRepresents a list of JDBC database output objects which defines the output destination for a DataBrew recipe job to write into.\n\n(structure)\n\nRepresents a JDBC database output object which defines the output destination for a DataBrew recipe job to write into.\n\nGlueConnectionName -> (string)\n\nThe Glue connection that stores the connection information for the target database.\n\nDatabaseOptions -> (structure)\n\nRepresents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nTableName -> (string)\n\nA prefix for the name of a table DataBrew will create in the database.\n\nDatabaseOutputMode -> (string)\n\nThe output mode to write into the database. Currently supported option: NEW_TABLE.\n\nRecipeReference -> (structure)\n\nThe set of steps processed by the job.\n\nName -> (string)\n\nThe name of the recipe.\n\nRecipeVersion -> (string)\n\nThe identifier for the version for the recipe.\n\nStartedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who initiated the job run.\n\nStartedOn -> (timestamp)\n\nThe date and time when the job run began.\n\nJobSample -> (structure)\n\nA sample configuration for profile jobs only, which determines the number of rows on which the profile job is run. If a JobSample value isn’t provided, the default is used. The default value is CUSTOM_ROWS for the mode parameter and 20,000 for the size parameter.\n\nMode -> (string)\n\nA value that determines whether the profile job is run on the entire dataset or a specified number of rows. This value must be one of the following:\n\nFULL_DATASET - The profile job is run on the entire dataset.\n\nCUSTOM_ROWS - The profile job is run on the number of rows specified in the Size parameter.\n\nSize -> (long)\n\nThe Size parameter is only required when the mode is CUSTOM_ROWS. The profile job is run on the specified number of rows. The maximum value for size is Long.MAX_VALUE.\n\nLong.MAX_VALUE = 9223372036854775807\n\nNextToken -> (string)\n\nA token that you can use in a subsequent call to retrieve the next set of results."
    },
    {
      "command_name": "list-jobs",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/list-jobs.html",
      "command_description": "Description\n\nLists all of the DataBrew jobs that are defined.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\nlist-jobs is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: Jobs",
      "command_synopsis": "Synopsis\n  list-jobs\n[--dataset-name <value>]\n[--project-name <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--dataset-name <value>]",
        "[--project-name <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--dataset-name (string)\n\nThe name of a dataset. Using this parameter indicates to return only those jobs that act on the specified dataset.\n\n--project-name (string)\n\nThe name of a project. Using this parameter indicates to return only those jobs that are associated with the specified project.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nJobs -> (list)\n\nA list of jobs that are defined.\n\n(structure)\n\nRepresents all of the attributes of a DataBrew job.\n\nAccountId -> (string)\n\nThe ID of the Amazon Web Services account that owns the job.\n\nCreatedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who created the job.\n\nCreateDate -> (timestamp)\n\nThe date and time that the job was created.\n\nDatasetName -> (string)\n\nA dataset that the job is to process.\n\nEncryptionKeyArn -> (string)\n\nThe Amazon Resource Name (ARN) of an encryption key that is used to protect the job output. For more information, see Encrypting data written by DataBrew jobs\n\nEncryptionMode -> (string)\n\nThe encryption mode for the job, which can be one of the following:\n\nSSE-KMS - Server-side encryption with keys managed by KMS.\n\nSSE-S3 - Server-side encryption with keys managed by Amazon S3.\n\nName -> (string)\n\nThe unique name of the job.\n\nType -> (string)\n\nThe job type of the job, which must be one of the following:\n\nPROFILE - A job to analyze a dataset, to determine its size, data types, data distribution, and more.\n\nRECIPE - A job to apply one or more transformations to a dataset.\n\nLastModifiedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who last modified the job.\n\nLastModifiedDate -> (timestamp)\n\nThe modification date and time of the job.\n\nLogSubscription -> (string)\n\nThe current status of Amazon CloudWatch logging for the job.\n\nMaxCapacity -> (integer)\n\nThe maximum number of nodes that can be consumed when the job processes data.\n\nMaxRetries -> (integer)\n\nThe maximum number of times to retry the job after a job run fails.\n\nOutputs -> (list)\n\nOne or more artifacts that represent output from running the job.\n\n(structure)\n\nRepresents options that specify how and where in Amazon S3 DataBrew writes the output generated by recipe jobs or profile jobs.\n\nCompressionFormat -> (string)\n\nThe compression algorithm used to compress the output text of the job.\n\nFormat -> (string)\n\nThe data format of the output of the job.\n\nPartitionColumns -> (list)\n\nThe names of one or more partition columns for the output of the job.\n\n(string)\n\nLocation -> (structure)\n\nThe location in Amazon S3 where the job writes its output.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nOverwrite -> (boolean)\n\nA value that, if true, means that any data in the location specified for output is overwritten with new output.\n\nFormatOptions -> (structure)\n\nRepresents options that define how DataBrew formats job output files.\n\nCsv -> (structure)\n\nRepresents a set of options that define the structure of comma-separated value (CSV) job output.\n\nDelimiter -> (string)\n\nA single character that specifies the delimiter used to create CSV job output.\n\nDataCatalogOutputs -> (list)\n\nOne or more artifacts that represent the Glue Data Catalog output from running the job.\n\n(structure)\n\nRepresents options that specify how and where in the Glue Data Catalog DataBrew writes the output generated by recipe jobs.\n\nCatalogId -> (string)\n\nThe unique identifier of the Amazon Web Services account that holds the Data Catalog that stores the data.\n\nDatabaseName -> (string)\n\nThe name of a database in the Data Catalog.\n\nTableName -> (string)\n\nThe name of a table in the Data Catalog.\n\nS3Options -> (structure)\n\nRepresents options that specify how and where DataBrew writes the Amazon S3 output generated by recipe jobs.\n\nLocation -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can write output from a job.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDatabaseOptions -> (structure)\n\nRepresents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nTableName -> (string)\n\nA prefix for the name of a table DataBrew will create in the database.\n\nOverwrite -> (boolean)\n\nA value that, if true, means that any data in the location specified for output is overwritten with new output. Not supported with DatabaseOptions.\n\nDatabaseOutputs -> (list)\n\nRepresents a list of JDBC database output objects which defines the output destination for a DataBrew recipe job to write into.\n\n(structure)\n\nRepresents a JDBC database output object which defines the output destination for a DataBrew recipe job to write into.\n\nGlueConnectionName -> (string)\n\nThe Glue connection that stores the connection information for the target database.\n\nDatabaseOptions -> (structure)\n\nRepresents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nTableName -> (string)\n\nA prefix for the name of a table DataBrew will create in the database.\n\nDatabaseOutputMode -> (string)\n\nThe output mode to write into the database. Currently supported option: NEW_TABLE.\n\nProjectName -> (string)\n\nThe name of the project that the job is associated with.\n\nRecipeReference -> (structure)\n\nA set of steps that the job runs.\n\nName -> (string)\n\nThe name of the recipe.\n\nRecipeVersion -> (string)\n\nThe identifier for the version for the recipe.\n\nResourceArn -> (string)\n\nThe unique Amazon Resource Name (ARN) for the job.\n\nRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the role to be assumed for this job.\n\nTimeout -> (integer)\n\nThe job’s timeout in minutes. A job that attempts to run longer than this timeout period ends with a status of TIMEOUT .\n\nTags -> (map)\n\nMetadata tags that have been applied to the job.\n\nkey -> (string)\n\nvalue -> (string)\n\nJobSample -> (structure)\n\nA sample configuration for profile jobs only, which determines the number of rows on which the profile job is run. If a JobSample value isn’t provided, the default value is used. The default value is CUSTOM_ROWS for the mode parameter and 20,000 for the size parameter.\n\nMode -> (string)\n\nA value that determines whether the profile job is run on the entire dataset or a specified number of rows. This value must be one of the following:\n\nFULL_DATASET - The profile job is run on the entire dataset.\n\nCUSTOM_ROWS - The profile job is run on the number of rows specified in the Size parameter.\n\nSize -> (long)\n\nThe Size parameter is only required when the mode is CUSTOM_ROWS. The profile job is run on the specified number of rows. The maximum value for size is Long.MAX_VALUE.\n\nLong.MAX_VALUE = 9223372036854775807\n\nNextToken -> (string)\n\nA token that you can use in a subsequent call to retrieve the next set of results."
    },
    {
      "command_name": "list-projects",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/list-projects.html",
      "command_description": "Description\n\nLists all of the DataBrew projects that are defined.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\nlist-projects is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: Projects",
      "command_synopsis": "Synopsis\n  list-projects\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nProjects -> (list)\n\nA list of projects that are defined .\n\n(structure)\n\nRepresents all of the attributes of a DataBrew project.\n\nAccountId -> (string)\n\nThe ID of the Amazon Web Services account that owns the project.\n\nCreateDate -> (timestamp)\n\nThe date and time that the project was created.\n\nCreatedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who crated the project.\n\nDatasetName -> (string)\n\nThe dataset that the project is to act upon.\n\nLastModifiedDate -> (timestamp)\n\nThe last modification date and time for the project.\n\nLastModifiedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who last modified the project.\n\nName -> (string)\n\nThe unique name of a project.\n\nRecipeName -> (string)\n\nThe name of a recipe that will be developed during a project session.\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) for the project.\n\nSample -> (structure)\n\nThe sample size and sampling type to apply to the data. If this parameter isn’t specified, then the sample consists of the first 500 rows from the dataset.\n\nSize -> (integer)\n\nThe number of rows in the sample.\n\nType -> (string)\n\nThe way in which DataBrew obtains rows from a dataset.\n\nTags -> (map)\n\nMetadata tags that have been applied to the project.\n\nkey -> (string)\n\nvalue -> (string)\n\nRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the role that will be assumed for this project.\n\nOpenedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user that opened the project for use.\n\nOpenDate -> (timestamp)\n\nThe date and time when the project was opened.\n\nNextToken -> (string)\n\nA token that you can use in a subsequent call to retrieve the next set of results."
    },
    {
      "command_name": "list-recipe-versions",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/list-recipe-versions.html",
      "command_description": "Description\n\nLists the versions of a particular DataBrew recipe, except for LATEST_WORKING .\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\nlist-recipe-versions is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: Recipes",
      "command_synopsis": "Synopsis\n  list-recipe-versions\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the recipe for which to return version information.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nNextToken -> (string)\n\nA token that you can use in a subsequent call to retrieve the next set of results.\n\nRecipes -> (list)\n\nA list of versions for the specified recipe.\n\n(structure)\n\nRepresents one or more actions to be performed on a DataBrew dataset.\n\nCreatedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who created the recipe.\n\nCreateDate -> (timestamp)\n\nThe date and time that the recipe was created.\n\nLastModifiedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who last modified the recipe.\n\nLastModifiedDate -> (timestamp)\n\nThe last modification date and time of the recipe.\n\nProjectName -> (string)\n\nThe name of the project that the recipe is associated with.\n\nPublishedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who published the recipe.\n\nPublishedDate -> (timestamp)\n\nThe date and time when the recipe was published.\n\nDescription -> (string)\n\nThe description of the recipe.\n\nName -> (string)\n\nThe unique name for the recipe.\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) for the recipe.\n\nSteps -> (list)\n\nA list of steps that are defined by the recipe.\n\n(structure)\n\nRepresents a single step from a DataBrew recipe to be performed.\n\nAction -> (structure)\n\nThe particular action to be performed in the recipe step.\n\nOperation -> (string)\n\nThe name of a valid DataBrew transformation to be performed on the data.\n\nParameters -> (map)\n\nContextual parameters for the transformation.\n\nkey -> (string)\n\nvalue -> (string)\n\nConditionExpressions -> (list)\n\nOne or more conditions that must be met for the recipe step to succeed.\n\nNote\n\nAll of the conditions in the array must be met. In other words, all of the conditions must be combined using a logical AND operation.\n\n(structure)\n\nRepresents an individual condition that evaluates to true or false.\n\nConditions are used with recipe actions. The action is only performed for column values where the condition evaluates to true.\n\nIf a recipe requires more than one condition, then the recipe must specify multiple ConditionExpression elements. Each condition is applied to the rows in a dataset first, before the recipe action is performed.\n\nCondition -> (string)\n\nA specific condition to apply to a recipe action. For more information, see Recipe structure in the Glue DataBrew Developer Guide .\n\nValue -> (string)\n\nA value that the condition must evaluate to for the condition to succeed.\n\nTargetColumn -> (string)\n\nA column to apply this condition to.\n\nTags -> (map)\n\nMetadata tags that have been applied to the recipe.\n\nkey -> (string)\n\nvalue -> (string)\n\nRecipeVersion -> (string)\n\nThe identifier for the version for the recipe. Must be one of the following:\n\nNumeric version (X.Y ) - X and Y stand for major and minor version numbers. The maximum length of each is 6 digits, and neither can be negative values. Both X and Y are required, and “0.0” isn’t a valid version.\n\nLATEST_WORKING - the most recent valid version being developed in a DataBrew project.\n\nLATEST_PUBLISHED - the most recent published version."
    },
    {
      "command_name": "list-recipes",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/list-recipes.html",
      "command_description": "Description\n\nLists all of the DataBrew recipes that are defined.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\nlist-recipes is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: Recipes",
      "command_synopsis": "Synopsis\n  list-recipes\n[--recipe-version <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--recipe-version <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--recipe-version (string)\n\nReturn only those recipes with a version identifier of LATEST_WORKING or LATEST_PUBLISHED . If RecipeVersion is omitted, ListRecipes returns all of the LATEST_PUBLISHED recipe versions.\n\nValid values: LATEST_WORKING | LATEST_PUBLISHED\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nRecipes -> (list)\n\nA list of recipes that are defined.\n\n(structure)\n\nRepresents one or more actions to be performed on a DataBrew dataset.\n\nCreatedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who created the recipe.\n\nCreateDate -> (timestamp)\n\nThe date and time that the recipe was created.\n\nLastModifiedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who last modified the recipe.\n\nLastModifiedDate -> (timestamp)\n\nThe last modification date and time of the recipe.\n\nProjectName -> (string)\n\nThe name of the project that the recipe is associated with.\n\nPublishedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who published the recipe.\n\nPublishedDate -> (timestamp)\n\nThe date and time when the recipe was published.\n\nDescription -> (string)\n\nThe description of the recipe.\n\nName -> (string)\n\nThe unique name for the recipe.\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) for the recipe.\n\nSteps -> (list)\n\nA list of steps that are defined by the recipe.\n\n(structure)\n\nRepresents a single step from a DataBrew recipe to be performed.\n\nAction -> (structure)\n\nThe particular action to be performed in the recipe step.\n\nOperation -> (string)\n\nThe name of a valid DataBrew transformation to be performed on the data.\n\nParameters -> (map)\n\nContextual parameters for the transformation.\n\nkey -> (string)\n\nvalue -> (string)\n\nConditionExpressions -> (list)\n\nOne or more conditions that must be met for the recipe step to succeed.\n\nNote\n\nAll of the conditions in the array must be met. In other words, all of the conditions must be combined using a logical AND operation.\n\n(structure)\n\nRepresents an individual condition that evaluates to true or false.\n\nConditions are used with recipe actions. The action is only performed for column values where the condition evaluates to true.\n\nIf a recipe requires more than one condition, then the recipe must specify multiple ConditionExpression elements. Each condition is applied to the rows in a dataset first, before the recipe action is performed.\n\nCondition -> (string)\n\nA specific condition to apply to a recipe action. For more information, see Recipe structure in the Glue DataBrew Developer Guide .\n\nValue -> (string)\n\nA value that the condition must evaluate to for the condition to succeed.\n\nTargetColumn -> (string)\n\nA column to apply this condition to.\n\nTags -> (map)\n\nMetadata tags that have been applied to the recipe.\n\nkey -> (string)\n\nvalue -> (string)\n\nRecipeVersion -> (string)\n\nThe identifier for the version for the recipe. Must be one of the following:\n\nNumeric version (X.Y ) - X and Y stand for major and minor version numbers. The maximum length of each is 6 digits, and neither can be negative values. Both X and Y are required, and “0.0” isn’t a valid version.\n\nLATEST_WORKING - the most recent valid version being developed in a DataBrew project.\n\nLATEST_PUBLISHED - the most recent published version.\n\nNextToken -> (string)\n\nA token that you can use in a subsequent call to retrieve the next set of results."
    },
    {
      "command_name": "list-schedules",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/list-schedules.html",
      "command_description": "Description\n\nLists the DataBrew schedules that are defined.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\nlist-schedules is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: Schedules",
      "command_synopsis": "Synopsis\n  list-schedules\n[--job-name <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--job-name <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--job-name (string)\n\nThe name of the job that these schedules apply to.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nSchedules -> (list)\n\nA list of schedules that are defined.\n\n(structure)\n\nRepresents one or more dates and times when a job is to run.\n\nAccountId -> (string)\n\nThe ID of the Amazon Web Services account that owns the schedule.\n\nCreatedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who created the schedule.\n\nCreateDate -> (timestamp)\n\nThe date and time that the schedule was created.\n\nJobNames -> (list)\n\nA list of jobs to be run, according to the schedule.\n\n(string)\n\nLastModifiedBy -> (string)\n\nThe Amazon Resource Name (ARN) of the user who last modified the schedule.\n\nLastModifiedDate -> (timestamp)\n\nThe date and time when the schedule was last modified.\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the schedule.\n\nCronExpression -> (string)\n\nThe dates and times when the job is to run. For more information, see Cron expressions in the Glue DataBrew Developer Guide .\n\nTags -> (map)\n\nMetadata tags that have been applied to the schedule.\n\nkey -> (string)\n\nvalue -> (string)\n\nName -> (string)\n\nThe name of the schedule.\n\nNextToken -> (string)\n\nA token that you can use in a subsequent call to retrieve the next set of results."
    },
    {
      "command_name": "list-tags-for-resource",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/list-tags-for-resource.html",
      "command_description": "Description\n\nLists all the tags for a DataBrew resource.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  list-tags-for-resource\n--resource-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--resource-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--resource-arn (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the DataBrew resource.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nTags -> (map)\n\nA list of tags associated with the DataBrew resource.\n\nkey -> (string)\n\nvalue -> (string)"
    },
    {
      "command_name": "publish-recipe",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/publish-recipe.html",
      "command_description": "Description\n\nPublishes a new version of a DataBrew recipe.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  publish-recipe\n[--description <value>]\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--description <value>]",
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--description (string)\n\nA description of the recipe to be published, for this version of the recipe.\n\n--name (string)\n\nThe name of the recipe to be published.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the recipe that you published."
    },
    {
      "command_name": "send-project-session-action",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/send-project-session-action.html",
      "command_description": "Description\n\nPerforms a recipe step within an interactive DataBrew session that’s currently open.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  send-project-session-action\n[--preview | --no-preview]\n--name <value>\n[--recipe-step <value>]\n[--step-index <value>]\n[--client-session-id <value>]\n[--view-frame <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--preview | --no-preview]",
        "--name <value>",
        "[--recipe-step <value>]",
        "[--step-index <value>]",
        "[--client-session-id <value>]",
        "[--view-frame <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--preview | --no-preview (boolean)\n\nIf true, the result of the recipe step will be returned, but not applied.\n\n--name (string)\n\nThe name of the project to apply the action to.\n\n--recipe-step (structure)\n\nRepresents a single step from a DataBrew recipe to be performed.\n\nAction -> (structure)\n\nThe particular action to be performed in the recipe step.\n\nOperation -> (string)\n\nThe name of a valid DataBrew transformation to be performed on the data.\n\nParameters -> (map)\n\nContextual parameters for the transformation.\n\nkey -> (string)\n\nvalue -> (string)\n\nConditionExpressions -> (list)\n\nOne or more conditions that must be met for the recipe step to succeed.\n\nNote\n\nAll of the conditions in the array must be met. In other words, all of the conditions must be combined using a logical AND operation.\n\n(structure)\n\nRepresents an individual condition that evaluates to true or false.\n\nConditions are used with recipe actions. The action is only performed for column values where the condition evaluates to true.\n\nIf a recipe requires more than one condition, then the recipe must specify multiple ConditionExpression elements. Each condition is applied to the rows in a dataset first, before the recipe action is performed.\n\nCondition -> (string)\n\nA specific condition to apply to a recipe action. For more information, see Recipe structure in the Glue DataBrew Developer Guide .\n\nValue -> (string)\n\nA value that the condition must evaluate to for the condition to succeed.\n\nTargetColumn -> (string)\n\nA column to apply this condition to.\n\nShorthand Syntax:\n\nAction={Operation=string,Parameters={KeyName1=string,KeyName2=string}},ConditionExpressions=[{Condition=string,Value=string,TargetColumn=string},{Condition=string,Value=string,TargetColumn=string}]\n\n\nJSON Syntax:\n\n{\n  \"Action\": {\n    \"Operation\": \"string\",\n    \"Parameters\": {\"string\": \"string\"\n      ...}\n  },\n  \"ConditionExpressions\": [\n    {\n      \"Condition\": \"string\",\n      \"Value\": \"string\",\n      \"TargetColumn\": \"string\"\n    }\n    ...\n  ]\n}\n\n\n--step-index (integer)\n\nThe index from which to preview a step. This index is used to preview the result of steps that have already been applied, so that the resulting view frame is from earlier in the view frame stack.\n\n--client-session-id (string)\n\nA unique identifier for an interactive session that’s currently open and ready for work. The action will be performed on this session.\n\n--view-frame (structure)\n\nRepresents the data being transformed during an action.\n\nStartColumnIndex -> (integer)\n\nThe starting index for the range of columns to return in the view frame.\n\nColumnRange -> (integer)\n\nThe number of columns to include in the view frame, beginning with the StartColumnIndex value and ignoring any columns in the HiddenColumns list.\n\nHiddenColumns -> (list)\n\nA list of columns to hide in the view frame.\n\n(string)\n\nShorthand Syntax:\n\nStartColumnIndex=integer,ColumnRange=integer,HiddenColumns=string,string\n\n\nJSON Syntax:\n\n{\n  \"StartColumnIndex\": integer,\n  \"ColumnRange\": integer,\n  \"HiddenColumns\": [\"string\", ...]\n}\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nResult -> (string)\n\nA message indicating the result of performing the action.\n\nName -> (string)\n\nThe name of the project that was affected by the action.\n\nActionId -> (integer)\n\nA unique identifier for the action that was performed."
    },
    {
      "command_name": "start-job-run",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/start-job-run.html",
      "command_description": "Description\n\nRuns a DataBrew job.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  start-job-run\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the job to be run.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nRunId -> (string)\n\nA system-generated identifier for this particular job run."
    },
    {
      "command_name": "start-project-session",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/start-project-session.html",
      "command_description": "Description\n\nCreates an interactive session, enabling you to manipulate data in a DataBrew project.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  start-project-session\n--name <value>\n[--assume-control | --no-assume-control]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--assume-control | --no-assume-control]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the project to act upon.\n\n--assume-control | --no-assume-control (boolean)\n\nA value that, if true, enables you to take control of a session, even if a different client is currently accessing the project.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the project to be acted upon.\n\nClientSessionId -> (string)\n\nA system-generated identifier for the session."
    },
    {
      "command_name": "stop-job-run",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/stop-job-run.html",
      "command_description": "Description\n\nStops a particular run of a job.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  stop-job-run\n--name <value>\n--run-id <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "--run-id <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the job to be stopped.\n\n--run-id (string)\n\nThe ID of the job run to be stopped.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nRunId -> (string)\n\nThe ID of the job run that you stopped."
    },
    {
      "command_name": "tag-resource",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/tag-resource.html",
      "command_description": "Description\n\nAdds metadata tags to a DataBrew resource, such as a dataset, project, recipe, job, or schedule.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  tag-resource\n--resource-arn <value>\n--tags <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--resource-arn <value>",
        "--tags <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--resource-arn (string)\n\nThe DataBrew resource to which tags should be added. The value for this parameter is an Amazon Resource Name (ARN). For DataBrew, you can tag a dataset, a job, a project, or a recipe.\n\n--tags (map)\n\nOne or more tags to be assigned to the resource.\n\nkey -> (string)\n\nvalue -> (string)\n\nShorthand Syntax:\n\nKeyName1=string,KeyName2=string\n\n\nJSON Syntax:\n\n{\"string\": \"string\"\n  ...}\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nNone"
    },
    {
      "command_name": "untag-resource",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/untag-resource.html",
      "command_description": "Description\n\nRemoves metadata tags from a DataBrew resource.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  untag-resource\n--resource-arn <value>\n--tag-keys <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--resource-arn <value>",
        "--tag-keys <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--resource-arn (string)\n\nA DataBrew resource from which you want to remove a tag or tags. The value for this parameter is an Amazon Resource Name (ARN).\n\n--tag-keys (list)\n\nThe tag keys (names) of one or more tags to be removed.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nNone"
    },
    {
      "command_name": "update-dataset",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/update-dataset.html",
      "command_description": "Description\n\nModifies the definition of an existing DataBrew dataset.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  update-dataset\n--name <value>\n[--format <value>]\n[--format-options <value>]\n--input <value>\n[--path-options <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--name <value>",
        "[--format <value>]",
        "[--format-options <value>]",
        "--input <value>",
        "[--path-options <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--name (string)\n\nThe name of the dataset to be updated.\n\n--format (string)\n\nThe file format of a dataset that is created from an Amazon S3 file or folder.\n\nPossible values:\n\nCSV\n\nJSON\n\nPARQUET\n\nEXCEL\n\n--format-options (structure)\n\nRepresents a set of options that define the structure of either comma-separated value (CSV), Excel, or JSON input.\n\nJson -> (structure)\n\nOptions that define how JSON input is to be interpreted by DataBrew.\n\nMultiLine -> (boolean)\n\nA value that specifies whether JSON input contains embedded new line characters.\n\nExcel -> (structure)\n\nOptions that define how Excel input is to be interpreted by DataBrew.\n\nSheetNames -> (list)\n\nOne or more named sheets in the Excel file that will be included in the dataset.\n\n(string)\n\nSheetIndexes -> (list)\n\nOne or more sheet numbers in the Excel file that will be included in the dataset.\n\n(integer)\n\nHeaderRow -> (boolean)\n\nA variable that specifies whether the first row in the file is parsed as the header. If this value is false, column names are auto-generated.\n\nCsv -> (structure)\n\nOptions that define how CSV input is to be interpreted by DataBrew.\n\nDelimiter -> (string)\n\nA single character that specifies the delimiter being used in the CSV file.\n\nHeaderRow -> (boolean)\n\nA variable that specifies whether the first row in the file is parsed as the header. If this value is false, column names are auto-generated.\n\nShorthand Syntax:\n\nJson={MultiLine=boolean},Excel={SheetNames=[string,string],SheetIndexes=[integer,integer],HeaderRow=boolean},Csv={Delimiter=string,HeaderRow=boolean}\n\n\nJSON Syntax:\n\n{\n  \"Json\": {\n    \"MultiLine\": true|false\n  },\n  \"Excel\": {\n    \"SheetNames\": [\"string\", ...],\n    \"SheetIndexes\": [integer, ...],\n    \"HeaderRow\": true|false\n  },\n  \"Csv\": {\n    \"Delimiter\": \"string\",\n    \"HeaderRow\": true|false\n  }\n}\n\n\n--input (structure)\n\nRepresents information on how DataBrew can find data, in either the Glue Data Catalog or Amazon S3.\n\nS3InputDefinition -> (structure)\n\nThe Amazon S3 location where the data is stored.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDataCatalogInputDefinition -> (structure)\n\nThe Glue Data Catalog parameters for the data.\n\nCatalogId -> (string)\n\nThe unique identifier of the Amazon Web Services account that holds the Data Catalog that stores the data.\n\nDatabaseName -> (string)\n\nThe name of a database in the Data Catalog.\n\nTableName -> (string)\n\nThe name of a database table in the Data Catalog. This table corresponds to a DataBrew dataset.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon location where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDatabaseInputDefinition -> (structure)\n\nConnection information for dataset input files stored in a database.\n\nGlueConnectionName -> (string)\n\nThe Glue Connection that stores the connection information for the target database.\n\nDatabaseTableName -> (string)\n\nThe table within the target database.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can read input data, or write output from a job.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nShorthand Syntax:\n\nS3InputDefinition={Bucket=string,Key=string},DataCatalogInputDefinition={CatalogId=string,DatabaseName=string,TableName=string,TempDirectory={Bucket=string,Key=string}},DatabaseInputDefinition={GlueConnectionName=string,DatabaseTableName=string,TempDirectory={Bucket=string,Key=string}}\n\n\nJSON Syntax:\n\n{\n  \"S3InputDefinition\": {\n    \"Bucket\": \"string\",\n    \"Key\": \"string\"\n  },\n  \"DataCatalogInputDefinition\": {\n    \"CatalogId\": \"string\",\n    \"DatabaseName\": \"string\",\n    \"TableName\": \"string\",\n    \"TempDirectory\": {\n      \"Bucket\": \"string\",\n      \"Key\": \"string\"\n    }\n  },\n  \"DatabaseInputDefinition\": {\n    \"GlueConnectionName\": \"string\",\n    \"DatabaseTableName\": \"string\",\n    \"TempDirectory\": {\n      \"Bucket\": \"string\",\n      \"Key\": \"string\"\n    }\n  }\n}\n\n\n--path-options (structure)\n\nA set of options that defines how DataBrew interprets an Amazon S3 path of the dataset.\n\nLastModifiedDateCondition -> (structure)\n\nIf provided, this structure defines a date range for matching Amazon S3 objects based on their LastModifiedDate attribute in Amazon S3.\n\nExpression -> (string)\n\nThe expression which includes condition names followed by substitution variables, possibly grouped and combined with other conditions. For example, “(starts_with :prefix1 or starts_with :prefix2) and (ends_with :suffix1 or ends_with :suffix2)”. Substitution variables should start with ‘:’ symbol.\n\nValuesMap -> (map)\n\nThe map of substitution variable names to their values used in this filter expression.\n\nkey -> (string)\n\nvalue -> (string)\n\nFilesLimit -> (structure)\n\nIf provided, this structure imposes a limit on a number of files that should be selected.\n\nMaxFiles -> (integer)\n\nThe number of Amazon S3 files to select.\n\nOrderedBy -> (string)\n\nA criteria to use for Amazon S3 files sorting before their selection. By default uses LAST_MODIFIED_DATE as a sorting criteria. Currently it’s the only allowed value.\n\nOrder -> (string)\n\nA criteria to use for Amazon S3 files sorting before their selection. By default uses DESCENDING order, i.e. most recent files are selected first. Anotherpossible value is ASCENDING.\n\nParameters -> (map)\n\nA structure that maps names of parameters used in the Amazon S3 path of a dataset to their definitions.\n\nkey -> (string)\n\nvalue -> (structure)\n\nRepresents a dataset paramater that defines type and conditions for a parameter in the Amazon S3 path of the dataset.\n\nName -> (string)\n\nThe name of the parameter that is used in the dataset’s Amazon S3 path.\n\nType -> (string)\n\nThe type of the dataset parameter, can be one of a ‘String’, ‘Number’ or ‘Datetime’.\n\nDatetimeOptions -> (structure)\n\nAdditional parameter options such as a format and a timezone. Required for datetime parameters.\n\nFormat -> (string)\n\nRequired option, that defines the datetime format used for a date parameter in the Amazon S3 path. Should use only supported datetime specifiers and separation characters, all literal a-z or A-Z characters should be escaped with single quotes. E.g. “MM.dd.yyyy-‘at’-HH:mm”.\n\nTimezoneOffset -> (string)\n\nOptional value for a timezone offset of the datetime parameter value in the Amazon S3 path. Shouldn’t be used if Format for this parameter includes timezone fields. If no offset specified, UTC is assumed.\n\nLocaleCode -> (string)\n\nOptional value for a non-US locale code, needed for correct interpretation of some date formats.\n\nCreateColumn -> (boolean)\n\nOptional boolean value that defines whether the captured value of this parameter should be used to create a new column in a dataset.\n\nFilter -> (structure)\n\nThe optional filter expression structure to apply additional matching criteria to the parameter.\n\nExpression -> (string)\n\nThe expression which includes condition names followed by substitution variables, possibly grouped and combined with other conditions. For example, “(starts_with :prefix1 or starts_with :prefix2) and (ends_with :suffix1 or ends_with :suffix2)”. Substitution variables should start with ‘:’ symbol.\n\nValuesMap -> (map)\n\nThe map of substitution variable names to their values used in this filter expression.\n\nkey -> (string)\n\nvalue -> (string)\n\nShorthand Syntax:\n\nLastModifiedDateCondition={Expression=string,ValuesMap={KeyName1=string,KeyName2=string}},FilesLimit={MaxFiles=integer,OrderedBy=string,Order=string},Parameters={KeyName1={Name=string,Type=string,DatetimeOptions={Format=string,TimezoneOffset=string,LocaleCode=string},CreateColumn=boolean,Filter={Expression=string,ValuesMap={KeyName1=string,KeyName2=string}}},KeyName2={Name=string,Type=string,DatetimeOptions={Format=string,TimezoneOffset=string,LocaleCode=string},CreateColumn=boolean,Filter={Expression=string,ValuesMap={KeyName1=string,KeyName2=string}}}}\n\n\nJSON Syntax:\n\n{\n  \"LastModifiedDateCondition\": {\n    \"Expression\": \"string\",\n    \"ValuesMap\": {\"string\": \"string\"\n      ...}\n  },\n  \"FilesLimit\": {\n    \"MaxFiles\": integer,\n    \"OrderedBy\": \"LAST_MODIFIED_DATE\",\n    \"Order\": \"DESCENDING\"|\"ASCENDING\"\n  },\n  \"Parameters\": {\"string\": {\n        \"Name\": \"string\",\n        \"Type\": \"Datetime\"|\"Number\"|\"String\",\n        \"DatetimeOptions\": {\n          \"Format\": \"string\",\n          \"TimezoneOffset\": \"string\",\n          \"LocaleCode\": \"string\"\n        },\n        \"CreateColumn\": true|false,\n        \"Filter\": {\n          \"Expression\": \"string\",\n          \"ValuesMap\": {\"string\": \"string\"\n            ...}\n        }\n      }\n    ...}\n}\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the dataset that you updated."
    },
    {
      "command_name": "update-profile-job",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/update-profile-job.html",
      "command_description": "Description\n\nModifies the definition of an existing profile job.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  update-profile-job\n[--configuration <value>]\n[--encryption-key-arn <value>]\n[--encryption-mode <value>]\n--name <value>\n[--log-subscription <value>]\n[--max-capacity <value>]\n[--max-retries <value>]\n--output-location <value>\n--role-arn <value>\n[--timeout <value>]\n[--job-sample <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--configuration <value>]",
        "[--encryption-key-arn <value>]",
        "[--encryption-mode <value>]",
        "--name <value>",
        "[--log-subscription <value>]",
        "[--max-capacity <value>]",
        "[--max-retries <value>]",
        "--output-location <value>",
        "--role-arn <value>",
        "[--timeout <value>]",
        "[--job-sample <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--configuration (structure)\n\nConfiguration for profile jobs. Used to select columns, do evaluations, and override default parameters of evaluations. When configuration is null, the profile job will run with default settings.\n\nDatasetStatisticsConfiguration -> (structure)\n\nConfiguration for inter-column evaluations. Configuration can be used to select evaluations and override parameters of evaluations. When configuration is undefined, the profile job will run all supported inter-column evaluations.\n\nIncludedStatistics -> (list)\n\nList of included evaluations. When the list is undefined, all supported evaluations will be included.\n\n(string)\n\nOverrides -> (list)\n\nList of overrides for evaluations.\n\n(structure)\n\nOverride of a particular evaluation for a profile job.\n\nStatistic -> (string)\n\nThe name of an evaluation\n\nParameters -> (map)\n\nA map that includes overrides of an evaluation’s parameters.\n\nkey -> (string)\n\nvalue -> (string)\n\nProfileColumns -> (list)\n\nList of column selectors. ProfileColumns can be used to select columns from the dataset. When ProfileColumns is undefined, the profile job will profile all supported columns.\n\n(structure)\n\nSelector of a column from a dataset for profile job configuration. One selector includes either a column name or a regular expression.\n\nRegex -> (string)\n\nA regular expression for selecting a column from a dataset.\n\nName -> (string)\n\nThe name of a column from a dataset.\n\nColumnStatisticsConfigurations -> (list)\n\nList of configurations for column evaluations. ColumnStatisticsConfigurations are used to select evaluations and override parameters of evaluations for particular columns. When ColumnStatisticsConfigurations is undefined, the profile job will profile all supported columns and run all supported evaluations.\n\n(structure)\n\nConfiguration for column evaluations for a profile job. ColumnStatisticsConfiguration can be used to select evaluations and override parameters of evaluations for particular columns.\n\nSelectors -> (list)\n\nList of column selectors. Selectors can be used to select columns from the dataset. When selectors are undefined, configuration will be applied to all supported columns.\n\n(structure)\n\nSelector of a column from a dataset for profile job configuration. One selector includes either a column name or a regular expression.\n\nRegex -> (string)\n\nA regular expression for selecting a column from a dataset.\n\nName -> (string)\n\nThe name of a column from a dataset.\n\nStatistics -> (structure)\n\nConfiguration for evaluations. Statistics can be used to select evaluations and override parameters of evaluations.\n\nIncludedStatistics -> (list)\n\nList of included evaluations. When the list is undefined, all supported evaluations will be included.\n\n(string)\n\nOverrides -> (list)\n\nList of overrides for evaluations.\n\n(structure)\n\nOverride of a particular evaluation for a profile job.\n\nStatistic -> (string)\n\nThe name of an evaluation\n\nParameters -> (map)\n\nA map that includes overrides of an evaluation’s parameters.\n\nkey -> (string)\n\nvalue -> (string)\n\nJSON Syntax:\n\n{\n  \"DatasetStatisticsConfiguration\": {\n    \"IncludedStatistics\": [\"string\", ...],\n    \"Overrides\": [\n      {\n        \"Statistic\": \"string\",\n        \"Parameters\": {\"string\": \"string\"\n          ...}\n      }\n      ...\n    ]\n  },\n  \"ProfileColumns\": [\n    {\n      \"Regex\": \"string\",\n      \"Name\": \"string\"\n    }\n    ...\n  ],\n  \"ColumnStatisticsConfigurations\": [\n    {\n      \"Selectors\": [\n        {\n          \"Regex\": \"string\",\n          \"Name\": \"string\"\n        }\n        ...\n      ],\n      \"Statistics\": {\n        \"IncludedStatistics\": [\"string\", ...],\n        \"Overrides\": [\n          {\n            \"Statistic\": \"string\",\n            \"Parameters\": {\"string\": \"string\"\n              ...}\n          }\n          ...\n        ]\n      }\n    }\n    ...\n  ]\n}\n\n\n--encryption-key-arn (string)\n\nThe Amazon Resource Name (ARN) of an encryption key that is used to protect the job.\n\n--encryption-mode (string)\n\nThe encryption mode for the job, which can be one of the following:\n\nSSE-KMS - Server-side encryption with keys managed by KMS.\n\nSSE-S3 - Server-side encryption with keys managed by Amazon S3.\n\nPossible values:\n\nSSE-KMS\n\nSSE-S3\n\n--name (string)\n\nThe name of the job to be updated.\n\n--log-subscription (string)\n\nEnables or disables Amazon CloudWatch logging for the job. If logging is enabled, CloudWatch writes one log stream for each job run.\n\nPossible values:\n\nENABLE\n\nDISABLE\n\n--max-capacity (integer)\n\nThe maximum number of compute nodes that DataBrew can use when the job processes data.\n\n--max-retries (integer)\n\nThe maximum number of times to retry the job after a job run fails.\n\n--output-location (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can read input data, or write output from a job.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nShorthand Syntax:\n\nBucket=string,Key=string\n\n\nJSON Syntax:\n\n{\n  \"Bucket\": \"string\",\n  \"Key\": \"string\"\n}\n\n\n--role-arn (string)\n\nThe Amazon Resource Name (ARN) of the Identity and Access Management (IAM) role to be assumed when DataBrew runs the job.\n\n--timeout (integer)\n\nThe job’s timeout in minutes. A job that attempts to run longer than this timeout period ends with a status of TIMEOUT .\n\n--job-sample (structure)\n\nSample configuration for Profile Jobs only. Determines the number of rows on which the Profile job will be executed. If a JobSample value is not provided for profile jobs, the default value will be used. The default value is CUSTOM_ROWS for the mode parameter and 20000 for the size parameter.\n\nMode -> (string)\n\nA value that determines whether the profile job is run on the entire dataset or a specified number of rows. This value must be one of the following:\n\nFULL_DATASET - The profile job is run on the entire dataset.\n\nCUSTOM_ROWS - The profile job is run on the number of rows specified in the Size parameter.\n\nSize -> (long)\n\nThe Size parameter is only required when the mode is CUSTOM_ROWS. The profile job is run on the specified number of rows. The maximum value for size is Long.MAX_VALUE.\n\nLong.MAX_VALUE = 9223372036854775807\n\nShorthand Syntax:\n\nMode=string,Size=long\n\n\nJSON Syntax:\n\n{\n  \"Mode\": \"FULL_DATASET\"|\"CUSTOM_ROWS\",\n  \"Size\": long\n}\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the job that was updated."
    },
    {
      "command_name": "update-project",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/update-project.html",
      "command_description": "Description\n\nModifies the definition of an existing DataBrew project.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  update-project\n[--sample <value>]\n--role-arn <value>\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--sample <value>]",
        "--role-arn <value>",
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--sample (structure)\n\nRepresents the sample size and sampling type for DataBrew to use for interactive data analysis.\n\nSize -> (integer)\n\nThe number of rows in the sample.\n\nType -> (string)\n\nThe way in which DataBrew obtains rows from a dataset.\n\nShorthand Syntax:\n\nSize=integer,Type=string\n\n\nJSON Syntax:\n\n{\n  \"Size\": integer,\n  \"Type\": \"FIRST_N\"|\"LAST_N\"|\"RANDOM\"\n}\n\n\n--role-arn (string)\n\nThe Amazon Resource Name (ARN) of the IAM role to be assumed for this request.\n\n--name (string)\n\nThe name of the project to be updated.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nLastModifiedDate -> (timestamp)\n\nThe date and time that the project was last modified.\n\nName -> (string)\n\nThe name of the project that you updated."
    },
    {
      "command_name": "update-recipe",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/update-recipe.html",
      "command_description": "Description\n\nModifies the definition of the LATEST_WORKING version of a DataBrew recipe.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  update-recipe\n[--description <value>]\n--name <value>\n[--steps <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--description <value>]",
        "--name <value>",
        "[--steps <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--description (string)\n\nA description of the recipe.\n\n--name (string)\n\nThe name of the recipe to be updated.\n\n--steps (list)\n\nOne or more steps to be performed by the recipe. Each step consists of an action, and the conditions under which the action should succeed.\n\n(structure)\n\nRepresents a single step from a DataBrew recipe to be performed.\n\nAction -> (structure)\n\nThe particular action to be performed in the recipe step.\n\nOperation -> (string)\n\nThe name of a valid DataBrew transformation to be performed on the data.\n\nParameters -> (map)\n\nContextual parameters for the transformation.\n\nkey -> (string)\n\nvalue -> (string)\n\nConditionExpressions -> (list)\n\nOne or more conditions that must be met for the recipe step to succeed.\n\nNote\n\nAll of the conditions in the array must be met. In other words, all of the conditions must be combined using a logical AND operation.\n\n(structure)\n\nRepresents an individual condition that evaluates to true or false.\n\nConditions are used with recipe actions. The action is only performed for column values where the condition evaluates to true.\n\nIf a recipe requires more than one condition, then the recipe must specify multiple ConditionExpression elements. Each condition is applied to the rows in a dataset first, before the recipe action is performed.\n\nCondition -> (string)\n\nA specific condition to apply to a recipe action. For more information, see Recipe structure in the Glue DataBrew Developer Guide .\n\nValue -> (string)\n\nA value that the condition must evaluate to for the condition to succeed.\n\nTargetColumn -> (string)\n\nA column to apply this condition to.\n\nShorthand Syntax:\n\nAction={Operation=string,Parameters={KeyName1=string,KeyName2=string}},ConditionExpressions=[{Condition=string,Value=string,TargetColumn=string},{Condition=string,Value=string,TargetColumn=string}] ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Action\": {\n      \"Operation\": \"string\",\n      \"Parameters\": {\"string\": \"string\"\n        ...}\n    },\n    \"ConditionExpressions\": [\n      {\n        \"Condition\": \"string\",\n        \"Value\": \"string\",\n        \"TargetColumn\": \"string\"\n      }\n      ...\n    ]\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the recipe that was updated."
    },
    {
      "command_name": "update-recipe-job",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/update-recipe-job.html",
      "command_description": "Description\n\nModifies the definition of an existing DataBrew recipe job.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  update-recipe-job\n[--encryption-key-arn <value>]\n[--encryption-mode <value>]\n--name <value>\n[--log-subscription <value>]\n[--max-capacity <value>]\n[--max-retries <value>]\n[--outputs <value>]\n[--data-catalog-outputs <value>]\n[--database-outputs <value>]\n--role-arn <value>\n[--timeout <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--encryption-key-arn <value>]",
        "[--encryption-mode <value>]",
        "--name <value>",
        "[--log-subscription <value>]",
        "[--max-capacity <value>]",
        "[--max-retries <value>]",
        "[--outputs <value>]",
        "[--data-catalog-outputs <value>]",
        "[--database-outputs <value>]",
        "--role-arn <value>",
        "[--timeout <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--encryption-key-arn (string)\n\nThe Amazon Resource Name (ARN) of an encryption key that is used to protect the job.\n\n--encryption-mode (string)\n\nThe encryption mode for the job, which can be one of the following:\n\nSSE-KMS - Server-side encryption with keys managed by KMS.\n\nSSE-S3 - Server-side encryption with keys managed by Amazon S3.\n\nPossible values:\n\nSSE-KMS\n\nSSE-S3\n\n--name (string)\n\nThe name of the job to update.\n\n--log-subscription (string)\n\nEnables or disables Amazon CloudWatch logging for the job. If logging is enabled, CloudWatch writes one log stream for each job run.\n\nPossible values:\n\nENABLE\n\nDISABLE\n\n--max-capacity (integer)\n\nThe maximum number of nodes that DataBrew can consume when the job processes data.\n\n--max-retries (integer)\n\nThe maximum number of times to retry the job after a job run fails.\n\n--outputs (list)\n\nOne or more artifacts that represent the output from running the job.\n\n(structure)\n\nRepresents options that specify how and where in Amazon S3 DataBrew writes the output generated by recipe jobs or profile jobs.\n\nCompressionFormat -> (string)\n\nThe compression algorithm used to compress the output text of the job.\n\nFormat -> (string)\n\nThe data format of the output of the job.\n\nPartitionColumns -> (list)\n\nThe names of one or more partition columns for the output of the job.\n\n(string)\n\nLocation -> (structure)\n\nThe location in Amazon S3 where the job writes its output.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nOverwrite -> (boolean)\n\nA value that, if true, means that any data in the location specified for output is overwritten with new output.\n\nFormatOptions -> (structure)\n\nRepresents options that define how DataBrew formats job output files.\n\nCsv -> (structure)\n\nRepresents a set of options that define the structure of comma-separated value (CSV) job output.\n\nDelimiter -> (string)\n\nA single character that specifies the delimiter used to create CSV job output.\n\nShorthand Syntax:\n\nCompressionFormat=string,Format=string,PartitionColumns=string,string,Location={Bucket=string,Key=string},Overwrite=boolean,FormatOptions={Csv={Delimiter=string}} ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"CompressionFormat\": \"GZIP\"|\"LZ4\"|\"SNAPPY\"|\"BZIP2\"|\"DEFLATE\"|\"LZO\"|\"BROTLI\"|\"ZSTD\"|\"ZLIB\",\n    \"Format\": \"CSV\"|\"JSON\"|\"PARQUET\"|\"GLUEPARQUET\"|\"AVRO\"|\"ORC\"|\"XML\"|\"TABLEAUHYPER\",\n    \"PartitionColumns\": [\"string\", ...],\n    \"Location\": {\n      \"Bucket\": \"string\",\n      \"Key\": \"string\"\n    },\n    \"Overwrite\": true|false,\n    \"FormatOptions\": {\n      \"Csv\": {\n        \"Delimiter\": \"string\"\n      }\n    }\n  }\n  ...\n]\n\n\n--data-catalog-outputs (list)\n\nOne or more artifacts that represent the Glue Data Catalog output from running the job.\n\n(structure)\n\nRepresents options that specify how and where in the Glue Data Catalog DataBrew writes the output generated by recipe jobs.\n\nCatalogId -> (string)\n\nThe unique identifier of the Amazon Web Services account that holds the Data Catalog that stores the data.\n\nDatabaseName -> (string)\n\nThe name of a database in the Data Catalog.\n\nTableName -> (string)\n\nThe name of a table in the Data Catalog.\n\nS3Options -> (structure)\n\nRepresents options that specify how and where DataBrew writes the Amazon S3 output generated by recipe jobs.\n\nLocation -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can write output from a job.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nDatabaseOptions -> (structure)\n\nRepresents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nTableName -> (string)\n\nA prefix for the name of a table DataBrew will create in the database.\n\nOverwrite -> (boolean)\n\nA value that, if true, means that any data in the location specified for output is overwritten with new output. Not supported with DatabaseOptions.\n\nShorthand Syntax:\n\nCatalogId=string,DatabaseName=string,TableName=string,S3Options={Location={Bucket=string,Key=string}},DatabaseOptions={TempDirectory={Bucket=string,Key=string},TableName=string},Overwrite=boolean ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"CatalogId\": \"string\",\n    \"DatabaseName\": \"string\",\n    \"TableName\": \"string\",\n    \"S3Options\": {\n      \"Location\": {\n        \"Bucket\": \"string\",\n        \"Key\": \"string\"\n      }\n    },\n    \"DatabaseOptions\": {\n      \"TempDirectory\": {\n        \"Bucket\": \"string\",\n        \"Key\": \"string\"\n      },\n      \"TableName\": \"string\"\n    },\n    \"Overwrite\": true|false\n  }\n  ...\n]\n\n\n--database-outputs (list)\n\nRepresents a list of JDBC database output objects which defines the output destination for a DataBrew recipe job to write into.\n\n(structure)\n\nRepresents a JDBC database output object which defines the output destination for a DataBrew recipe job to write into.\n\nGlueConnectionName -> (string)\n\nThe Glue connection that stores the connection information for the target database.\n\nDatabaseOptions -> (structure)\n\nRepresents options that specify how and where DataBrew writes the database output generated by recipe jobs.\n\nTempDirectory -> (structure)\n\nRepresents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.\n\nBucket -> (string)\n\nThe Amazon S3 bucket name.\n\nKey -> (string)\n\nThe unique name of the object in the bucket.\n\nTableName -> (string)\n\nA prefix for the name of a table DataBrew will create in the database.\n\nDatabaseOutputMode -> (string)\n\nThe output mode to write into the database. Currently supported option: NEW_TABLE.\n\nShorthand Syntax:\n\nGlueConnectionName=string,DatabaseOptions={TempDirectory={Bucket=string,Key=string},TableName=string},DatabaseOutputMode=string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"GlueConnectionName\": \"string\",\n    \"DatabaseOptions\": {\n      \"TempDirectory\": {\n        \"Bucket\": \"string\",\n        \"Key\": \"string\"\n      },\n      \"TableName\": \"string\"\n    },\n    \"DatabaseOutputMode\": \"NEW_TABLE\"\n  }\n  ...\n]\n\n\n--role-arn (string)\n\nThe Amazon Resource Name (ARN) of the Identity and Access Management (IAM) role to be assumed when DataBrew runs the job.\n\n--timeout (integer)\n\nThe job’s timeout in minutes. A job that attempts to run longer than this timeout period ends with a status of TIMEOUT .\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the job that you updated."
    },
    {
      "command_name": "update-schedule",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/databrew/update-schedule.html",
      "command_description": "Description\n\nModifies the definition of an existing DataBrew schedule.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  update-schedule\n[--job-names <value>]\n--cron-expression <value>\n--name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--job-names <value>]",
        "--cron-expression <value>",
        "--name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--job-names (list)\n\nThe name or names of one or more jobs to be run for this schedule.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--cron-expression (string)\n\nThe date or dates and time or times when the jobs are to be run. For more information, see Cron expressions in the Glue DataBrew Developer Guide .\n\n--name (string)\n\nThe name of the schedule to update.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nName -> (string)\n\nThe name of the schedule that was updated."
    }
  ],
  "service_description": "Description\n\nGlue DataBrew is a visual, cloud-scale data-preparation service. DataBrew simplifies data preparation tasks, targeting data issues that are hard to spot and time-consuming to fix. DataBrew empowers users of all technical levels to visualize the data and perform one-click data transformations, with no coding required."
}