{
  "service_name": "dms",
  "service_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/index.html",
  "service_commands": [
    {
      "command_name": "add-tags-to-resource",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/add-tags-to-resource.html",
      "command_description": "Description\n\nAdds metadata tags to an DMS resource, including replication instance, endpoint, security group, and migration task. These tags can also be used with cost allocation reporting to track cost associated with DMS resources, or used in a Condition statement in an IAM policy for DMS. For more information, see ` Tag https://docs.aws.amazon.com/dms/latest/APIReference/API_Tag.html`__ data type description.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  add-tags-to-resource\n--resource-arn <value>\n--tags <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--resource-arn <value>",
        "--tags <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--resource-arn (string)\n\nIdentifies the DMS resource to which tags should be added. The value for this parameter is an Amazon Resource Name (ARN).\n\nFor DMS, you can tag a replication instance, an endpoint, or a replication task.\n\n--tags (list)\n\nOne or more tags to be assigned to the resource.\n\n(structure)\n\nA user-defined key-value pair that describes metadata added to an DMS resource and that is used by operations such as the following:\n\nAddTagsToResource\n\nListTagsForResource\n\nRemoveTagsFromResource\n\nKey -> (string)\n\nA key is the required name of the tag. The string value can be 1-128 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nValue -> (string)\n\nA value is the optional value of the tag. The string value can be 1-256 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the resource for which the tag is created.\n\nShorthand Syntax:\n\nKey=string,Value=string,ResourceArn=string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Key\": \"string\",\n    \"Value\": \"string\",\n    \"ResourceArn\": \"string\"\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nNone",
      "command_examples": "Examples\n\nTo add tags to a resource\n\nThe following add-tags-to-resource example adds tags to a replication instance.\n\naws dms add-tags-to-resource \\\n    --resource-arn arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE \\\n    --tags Key=Environment,Value=PROD Key=Project,Value=dbMigration\n\n\nThis command produces no output.\n\nFor more information, see Tagging Resources in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "apply-pending-maintenance-action",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/apply-pending-maintenance-action.html",
      "command_description": "Description\n\nApplies a pending maintenance action to a resource (for example, to a replication instance).\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  apply-pending-maintenance-action\n--replication-instance-arn <value>\n--apply-action <value>\n--opt-in-type <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-instance-arn <value>",
        "--apply-action <value>",
        "--opt-in-type <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-instance-arn (string)\n\nThe Amazon Resource Name (ARN) of the DMS resource that the pending maintenance action applies to.\n\n--apply-action (string)\n\nThe pending maintenance action to apply to this resource.\n\nValid values: os-upgrade , system-update , db-upgrade\n\n--opt-in-type (string)\n\nA value that specifies the type of opt-in request, or undoes an opt-in request. You can’t undo an opt-in request of type immediate .\n\nValid values:\n\nimmediate - Apply the maintenance action immediately.\n\nnext-maintenance - Apply the maintenance action during the next maintenance window for the resource.\n\nundo-opt-in - Cancel any existing next-maintenance opt-in requests.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nResourcePendingMaintenanceActions -> (structure)\n\nThe DMS resource that the pending maintenance action will be applied to.\n\nResourceIdentifier -> (string)\n\nThe Amazon Resource Name (ARN) of the DMS resource that the pending maintenance action applies to. For information about creating an ARN, see Constructing an Amazon Resource Name (ARN) for DMS in the DMS documentation.\n\nPendingMaintenanceActionDetails -> (list)\n\nDetailed information about the pending maintenance action.\n\n(structure)\n\nDescribes a maintenance action pending for an DMS resource, including when and how it will be applied. This data type is a response element to the DescribePendingMaintenanceActions operation.\n\nAction -> (string)\n\nThe type of pending maintenance action that is available for the resource.\n\nAutoAppliedAfterDate -> (timestamp)\n\nThe date of the maintenance window when the action is to be applied. The maintenance action is applied to the resource during its first maintenance window after this date. If this date is specified, any next-maintenance opt-in requests are ignored.\n\nForcedApplyDate -> (timestamp)\n\nThe date when the maintenance action will be automatically applied. The maintenance action is applied to the resource on this date regardless of the maintenance window for the resource. If this date is specified, any immediate opt-in requests are ignored.\n\nOptInStatus -> (string)\n\nThe type of opt-in request that has been received for the resource.\n\nCurrentApplyDate -> (timestamp)\n\nThe effective date when the pending maintenance action will be applied to the resource. This date takes into account opt-in requests received from the ApplyPendingMaintenanceAction API operation, and also the AutoAppliedAfterDate and ForcedApplyDate parameter values. This value is blank if an opt-in request has not been received and nothing has been specified for AutoAppliedAfterDate or ForcedApplyDate .\n\nDescription -> (string)\n\nA description providing more detail about the maintenance action."
    },
    {
      "command_name": "cancel-replication-task-assessment-run",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/cancel-replication-task-assessment-run.html",
      "command_description": "Description\n\nCancels a single premigration assessment run.\n\nThis operation prevents any individual assessments from running if they haven’t started running. It also attempts to cancel any individual assessments that are currently running.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  cancel-replication-task-assessment-run\n--replication-task-assessment-run-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-task-assessment-run-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-assessment-run-arn (string)\n\nAmazon Resource Name (ARN) of the premigration assessment run to be canceled.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationTaskAssessmentRun -> (structure)\n\nThe ReplicationTaskAssessmentRun object for the canceled assessment run.\n\nReplicationTaskAssessmentRunArn -> (string)\n\nAmazon Resource Name (ARN) of this assessment run.\n\nReplicationTaskArn -> (string)\n\nARN of the migration task associated with this premigration assessment run.\n\nStatus -> (string)\n\nAssessment run status.\n\nThis status can have one of the following values:\n\n\"cancelling\" – The assessment run was canceled by the CancelReplicationTaskAssessmentRun operation.\n\n\"deleting\" – The assessment run was deleted by the DeleteReplicationTaskAssessmentRun operation.\n\n\"failed\" – At least one individual assessment completed with a failed status.\n\n\"error-provisioning\" – An internal error occurred while resources were provisioned (during provisioning status).\n\n\"error-executing\" – An internal error occurred while individual assessments ran (during running status).\n\n\"invalid state\" – The assessment run is in an unknown state.\n\n\"passed\" – All individual assessments have completed, and none has a failed status.\n\n\"provisioning\" – Resources required to run individual assessments are being provisioned.\n\n\"running\" – Individual assessments are being run.\n\n\"starting\" – The assessment run is starting, but resources are not yet being provisioned for individual assessments.\n\nReplicationTaskAssessmentRunCreationDate -> (timestamp)\n\nDate on which the assessment run was created using the StartReplicationTaskAssessmentRun operation.\n\nAssessmentProgress -> (structure)\n\nIndication of the completion progress for the individual assessments specified to run.\n\nIndividualAssessmentCount -> (integer)\n\nThe number of individual assessments that are specified to run.\n\nIndividualAssessmentCompletedCount -> (integer)\n\nThe number of individual assessments that have completed, successfully or not.\n\nLastFailureMessage -> (string)\n\nLast message generated by an individual assessment failure.\n\nServiceAccessRoleArn -> (string)\n\nARN of the service role used to start the assessment run using the StartReplicationTaskAssessmentRun operation. The role must allow the iam:PassRole action.\n\nResultLocationBucket -> (string)\n\nAmazon S3 bucket where DMS stores the results of this assessment run.\n\nResultLocationFolder -> (string)\n\nFolder in an Amazon S3 bucket where DMS stores the results of this assessment run.\n\nResultEncryptionMode -> (string)\n\nEncryption mode used to encrypt the assessment run results.\n\nResultKmsKeyArn -> (string)\n\nARN of the KMS encryption key used to encrypt the assessment run results.\n\nAssessmentRunName -> (string)\n\nUnique name of the assessment run."
    },
    {
      "command_name": "create-endpoint",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/create-endpoint.html",
      "command_description": "Description\n\nCreates an endpoint using the provided settings.\n\nNote\n\nFor a MySQL source or target endpoint, don’t explicitly specify the database using the DatabaseName request parameter on the CreateEndpoint API call. Specifying DatabaseName when you create a MySQL endpoint replicates all the task tables to this single database. For MySQL endpoints, you specify the database only when you specify the schema in the table-mapping rules of the DMS task.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  create-endpoint\n--endpoint-identifier <value>\n--endpoint-type <value>\n--engine-name <value>\n[--username <value>]\n[--password <value>]\n[--server-name <value>]\n[--port <value>]\n[--database-name <value>]\n[--extra-connection-attributes <value>]\n[--kms-key-id <value>]\n[--tags <value>]\n[--certificate-arn <value>]\n[--ssl-mode <value>]\n[--service-access-role-arn <value>]\n[--external-table-definition <value>]\n[--dynamo-db-settings <value>]\n[--s3-settings <value>]\n[--dms-transfer-settings <value>]\n[--mongo-db-settings <value>]\n[--kinesis-settings <value>]\n[--kafka-settings <value>]\n[--elasticsearch-settings <value>]\n[--neptune-settings <value>]\n[--redshift-settings <value>]\n[--postgre-sql-settings <value>]\n[--my-sql-settings <value>]\n[--oracle-settings <value>]\n[--sybase-settings <value>]\n[--microsoft-sql-server-settings <value>]\n[--ibm-db2-settings <value>]\n[--resource-identifier <value>]\n[--doc-db-settings <value>]\n[--redis-settings <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--endpoint-identifier <value>",
        "--endpoint-type <value>",
        "--engine-name <value>",
        "[--username <value>]",
        "[--password <value>]",
        "[--server-name <value>]",
        "[--port <value>]",
        "[--database-name <value>]",
        "[--extra-connection-attributes <value>]",
        "[--kms-key-id <value>]",
        "[--tags <value>]",
        "[--certificate-arn <value>]",
        "[--ssl-mode <value>]",
        "[--service-access-role-arn <value>]",
        "[--external-table-definition <value>]",
        "[--dynamo-db-settings <value>]",
        "[--s3-settings <value>]",
        "[--dms-transfer-settings <value>]",
        "[--mongo-db-settings <value>]",
        "[--kinesis-settings <value>]",
        "[--kafka-settings <value>]",
        "[--elasticsearch-settings <value>]",
        "[--neptune-settings <value>]",
        "[--redshift-settings <value>]",
        "[--postgre-sql-settings <value>]",
        "[--my-sql-settings <value>]",
        "[--oracle-settings <value>]",
        "[--sybase-settings <value>]",
        "[--microsoft-sql-server-settings <value>]",
        "[--ibm-db2-settings <value>]",
        "[--resource-identifier <value>]",
        "[--doc-db-settings <value>]",
        "[--redis-settings <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--endpoint-identifier (string)\n\nThe database endpoint identifier. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen, or contain two consecutive hyphens.\n\n--endpoint-type (string)\n\nThe type of endpoint. Valid values are source and target .\n\nPossible values:\n\nsource\n\ntarget\n\n--engine-name (string)\n\nThe type of engine for the endpoint. Valid values, depending on the EndpointType value, include \"mysql\" , \"oracle\" , \"postgres\" , \"mariadb\" , \"aurora\" , \"aurora-postgresql\" , \"redshift\" , \"s3\" , \"db2\" , \"azuredb\" , \"sybase\" , \"dynamodb\" , \"mongodb\" , \"kinesis\" , \"kafka\" , \"elasticsearch\" , \"docdb\" , \"sqlserver\" , and \"neptune\" .\n\n--username (string)\n\nThe user name to be used to log in to the endpoint database.\n\n--password (string)\n\nThe password to be used to log in to the endpoint database.\n\n--server-name (string)\n\nThe name of the server where the endpoint database resides.\n\n--port (integer)\n\nThe port used by the endpoint database.\n\n--database-name (string)\n\nThe name of the endpoint database. For a MySQL source or target endpoint, do not specify DatabaseName.\n\n--extra-connection-attributes (string)\n\nAdditional attributes associated with the connection. Each attribute is specified as a name-value pair associated by an equal sign (=). Multiple attributes are separated by a semicolon (;) with no additional white space. For information on the attributes available for connecting your source or target endpoint, see Working with DMS Endpoints in the Database Migration Service User Guide.\n\n--kms-key-id (string)\n\nAn KMS key identifier that is used to encrypt the connection parameters for the endpoint.\n\nIf you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key.\n\nKMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\n--tags (list)\n\nOne or more tags to be assigned to the endpoint.\n\n(structure)\n\nA user-defined key-value pair that describes metadata added to an DMS resource and that is used by operations such as the following:\n\nAddTagsToResource\n\nListTagsForResource\n\nRemoveTagsFromResource\n\nKey -> (string)\n\nA key is the required name of the tag. The string value can be 1-128 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nValue -> (string)\n\nA value is the optional value of the tag. The string value can be 1-256 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the resource for which the tag is created.\n\nShorthand Syntax:\n\nKey=string,Value=string,ResourceArn=string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Key\": \"string\",\n    \"Value\": \"string\",\n    \"ResourceArn\": \"string\"\n  }\n  ...\n]\n\n\n--certificate-arn (string)\n\nThe Amazon Resource Name (ARN) for the certificate.\n\n--ssl-mode (string)\n\nThe Secure Sockets Layer (SSL) mode to use for the SSL connection. The default is none\n\nPossible values:\n\nnone\n\nrequire\n\nverify-ca\n\nverify-full\n\n--service-access-role-arn (string)\n\nThe Amazon Resource Name (ARN) for the service access role that you want to use to create the endpoint. The role must allow the iam:PassRole action.\n\n--external-table-definition (string)\n\nThe external table definition.\n\n--dynamo-db-settings (structure)\n\nSettings in JSON format for the target Amazon DynamoDB endpoint. For information about other available settings, see Using Object Mapping to Migrate Data to DynamoDB in the Database Migration Service User Guide.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nShorthand Syntax:\n\nServiceAccessRoleArn=string\n\n\nJSON Syntax:\n\n{\n  \"ServiceAccessRoleArn\": \"string\"\n}\n\n\n--s3-settings (structure)\n\nSettings in JSON format for the target Amazon S3 endpoint. For more information about the available settings, see Extra Connection Attributes When Using Amazon S3 as a Target for DMS in the Database Migration Service User Guide.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action. It is a required parameter that enables DMS to write and read objects from an S3 bucket.\n\nExternalTableDefinition -> (string)\n\nSpecifies how tables are defined in the S3 source files only.\n\nCsvRowDelimiter -> (string)\n\nThe delimiter used to separate rows in the .csv file for both source and target. The default is a carriage return (\\n ).\n\nCsvDelimiter -> (string)\n\nThe delimiter used to separate columns in the .csv file for both source and target. The default is a comma.\n\nBucketFolder -> (string)\n\nAn optional parameter to set a folder name in the S3 bucket. If provided, tables are created in the path `` bucketFolder /schema_name /table_name /`` . If this parameter isn’t specified, then the path used is `` schema_name /table_name /`` .\n\nBucketName -> (string)\n\nThe name of the S3 bucket.\n\nCompressionType -> (string)\n\nAn optional parameter to use GZIP to compress the target files. Set to GZIP to compress the target files. Either set this parameter to NONE (the default) or don’t use it to leave the files uncompressed. This parameter applies to both .csv and .parquet file formats.\n\nEncryptionMode -> (string)\n\nThe type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either SSE_S3 (the default) or SSE_KMS .\n\nNote\n\nFor the ModifyEndpoint operation, you can change the existing value of the EncryptionMode parameter from SSE_KMS to SSE_S3 . But you can’t change the existing value from SSE_S3 to SSE_KMS .\n\nTo use SSE_S3 , you need an Identity and Access Management (IAM) role with permission to allow \"arn:aws:s3:::dms-*\" to use the following actions:\n\ns3:CreateBucket\n\ns3:ListBucket\n\ns3:DeleteBucket\n\ns3:GetBucketLocation\n\ns3:GetObject\n\ns3:PutObject\n\ns3:DeleteObject\n\ns3:GetObjectVersion\n\ns3:GetBucketPolicy\n\ns3:PutBucketPolicy\n\ns3:DeleteBucketPolicy\n\nServerSideEncryptionKmsKeyId -> (string)\n\nIf you are using SSE_KMS for the EncryptionMode , provide the KMS key ID. The key that you use needs an attached policy that enables Identity and Access Management (IAM) user permissions and allows use of the key.\n\nHere is a CLI example: ``aws dms create-endpoint –endpoint-identifier value –endpoint-type target –engine-name s3 –s3-settings ServiceAccessRoleArn=*value* ,BucketFolder=*value* ,BucketName=*value* ,EncryptionMode=SSE_KMS,ServerSideEncryptionKmsKeyId=*value* ``\n\nDataFormat -> (string)\n\nThe format of the data that you want to use for output. You can choose one of the following:\n\ncsv : This is a row-based file format with comma-separated values (.csv).\n\nparquet : Apache Parquet (.parquet) is a columnar storage file format that features efficient compression and provides faster query response.\n\nEncodingType -> (string)\n\nThe type of encoding you are using:\n\nRLE_DICTIONARY uses a combination of bit-packing and run-length encoding to store repeated values more efficiently. This is the default.\n\nPLAIN doesn’t use encoding at all. Values are stored as they are.\n\nPLAIN_DICTIONARY builds a dictionary of the values encountered in a given column. The dictionary is stored in a dictionary page for each column chunk.\n\nDictPageSizeLimit -> (integer)\n\nThe maximum size of an encoded dictionary page of a column. If the dictionary page exceeds this, this column is stored using an encoding type of PLAIN . This parameter defaults to 1024 * 1024 bytes (1 MiB), the maximum size of a dictionary page before it reverts to PLAIN encoding. This size is used for .parquet file format only.\n\nRowGroupLength -> (integer)\n\nThe number of rows in a row group. A smaller row group size provides faster reads. But as the number of row groups grows, the slower writes become. This parameter defaults to 10,000 rows. This number is used for .parquet file format only.\n\nIf you choose a value larger than the maximum, RowGroupLength is set to the max row group length in bytes (64 * 1024 * 1024).\n\nDataPageSize -> (integer)\n\nThe size of one data page in bytes. This parameter defaults to 1024 * 1024 bytes (1 MiB). This number is used for .parquet file format only.\n\nParquetVersion -> (string)\n\nThe version of the Apache Parquet format that you want to use: parquet_1_0 (the default) or parquet_2_0 .\n\nEnableStatistics -> (boolean)\n\nA value that enables statistics for Parquet pages and row groups. Choose true to enable statistics, false to disable. Statistics include NULL , DISTINCT , MAX , and MIN values. This parameter defaults to true . This value is used for .parquet file format only.\n\nIncludeOpForFullLoad -> (boolean)\n\nA value that enables a full load to write INSERT operations to the comma-separated value (.csv) output files only to indicate how the rows were added to the source database.\n\nNote\n\nDMS supports the IncludeOpForFullLoad parameter in versions 3.1.4 and later.\n\nFor full load, records can only be inserted. By default (the false setting), no information is recorded in these output files for a full load to indicate that the rows were inserted at the source database. If IncludeOpForFullLoad is set to true or y , the INSERT is recorded as an I annotation in the first field of the .csv file. This allows the format of your target records from a full load to be consistent with the target records from a CDC load.\n\nNote\n\nThis setting works together with the CdcInsertsOnly and the CdcInsertsAndUpdates parameters for output to .csv files only. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nCdcInsertsOnly -> (boolean)\n\nA value that enables a change data capture (CDC) load to write only INSERT operations to .csv or columnar storage (.parquet) output files. By default (the false setting), the first field in a .csv or .parquet record contains the letter I (INSERT), U (UPDATE), or D (DELETE). These values indicate whether the row was inserted, updated, or deleted at the source database for a CDC load to the target.\n\nIf CdcInsertsOnly is set to true or y , only INSERTs from the source database are migrated to the .csv or .parquet file. For .csv format only, how these INSERTs are recorded depends on the value of IncludeOpForFullLoad . If IncludeOpForFullLoad is set to true , the first field of every CDC record is set to I to indicate the INSERT operation at the source. If IncludeOpForFullLoad is set to false , every CDC record is written without a first field to indicate the INSERT operation at the source. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nNote\n\nDMS supports the interaction described preceding between the CdcInsertsOnly and IncludeOpForFullLoad parameters in versions 3.1.4 and later.\n\nCdcInsertsOnly and CdcInsertsAndUpdates can’t both be set to true for the same endpoint. Set either CdcInsertsOnly or CdcInsertsAndUpdates to true for the same endpoint, but not both.\n\nTimestampColumnName -> (string)\n\nA value that when nonblank causes DMS to add a column with timestamp information to the endpoint data for an Amazon S3 target.\n\nNote\n\nDMS supports the TimestampColumnName parameter in versions 3.1.4 and later.\n\nDMS includes an additional STRING column in the .csv or .parquet object files of your migrated data when you set TimestampColumnName to a nonblank value.\n\nFor a full load, each row of this timestamp column contains a timestamp for when the data was transferred from the source to the target by DMS.\n\nFor a change data capture (CDC) load, each row of the timestamp column contains the timestamp for the commit of that row in the source database.\n\nThe string format for this timestamp column value is yyyy-MM-dd HH:mm:ss.SSSSSS . By default, the precision of this value is in microseconds. For a CDC load, the rounding of the precision depends on the commit timestamp supported by DMS for the source database.\n\nWhen the AddColumnName parameter is set to true , DMS also includes a name for the timestamp column that you set with TimestampColumnName .\n\nParquetTimestampInMillisecond -> (boolean)\n\nA value that specifies the precision of any TIMESTAMP column values that are written to an Amazon S3 object file in .parquet format.\n\nNote\n\nDMS supports the ParquetTimestampInMillisecond parameter in versions 3.1.4 and later.\n\nWhen ParquetTimestampInMillisecond is set to true or y , DMS writes all TIMESTAMP columns in a .parquet formatted file with millisecond precision. Otherwise, DMS writes them with microsecond precision.\n\nCurrently, Amazon Athena and Glue can handle only millisecond precision for TIMESTAMP values. Set this parameter to true for S3 endpoint object files that are .parquet formatted only if you plan to query or process the data with Athena or Glue.\n\nNote\n\nDMS writes any TIMESTAMP column values written to an S3 file in .csv format with microsecond precision.\n\nSetting ParquetTimestampInMillisecond has no effect on the string format of the timestamp column value that is inserted by setting the TimestampColumnName parameter.\n\nCdcInsertsAndUpdates -> (boolean)\n\nA value that enables a change data capture (CDC) load to write INSERT and UPDATE operations to .csv or .parquet (columnar storage) output files. The default setting is false , but when CdcInsertsAndUpdates is set to true or y , only INSERTs and UPDATEs from the source database are migrated to the .csv or .parquet file.\n\nFor .csv file format only, how these INSERTs and UPDATEs are recorded depends on the value of the IncludeOpForFullLoad parameter. If IncludeOpForFullLoad is set to true , the first field of every CDC record is set to either I or U to indicate INSERT and UPDATE operations at the source. But if IncludeOpForFullLoad is set to false , CDC records are written without an indication of INSERT or UPDATE operations at the source. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nNote\n\nDMS supports the use of the CdcInsertsAndUpdates parameter in versions 3.3.1 and later.\n\nCdcInsertsOnly and CdcInsertsAndUpdates can’t both be set to true for the same endpoint. Set either CdcInsertsOnly or CdcInsertsAndUpdates to true for the same endpoint, but not both.\n\nDatePartitionEnabled -> (boolean)\n\nWhen set to true , this parameter partitions S3 bucket folders based on transaction commit dates. The default value is false . For more information about date-based folder partitioning, see Using date-based folder partitioning .\n\nDatePartitionSequence -> (string)\n\nIdentifies the sequence of the date format to use during folder partitioning. The default value is YYYYMMDD . Use this parameter when DatePartitionedEnabled is set to true .\n\nDatePartitionDelimiter -> (string)\n\nSpecifies a date separating delimiter to use during folder partitioning. The default value is SLASH . Use this parameter when DatePartitionedEnabled is set to true .\n\nUseCsvNoSupValue -> (boolean)\n\nThis setting applies if the S3 output files during a change data capture (CDC) load are written in .csv format. If set to true for columns not included in the supplemental log, DMS uses the value specified by ` CsvNoSupValue https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CsvNoSupValue`__ . If not set or set to false , DMS uses the null value for these columns.\n\nNote\n\nThis setting is supported in DMS versions 3.4.1 and later.\n\nCsvNoSupValue -> (string)\n\nThis setting only applies if your Amazon S3 output files during a change data capture (CDC) load are written in .csv format. If ` UseCsvNoSupValue https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-UseCsvNoSupValue`__ is set to true, specify a string value that you want DMS to use for all columns not included in the supplemental log. If you do not specify a string value, DMS uses the null value for these columns regardless of the UseCsvNoSupValue setting.\n\nNote\n\nThis setting is supported in DMS versions 3.4.1 and later.\n\nPreserveTransactions -> (boolean)\n\nIf set to true , DMS saves the transaction order for a change data capture (CDC) load on the Amazon S3 target specified by ` CdcPath https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CdcPath`__ . For more information, see Capturing data changes (CDC) including transaction order on the S3 target .\n\nNote\n\nThis setting is supported in DMS versions 3.4.2 and later.\n\nCdcPath -> (string)\n\nSpecifies the folder path of CDC files. For an S3 source, this setting is required if a task captures change data; otherwise, it’s optional. If CdcPath is set, DMS reads CDC files from this path and replicates the data changes to the target endpoint. For an S3 target if you set ` PreserveTransactions https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-PreserveTransactions`__ to true , DMS verifies that you have set this parameter to a folder path on your S3 target where DMS can save the transaction order for the CDC load. DMS creates this CDC folder path in either your S3 target working directory or the S3 target location specified by ` BucketFolder https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketFolder`__ and ` BucketName https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketName`__ .\n\nFor example, if you specify CdcPath as MyChangedData , and you specify BucketName as MyTargetBucket but do not specify BucketFolder , DMS creates the CDC folder path following: MyTargetBucket/MyChangedData .\n\nIf you specify the same CdcPath , and you specify BucketName as MyTargetBucket and BucketFolder as MyTargetData , DMS creates the CDC folder path following: MyTargetBucket/MyTargetData/MyChangedData .\n\nFor more information on CDC including transaction order on an S3 target, see Capturing data changes (CDC) including transaction order on the S3 target .\n\nNote\n\nThis setting is supported in DMS versions 3.4.2 and later.\n\nCannedAclForObjects -> (string)\n\nA value that enables DMS to specify a predefined (canned) access control list for objects created in an Amazon S3 bucket as .csv or .parquet files. For more information about Amazon S3 canned ACLs, see Canned ACL in the Amazon S3 Developer Guide.\n\nThe default value is NONE. Valid values include NONE, PRIVATE, PUBLIC_READ, PUBLIC_READ_WRITE, AUTHENTICATED_READ, AWS_EXEC_READ, BUCKET_OWNER_READ, and BUCKET_OWNER_FULL_CONTROL.\n\nAddColumnName -> (boolean)\n\nAn optional parameter that, when set to true or y , you can use to add column name information to the .csv output file.\n\nThe default value is false . Valid values are true , false , y , and n .\n\nCdcMaxBatchInterval -> (integer)\n\nMaximum length of the interval, defined in seconds, after which to output a file to Amazon S3.\n\nWhen CdcMaxBatchInterval and CdcMinFileSize are both specified, the file write is triggered by whichever parameter condition is met first within an DMS CloudFormation template.\n\nThe default value is 60 seconds.\n\nCdcMinFileSize -> (integer)\n\nMinimum file size, defined in megabytes, to reach for a file output to Amazon S3.\n\nWhen CdcMinFileSize and CdcMaxBatchInterval are both specified, the file write is triggered by whichever parameter condition is met first within an DMS CloudFormation template.\n\nThe default value is 32 MB.\n\nCsvNullValue -> (string)\n\nAn optional parameter that specifies how DMS treats null values. While handling the null value, you can use this parameter to pass a user-defined string as null when writing to the target. For example, when target columns are not nullable, you can use this option to differentiate between the empty string value and the null value. So, if you set this parameter value to the empty string (“” or ‘’), DMS treats the empty string as the null value instead of NULL .\n\nThe default value is NULL . Valid values include any valid string.\n\nIgnoreHeaderRows -> (integer)\n\nWhen this value is set to 1, DMS ignores the first row header in a .csv file. A value of 1 turns on the feature; a value of 0 turns off the feature.\n\nThe default is 0.\n\nMaxFileSize -> (integer)\n\nA value that specifies the maximum size (in KB) of any .csv file to be created while migrating to an S3 target during full load.\n\nThe default value is 1,048,576 KB (1 GB). Valid values include 1 to 1,048,576.\n\nRfc4180 -> (boolean)\n\nFor an S3 source, when this value is set to true or y , each leading double quotation mark has to be followed by an ending double quotation mark. This formatting complies with RFC 4180. When this value is set to false or n , string literals are copied to the target as is. In this case, a delimiter (row or column) signals the end of the field. Thus, you can’t use a delimiter as part of the string, because it signals the end of the value.\n\nFor an S3 target, an optional parameter used to set behavior to comply with RFC 4180 for data migrated to Amazon S3 using .csv file format only. When this value is set to true or y using Amazon S3 as a target, if the data has quotation marks or newline characters in it, DMS encloses the entire column with an additional pair of double quotation marks (“). Every quotation mark within the data is repeated twice.\n\nThe default value is true . Valid values include true , false , y , and n .\n\nShorthand Syntax:\n\nServiceAccessRoleArn=string,ExternalTableDefinition=string,CsvRowDelimiter=string,CsvDelimiter=string,BucketFolder=string,BucketName=string,CompressionType=string,EncryptionMode=string,ServerSideEncryptionKmsKeyId=string,DataFormat=string,EncodingType=string,DictPageSizeLimit=integer,RowGroupLength=integer,DataPageSize=integer,ParquetVersion=string,EnableStatistics=boolean,IncludeOpForFullLoad=boolean,CdcInsertsOnly=boolean,TimestampColumnName=string,ParquetTimestampInMillisecond=boolean,CdcInsertsAndUpdates=boolean,DatePartitionEnabled=boolean,DatePartitionSequence=string,DatePartitionDelimiter=string,UseCsvNoSupValue=boolean,CsvNoSupValue=string,PreserveTransactions=boolean,CdcPath=string,CannedAclForObjects=string,AddColumnName=boolean,CdcMaxBatchInterval=integer,CdcMinFileSize=integer,CsvNullValue=string,IgnoreHeaderRows=integer,MaxFileSize=integer,Rfc4180=boolean\n\n\nJSON Syntax:\n\n{\n  \"ServiceAccessRoleArn\": \"string\",\n  \"ExternalTableDefinition\": \"string\",\n  \"CsvRowDelimiter\": \"string\",\n  \"CsvDelimiter\": \"string\",\n  \"BucketFolder\": \"string\",\n  \"BucketName\": \"string\",\n  \"CompressionType\": \"none\"|\"gzip\",\n  \"EncryptionMode\": \"sse-s3\"|\"sse-kms\",\n  \"ServerSideEncryptionKmsKeyId\": \"string\",\n  \"DataFormat\": \"csv\"|\"parquet\",\n  \"EncodingType\": \"plain\"|\"plain-dictionary\"|\"rle-dictionary\",\n  \"DictPageSizeLimit\": integer,\n  \"RowGroupLength\": integer,\n  \"DataPageSize\": integer,\n  \"ParquetVersion\": \"parquet-1-0\"|\"parquet-2-0\",\n  \"EnableStatistics\": true|false,\n  \"IncludeOpForFullLoad\": true|false,\n  \"CdcInsertsOnly\": true|false,\n  \"TimestampColumnName\": \"string\",\n  \"ParquetTimestampInMillisecond\": true|false,\n  \"CdcInsertsAndUpdates\": true|false,\n  \"DatePartitionEnabled\": true|false,\n  \"DatePartitionSequence\": \"YYYYMMDD\"|\"YYYYMMDDHH\"|\"YYYYMM\"|\"MMYYYYDD\"|\"DDMMYYYY\",\n  \"DatePartitionDelimiter\": \"SLASH\"|\"UNDERSCORE\"|\"DASH\"|\"NONE\",\n  \"UseCsvNoSupValue\": true|false,\n  \"CsvNoSupValue\": \"string\",\n  \"PreserveTransactions\": true|false,\n  \"CdcPath\": \"string\",\n  \"CannedAclForObjects\": \"none\"|\"private\"|\"public-read\"|\"public-read-write\"|\"authenticated-read\"|\"aws-exec-read\"|\"bucket-owner-read\"|\"bucket-owner-full-control\",\n  \"AddColumnName\": true|false,\n  \"CdcMaxBatchInterval\": integer,\n  \"CdcMinFileSize\": integer,\n  \"CsvNullValue\": \"string\",\n  \"IgnoreHeaderRows\": integer,\n  \"MaxFileSize\": integer,\n  \"Rfc4180\": true|false\n}\n\n\n--dms-transfer-settings (structure)\n\nThe settings in JSON format for the DMS transfer type of source endpoint.\n\nPossible settings include the following:\n\nServiceAccessRoleArn - The Amazon Resource Name (ARN) used by the service access IAM role. The role must allow the iam:PassRole action.\n\nBucketName - The name of the S3 bucket to use.\n\nShorthand syntax for these settings is as follows: ServiceAccessRoleArn=string,BucketName=string\n\nJSON syntax for these settings is as follows: { \"ServiceAccessRoleArn\": \"string\", \"BucketName\": \"string\", }\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service access IAM role. The role must allow the iam:PassRole action.\n\nBucketName -> (string)\n\nThe name of the S3 bucket to use.\n\nShorthand Syntax:\n\nServiceAccessRoleArn=string,BucketName=string\n\n\nJSON Syntax:\n\n{\n  \"ServiceAccessRoleArn\": \"string\",\n  \"BucketName\": \"string\"\n}\n\n\n--mongo-db-settings (structure)\n\nSettings in JSON format for the source MongoDB endpoint. For more information about the available settings, see Endpoint configuration settings when using MongoDB as a source for Database Migration Service in the Database Migration Service User Guide.\n\nUsername -> (string)\n\nThe user name you use to access the MongoDB source endpoint.\n\nPassword -> (string)\n\nThe password for the user account you use to access the MongoDB source endpoint.\n\nServerName -> (string)\n\nThe name of the server on the MongoDB source endpoint.\n\nPort -> (integer)\n\nThe port value for the MongoDB source endpoint.\n\nDatabaseName -> (string)\n\nThe database name on the MongoDB source endpoint.\n\nAuthType -> (string)\n\nThe authentication type you use to access the MongoDB source endpoint.\n\nWhen when set to \"no\" , user name and password parameters are not used and can be empty.\n\nAuthMechanism -> (string)\n\nThe authentication mechanism you use to access the MongoDB source endpoint.\n\nFor the default value, in MongoDB version 2.x, \"default\" is \"mongodb_cr\" . For MongoDB version 3.x or later, \"default\" is \"scram_sha_1\" . This setting isn’t used when AuthType is set to \"no\" .\n\nNestingLevel -> (string)\n\nSpecifies either document or table mode.\n\nDefault value is \"none\" . Specify \"none\" to use document mode. Specify \"one\" to use table mode.\n\nExtractDocId -> (string)\n\nSpecifies the document ID. Use this setting when NestingLevel is set to \"none\" .\n\nDefault value is \"false\" .\n\nDocsToInvestigate -> (string)\n\nIndicates the number of documents to preview to determine the document organization. Use this setting when NestingLevel is set to \"one\" .\n\nMust be a positive value greater than 0 . Default value is 1000 .\n\nAuthSource -> (string)\n\nThe MongoDB database name. This setting isn’t used when AuthType is set to \"no\" .\n\nThe default is \"admin\" .\n\nKmsKeyId -> (string)\n\nThe KMS key identifier that is used to encrypt the content on the replication instance. If you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key. KMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the MongoDB endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the MongoDB endpoint connection details.\n\nShorthand Syntax:\n\nUsername=string,Password=string,ServerName=string,Port=integer,DatabaseName=string,AuthType=string,AuthMechanism=string,NestingLevel=string,ExtractDocId=string,DocsToInvestigate=string,AuthSource=string,KmsKeyId=string,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"Username\": \"string\",\n  \"Password\": \"string\",\n  \"ServerName\": \"string\",\n  \"Port\": integer,\n  \"DatabaseName\": \"string\",\n  \"AuthType\": \"no\"|\"password\",\n  \"AuthMechanism\": \"default\"|\"mongodb_cr\"|\"scram_sha_1\",\n  \"NestingLevel\": \"none\"|\"one\",\n  \"ExtractDocId\": \"string\",\n  \"DocsToInvestigate\": \"string\",\n  \"AuthSource\": \"string\",\n  \"KmsKeyId\": \"string\",\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--kinesis-settings (structure)\n\nSettings in JSON format for the target endpoint for Amazon Kinesis Data Streams. For more information about the available settings, see Using object mapping to migrate data to a Kinesis data stream in the Database Migration Service User Guide.\n\nStreamArn -> (string)\n\nThe Amazon Resource Name (ARN) for the Amazon Kinesis Data Streams endpoint.\n\nMessageFormat -> (string)\n\nThe output format for the records created on the endpoint. The message format is JSON (default) or JSON_UNFORMATTED (a single line with no tab).\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) for the IAM role that DMS uses to write to the Kinesis data stream. The role must allow the iam:PassRole action.\n\nIncludeTransactionDetails -> (boolean)\n\nProvides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for transaction_id , previous transaction_id , and transaction_record_id (the record offset within a transaction). The default is false .\n\nIncludePartitionValue -> (boolean)\n\nShows the partition value within the Kinesis message output, unless the partition type is schema-table-type . The default is false .\n\nPartitionIncludeSchemaTable -> (boolean)\n\nPrefixes schema and table names to partition values, when the partition type is primary-key-type . Doing this increases data distribution among Kinesis shards. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same shard, which causes throttling. The default is false .\n\nIncludeTableAlterOperations -> (boolean)\n\nIncludes any data definition language (DDL) operations that change the table in the control data, such as rename-table , drop-table , add-column , drop-column , and rename-column . The default is false .\n\nIncludeControlDetails -> (boolean)\n\nShows detailed control information for table definition, column definition, and table and column changes in the Kinesis message output. The default is false .\n\nIncludeNullAndEmpty -> (boolean)\n\nInclude NULL and empty columns for records migrated to the endpoint. The default is false .\n\nNoHexPrefix -> (boolean)\n\nSet this optional parameter to true to avoid adding a ‘0x’ prefix to raw data in hexadecimal format. For example, by default, DMS adds a ‘0x’ prefix to the LOB column type in hexadecimal format moving from an Oracle source to an Amazon Kinesis target. Use the NoHexPrefix endpoint setting to enable migration of RAW data type columns without adding the ‘0x’ prefix.\n\nShorthand Syntax:\n\nStreamArn=string,MessageFormat=string,ServiceAccessRoleArn=string,IncludeTransactionDetails=boolean,IncludePartitionValue=boolean,PartitionIncludeSchemaTable=boolean,IncludeTableAlterOperations=boolean,IncludeControlDetails=boolean,IncludeNullAndEmpty=boolean,NoHexPrefix=boolean\n\n\nJSON Syntax:\n\n{\n  \"StreamArn\": \"string\",\n  \"MessageFormat\": \"json\"|\"json-unformatted\",\n  \"ServiceAccessRoleArn\": \"string\",\n  \"IncludeTransactionDetails\": true|false,\n  \"IncludePartitionValue\": true|false,\n  \"PartitionIncludeSchemaTable\": true|false,\n  \"IncludeTableAlterOperations\": true|false,\n  \"IncludeControlDetails\": true|false,\n  \"IncludeNullAndEmpty\": true|false,\n  \"NoHexPrefix\": true|false\n}\n\n\n--kafka-settings (structure)\n\nSettings in JSON format for the target Apache Kafka endpoint. For more information about the available settings, see Using object mapping to migrate data to a Kafka topic in the Database Migration Service User Guide.\n\nBroker -> (string)\n\nA comma-separated list of one or more broker locations in your Kafka cluster that host your Kafka instance. Specify each broker location in the form `` broker-hostname-or-ip :port `` . For example, \"ec2-12-345-678-901.compute-1.amazonaws.com:2345\" . For more information and examples of specifying a list of broker locations, see Using Apache Kafka as a target for Database Migration Service in the Database Migration Service User Guide .\n\nTopic -> (string)\n\nThe topic to which you migrate the data. If you don’t specify a topic, DMS specifies \"kafka-default-topic\" as the migration topic.\n\nMessageFormat -> (string)\n\nThe output format for the records created on the endpoint. The message format is JSON (default) or JSON_UNFORMATTED (a single line with no tab).\n\nIncludeTransactionDetails -> (boolean)\n\nProvides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for transaction_id , previous transaction_id , and transaction_record_id (the record offset within a transaction). The default is false .\n\nIncludePartitionValue -> (boolean)\n\nShows the partition value within the Kafka message output unless the partition type is schema-table-type . The default is false .\n\nPartitionIncludeSchemaTable -> (boolean)\n\nPrefixes schema and table names to partition values, when the partition type is primary-key-type . Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. The default is false .\n\nIncludeTableAlterOperations -> (boolean)\n\nIncludes any data definition language (DDL) operations that change the table in the control data, such as rename-table , drop-table , add-column , drop-column , and rename-column . The default is false .\n\nIncludeControlDetails -> (boolean)\n\nShows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. The default is false .\n\nMessageMaxBytes -> (integer)\n\nThe maximum size in bytes for records created on the endpoint The default is 1,000,000.\n\nIncludeNullAndEmpty -> (boolean)\n\nInclude NULL and empty columns for records migrated to the endpoint. The default is false .\n\nSecurityProtocol -> (string)\n\nSet secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include ssl-encryption , ssl-authentication , and sasl-ssl . sasl-ssl requires SaslUsername and SaslPassword .\n\nSslClientCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) of the client certificate used to securely connect to a Kafka target endpoint.\n\nSslClientKeyArn -> (string)\n\nThe Amazon Resource Name (ARN) for the client private key used to securely connect to a Kafka target endpoint.\n\nSslClientKeyPassword -> (string)\n\nThe password for the client private key used to securely connect to a Kafka target endpoint.\n\nSslCaCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the private certificate authority (CA) cert that DMS uses to securely connect to your Kafka target endpoint.\n\nSaslUsername -> (string)\n\nThe secure user name you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n\nSaslPassword -> (string)\n\nThe secure password you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n\nNoHexPrefix -> (boolean)\n\nSet this optional parameter to true to avoid adding a ‘0x’ prefix to raw data in hexadecimal format. For example, by default, DMS adds a ‘0x’ prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the NoHexPrefix endpoint setting to enable migration of RAW data type columns without adding the ‘0x’ prefix.\n\nShorthand Syntax:\n\nBroker=string,Topic=string,MessageFormat=string,IncludeTransactionDetails=boolean,IncludePartitionValue=boolean,PartitionIncludeSchemaTable=boolean,IncludeTableAlterOperations=boolean,IncludeControlDetails=boolean,MessageMaxBytes=integer,IncludeNullAndEmpty=boolean,SecurityProtocol=string,SslClientCertificateArn=string,SslClientKeyArn=string,SslClientKeyPassword=string,SslCaCertificateArn=string,SaslUsername=string,SaslPassword=string,NoHexPrefix=boolean\n\n\nJSON Syntax:\n\n{\n  \"Broker\": \"string\",\n  \"Topic\": \"string\",\n  \"MessageFormat\": \"json\"|\"json-unformatted\",\n  \"IncludeTransactionDetails\": true|false,\n  \"IncludePartitionValue\": true|false,\n  \"PartitionIncludeSchemaTable\": true|false,\n  \"IncludeTableAlterOperations\": true|false,\n  \"IncludeControlDetails\": true|false,\n  \"MessageMaxBytes\": integer,\n  \"IncludeNullAndEmpty\": true|false,\n  \"SecurityProtocol\": \"plaintext\"|\"ssl-authentication\"|\"ssl-encryption\"|\"sasl-ssl\",\n  \"SslClientCertificateArn\": \"string\",\n  \"SslClientKeyArn\": \"string\",\n  \"SslClientKeyPassword\": \"string\",\n  \"SslCaCertificateArn\": \"string\",\n  \"SaslUsername\": \"string\",\n  \"SaslPassword\": \"string\",\n  \"NoHexPrefix\": true|false\n}\n\n\n--elasticsearch-settings (structure)\n\nSettings in JSON format for the target Elasticsearch endpoint. For more information about the available settings, see Extra Connection Attributes When Using Elasticsearch as a Target for DMS in the Database Migration Service User Guide .\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nEndpointUri -> (string)\n\nThe endpoint for the Elasticsearch cluster. DMS uses HTTPS if a transport protocol (http/https) is not specified.\n\nFullLoadErrorPercentage -> (integer)\n\nThe maximum percentage of records that can fail to be written before a full load operation stops.\n\nTo avoid early failure, this counter is only effective after 1000 records are transferred. Elasticsearch also has the concept of error monitoring during the last 10 minutes of an Observation Window. If transfer of all records fail in the last 10 minutes, the full load operation stops.\n\nErrorRetryDuration -> (integer)\n\nThe maximum number of seconds for which DMS retries failed API requests to the Elasticsearch cluster.\n\nShorthand Syntax:\n\nServiceAccessRoleArn=string,EndpointUri=string,FullLoadErrorPercentage=integer,ErrorRetryDuration=integer\n\n\nJSON Syntax:\n\n{\n  \"ServiceAccessRoleArn\": \"string\",\n  \"EndpointUri\": \"string\",\n  \"FullLoadErrorPercentage\": integer,\n  \"ErrorRetryDuration\": integer\n}\n\n\n--neptune-settings (structure)\n\nSettings in JSON format for the target Amazon Neptune endpoint. For more information about the available settings, see Specifying graph-mapping rules using Gremlin and R2RML for Amazon Neptune as a target in the Database Migration Service User Guide.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the service role that you created for the Neptune target endpoint. The role must allow the iam:PassRole action. For more information, see Creating an IAM Service Role for Accessing Amazon Neptune as a Target in the Database Migration Service User Guide.\n\nS3BucketName -> (string)\n\nThe name of the Amazon S3 bucket where DMS can temporarily store migrated graph data in .csv files before bulk-loading it to the Neptune target database. DMS maps the SQL source data to graph data before storing it in these .csv files.\n\nS3BucketFolder -> (string)\n\nA folder path where you want DMS to store migrated graph data in the S3 bucket specified by S3BucketName\n\nErrorRetryDuration -> (integer)\n\nThe number of milliseconds for DMS to wait to retry a bulk-load of migrated graph data to the Neptune target database before raising an error. The default is 250.\n\nMaxFileSize -> (integer)\n\nThe maximum size in kilobytes of migrated graph data stored in a .csv file before DMS bulk-loads the data to the Neptune target database. The default is 1,048,576 KB. If the bulk load is successful, DMS clears the bucket, ready to store the next batch of migrated graph data.\n\nMaxRetryCount -> (integer)\n\nThe number of times for DMS to retry a bulk load of migrated graph data to the Neptune target database before raising an error. The default is 5.\n\nIamAuthEnabled -> (boolean)\n\nIf you want Identity and Access Management (IAM) authorization enabled for this endpoint, set this parameter to true . Then attach the appropriate IAM policy document to your service role specified by ServiceAccessRoleArn . The default is false .\n\nShorthand Syntax:\n\nServiceAccessRoleArn=string,S3BucketName=string,S3BucketFolder=string,ErrorRetryDuration=integer,MaxFileSize=integer,MaxRetryCount=integer,IamAuthEnabled=boolean\n\n\nJSON Syntax:\n\n{\n  \"ServiceAccessRoleArn\": \"string\",\n  \"S3BucketName\": \"string\",\n  \"S3BucketFolder\": \"string\",\n  \"ErrorRetryDuration\": integer,\n  \"MaxFileSize\": integer,\n  \"MaxRetryCount\": integer,\n  \"IamAuthEnabled\": true|false\n}\n\n\n--redshift-settings (structure)\n\nProvides information that defines an Amazon Redshift endpoint.\n\nAcceptAnyDate -> (boolean)\n\nA value that indicates to allow any date format, including invalid formats such as 00/00/00 00:00:00, to be loaded without generating an error. You can choose true or false (the default).\n\nThis parameter applies only to TIMESTAMP and DATE columns. Always use ACCEPTANYDATE with the DATEFORMAT parameter. If the date format for the data doesn’t match the DATEFORMAT specification, Amazon Redshift inserts a NULL value into that field.\n\nAfterConnectScript -> (string)\n\nCode to run after connecting. This parameter should contain the code itself, not the name of a file containing the code.\n\nBucketFolder -> (string)\n\nAn S3 folder where the comma-separated-value (.csv) files are stored before being uploaded to the target Redshift cluster.\n\nFor full load mode, DMS converts source records into .csv files and loads them to the BucketFolder/TableID path. DMS uses the Redshift COPY command to upload the .csv files to the target table. The files are deleted once the COPY operation has finished. For more information, see COPY in the Amazon Redshift Database Developer Guide .\n\nFor change-data-capture (CDC) mode, DMS creates a NetChanges table, and loads the .csv files to this BucketFolder/NetChangesTableID path.\n\nBucketName -> (string)\n\nThe name of the intermediate S3 bucket used to store .csv files before uploading data to Redshift.\n\nCaseSensitiveNames -> (boolean)\n\nIf Amazon Redshift is configured to support case sensitive schema names, set CaseSensitiveNames to true . The default is false .\n\nCompUpdate -> (boolean)\n\nIf you set CompUpdate to true Amazon Redshift applies automatic compression if the table is empty. This applies even if the table columns already have encodings other than RAW . If you set CompUpdate to false , automatic compression is disabled and existing column encodings aren’t changed. The default is true .\n\nConnectionTimeout -> (integer)\n\nA value that sets the amount of time to wait (in milliseconds) before timing out, beginning from when you initially establish a connection.\n\nDatabaseName -> (string)\n\nThe name of the Amazon Redshift data warehouse (service) that you are working with.\n\nDateFormat -> (string)\n\nThe date format that you are using. Valid values are auto (case-sensitive), your date format string enclosed in quotes, or NULL. If this parameter is left unset (NULL), it defaults to a format of ‘YYYY-MM-DD’. Using auto recognizes most strings, even some that aren’t supported when you use a date format string.\n\nIf your date and time values use formats different from each other, set this to auto .\n\nEmptyAsNull -> (boolean)\n\nA value that specifies whether DMS should migrate empty CHAR and VARCHAR fields as NULL. A value of true sets empty CHAR and VARCHAR fields to null. The default is false .\n\nEncryptionMode -> (string)\n\nThe type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either SSE_S3 (the default) or SSE_KMS .\n\nNote\n\nFor the ModifyEndpoint operation, you can change the existing value of the EncryptionMode parameter from SSE_KMS to SSE_S3 . But you can’t change the existing value from SSE_S3 to SSE_KMS .\n\nTo use SSE_S3 , create an Identity and Access Management (IAM) role with a policy that allows \"arn:aws:s3:::*\" to use the following actions: \"s3:PutObject\", \"s3:ListBucket\"\n\nExplicitIds -> (boolean)\n\nThis setting is only valid for a full-load migration task. Set ExplicitIds to true to have tables with IDENTITY columns override their auto-generated values with explicit values loaded from the source data files used to populate the tables. The default is false .\n\nFileTransferUploadStreams -> (integer)\n\nThe number of threads used to upload a single file. This parameter accepts a value from 1 through 64. It defaults to 10.\n\nThe number of parallel streams used to upload a single .csv file to an S3 bucket using S3 Multipart Upload. For more information, see Multipart upload overview .\n\nFileTransferUploadStreams accepts a value from 1 through 64. It defaults to 10.\n\nLoadTimeout -> (integer)\n\nThe amount of time to wait (in milliseconds) before timing out of operations performed by DMS on a Redshift cluster, such as Redshift COPY, INSERT, DELETE, and UPDATE.\n\nMaxFileSize -> (integer)\n\nThe maximum size (in KB) of any .csv file used to load data on an S3 bucket and transfer data to Amazon Redshift. It defaults to 1048576KB (1 GB).\n\nPassword -> (string)\n\nThe password for the user named in the username property.\n\nPort -> (integer)\n\nThe port number for Amazon Redshift. The default value is 5439.\n\nRemoveQuotes -> (boolean)\n\nA value that specifies to remove surrounding quotation marks from strings in the incoming data. All characters within the quotation marks, including delimiters, are retained. Choose true to remove quotation marks. The default is false .\n\nReplaceInvalidChars -> (string)\n\nA list of characters that you want to replace. Use with ReplaceChars .\n\nReplaceChars -> (string)\n\nA value that specifies to replaces the invalid characters specified in ReplaceInvalidChars , substituting the specified characters instead. The default is \"?\" .\n\nServerName -> (string)\n\nThe name of the Amazon Redshift cluster you are using.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the IAM role that has access to the Amazon Redshift service. The role must allow the iam:PassRole action.\n\nServerSideEncryptionKmsKeyId -> (string)\n\nThe KMS key ID. If you are using SSE_KMS for the EncryptionMode , provide this key ID. The key that you use needs an attached policy that enables IAM user permissions and allows use of the key.\n\nTimeFormat -> (string)\n\nThe time format that you want to use. Valid values are auto (case-sensitive), 'timeformat_string' , 'epochsecs' , or 'epochmillisecs' . It defaults to 10. Using auto recognizes most strings, even some that aren’t supported when you use a time format string.\n\nIf your date and time values use formats different from each other, set this parameter to auto .\n\nTrimBlanks -> (boolean)\n\nA value that specifies to remove the trailing white space characters from a VARCHAR string. This parameter applies only to columns with a VARCHAR data type. Choose true to remove unneeded white space. The default is false .\n\nTruncateColumns -> (boolean)\n\nA value that specifies to truncate data in columns to the appropriate number of characters, so that the data fits in the column. This parameter applies only to columns with a VARCHAR or CHAR data type, and rows with a size of 4 MB or less. Choose true to truncate data. The default is false .\n\nUsername -> (string)\n\nAn Amazon Redshift user name for a registered user.\n\nWriteBufferSize -> (integer)\n\nThe size (in KB) of the in-memory file write buffer used when generating .csv files on the local disk at the DMS replication instance. The default value is 1000 (buffer size is 1000KB).\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Amazon Redshift endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Amazon Redshift endpoint connection details.\n\nShorthand Syntax:\n\nAcceptAnyDate=boolean,AfterConnectScript=string,BucketFolder=string,BucketName=string,CaseSensitiveNames=boolean,CompUpdate=boolean,ConnectionTimeout=integer,DatabaseName=string,DateFormat=string,EmptyAsNull=boolean,EncryptionMode=string,ExplicitIds=boolean,FileTransferUploadStreams=integer,LoadTimeout=integer,MaxFileSize=integer,Password=string,Port=integer,RemoveQuotes=boolean,ReplaceInvalidChars=string,ReplaceChars=string,ServerName=string,ServiceAccessRoleArn=string,ServerSideEncryptionKmsKeyId=string,TimeFormat=string,TrimBlanks=boolean,TruncateColumns=boolean,Username=string,WriteBufferSize=integer,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"AcceptAnyDate\": true|false,\n  \"AfterConnectScript\": \"string\",\n  \"BucketFolder\": \"string\",\n  \"BucketName\": \"string\",\n  \"CaseSensitiveNames\": true|false,\n  \"CompUpdate\": true|false,\n  \"ConnectionTimeout\": integer,\n  \"DatabaseName\": \"string\",\n  \"DateFormat\": \"string\",\n  \"EmptyAsNull\": true|false,\n  \"EncryptionMode\": \"sse-s3\"|\"sse-kms\",\n  \"ExplicitIds\": true|false,\n  \"FileTransferUploadStreams\": integer,\n  \"LoadTimeout\": integer,\n  \"MaxFileSize\": integer,\n  \"Password\": \"string\",\n  \"Port\": integer,\n  \"RemoveQuotes\": true|false,\n  \"ReplaceInvalidChars\": \"string\",\n  \"ReplaceChars\": \"string\",\n  \"ServerName\": \"string\",\n  \"ServiceAccessRoleArn\": \"string\",\n  \"ServerSideEncryptionKmsKeyId\": \"string\",\n  \"TimeFormat\": \"string\",\n  \"TrimBlanks\": true|false,\n  \"TruncateColumns\": true|false,\n  \"Username\": \"string\",\n  \"WriteBufferSize\": integer,\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--postgre-sql-settings (structure)\n\nSettings in JSON format for the source and target PostgreSQL endpoint. For information about other available settings, see Extra connection attributes when using PostgreSQL as a source for DMS and Extra connection attributes when using PostgreSQL as a target for DMS in the Database Migration Service User Guide.\n\nAfterConnectScript -> (string)\n\nFor use with change data capture (CDC) only, this attribute has DMS bypass foreign keys and user triggers to reduce the time it takes to bulk load data.\n\nExample: afterConnectScript=SET session_replication_role='replica'\n\nCaptureDdls -> (boolean)\n\nTo capture DDL events, DMS creates various artifacts in the PostgreSQL database when the task starts. You can later remove these artifacts.\n\nIf this value is set to N , you don’t have to create tables or triggers on the source database.\n\nMaxFileSize -> (integer)\n\nSpecifies the maximum size (in KB) of any .csv file used to transfer data to PostgreSQL.\n\nExample: maxFileSize=512\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nDdlArtifactsSchema -> (string)\n\nThe schema in which the operational DDL database artifacts are created.\n\nExample: ddlArtifactsSchema=xyzddlschema;\n\nExecuteTimeout -> (integer)\n\nSets the client statement timeout for the PostgreSQL instance, in seconds. The default value is 60 seconds.\n\nExample: executeTimeout=100;\n\nFailTasksOnLobTruncation -> (boolean)\n\nWhen set to true , this value causes a task to fail if the actual size of a LOB column is greater than the specified LobMaxSize .\n\nIf task is set to Limited LOB mode and this option is set to true, the task fails instead of truncating the LOB data.\n\nHeartbeatEnable -> (boolean)\n\nThe write-ahead log (WAL) heartbeat feature mimics a dummy transaction. By doing this, it prevents idle logical replication slots from holding onto old WAL logs, which can result in storage full situations on the source. This heartbeat keeps restart_lsn moving and prevents storage full scenarios.\n\nHeartbeatSchema -> (string)\n\nSets the schema in which the heartbeat artifacts are created.\n\nHeartbeatFrequency -> (integer)\n\nSets the WAL heartbeat frequency (in minutes).\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSlotName -> (string)\n\nSets the name of a previously created logical replication slot for a change data capture (CDC) load of the PostgreSQL source instance.\n\nWhen used with the CdcStartPosition request parameter for the DMS API , this attribute also makes it possible to use native CDC start points. DMS verifies that the specified logical replication slot exists before starting the CDC load task. It also verifies that the task was created with a valid setting of CdcStartPosition . If the specified slot doesn’t exist or the task doesn’t have a valid CdcStartPosition setting, DMS raises an error.\n\nFor more information about setting the CdcStartPosition request parameter, see Determining a CDC native start point in the Database Migration Service User Guide . For more information about using CdcStartPosition , see CreateReplicationTask , StartReplicationTask , and ModifyReplicationTask .\n\nPluginName -> (string)\n\nSpecifies the plugin to use to create a replication slot.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the PostgreSQL endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the PostgreSQL endpoint connection details.\n\nShorthand Syntax:\n\nAfterConnectScript=string,CaptureDdls=boolean,MaxFileSize=integer,DatabaseName=string,DdlArtifactsSchema=string,ExecuteTimeout=integer,FailTasksOnLobTruncation=boolean,HeartbeatEnable=boolean,HeartbeatSchema=string,HeartbeatFrequency=integer,Password=string,Port=integer,ServerName=string,Username=string,SlotName=string,PluginName=string,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"AfterConnectScript\": \"string\",\n  \"CaptureDdls\": true|false,\n  \"MaxFileSize\": integer,\n  \"DatabaseName\": \"string\",\n  \"DdlArtifactsSchema\": \"string\",\n  \"ExecuteTimeout\": integer,\n  \"FailTasksOnLobTruncation\": true|false,\n  \"HeartbeatEnable\": true|false,\n  \"HeartbeatSchema\": \"string\",\n  \"HeartbeatFrequency\": integer,\n  \"Password\": \"string\",\n  \"Port\": integer,\n  \"ServerName\": \"string\",\n  \"Username\": \"string\",\n  \"SlotName\": \"string\",\n  \"PluginName\": \"no-preference\"|\"test-decoding\"|\"pglogical\",\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--my-sql-settings (structure)\n\nSettings in JSON format for the source and target MySQL endpoint. For information about other available settings, see Extra connection attributes when using MySQL as a source for DMS and Extra connection attributes when using a MySQL-compatible database as a target for DMS in the Database Migration Service User Guide.\n\nAfterConnectScript -> (string)\n\nSpecifies a script to run immediately after DMS connects to the endpoint. The migration task continues running regardless if the SQL statement succeeds or fails.\n\nFor this parameter, provide the code of the script itself, not the name of a file containing the script.\n\nCleanSourceMetadataOnMismatch -> (boolean)\n\nAdjusts the behavior of DMS when migrating from an SQL Server source database that is hosted as part of an Always On availability group cluster. If you need DMS to poll all the nodes in the Always On cluster for transaction backups, set this attribute to false .\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint. For a MySQL source or target endpoint, don’t explicitly specify the database using the DatabaseName request parameter on either the CreateEndpoint or ModifyEndpoint API call. Specifying DatabaseName when you create or modify a MySQL endpoint replicates all the task tables to this single database. For MySQL endpoints, you specify the database only when you specify the schema in the table-mapping rules of the DMS task.\n\nEventsPollInterval -> (integer)\n\nSpecifies how often to check the binary log for new changes/events when the database is idle.\n\nExample: eventsPollInterval=5;\n\nIn the example, DMS checks for changes in the binary logs every five seconds.\n\nTargetDbType -> (string)\n\nSpecifies where to migrate source tables on the target, either to a single database or multiple databases.\n\nExample: targetDbType=MULTIPLE_DATABASES\n\nMaxFileSize -> (integer)\n\nSpecifies the maximum size (in KB) of any .csv file used to transfer data to a MySQL-compatible database.\n\nExample: maxFileSize=512\n\nParallelLoadThreads -> (integer)\n\nImproves performance when loading data into the MySQL-compatible target database. Specifies how many threads to use to load the data into the MySQL-compatible target database. Setting a large number of threads can have an adverse effect on database performance, because a separate connection is required for each thread.\n\nExample: parallelLoadThreads=1\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nServerTimezone -> (string)\n\nSpecifies the time zone for the source MySQL database.\n\nExample: serverTimezone=US/Pacific;\n\nNote: Do not enclose time zones in single quotes.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the MySQL endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the MySQL endpoint connection details.\n\nShorthand Syntax:\n\nAfterConnectScript=string,CleanSourceMetadataOnMismatch=boolean,DatabaseName=string,EventsPollInterval=integer,TargetDbType=string,MaxFileSize=integer,ParallelLoadThreads=integer,Password=string,Port=integer,ServerName=string,ServerTimezone=string,Username=string,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"AfterConnectScript\": \"string\",\n  \"CleanSourceMetadataOnMismatch\": true|false,\n  \"DatabaseName\": \"string\",\n  \"EventsPollInterval\": integer,\n  \"TargetDbType\": \"specific-database\"|\"multiple-databases\",\n  \"MaxFileSize\": integer,\n  \"ParallelLoadThreads\": integer,\n  \"Password\": \"string\",\n  \"Port\": integer,\n  \"ServerName\": \"string\",\n  \"ServerTimezone\": \"string\",\n  \"Username\": \"string\",\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--oracle-settings (structure)\n\nSettings in JSON format for the source and target Oracle endpoint. For information about other available settings, see Extra connection attributes when using Oracle as a source for DMS and Extra connection attributes when using Oracle as a target for DMS in the Database Migration Service User Guide.\n\nAddSupplementalLogging -> (boolean)\n\nSet this attribute to set up table-level supplemental logging for the Oracle database. This attribute enables PRIMARY KEY supplemental logging on all tables selected for a migration task.\n\nIf you use this option, you still need to enable database-level supplemental logging.\n\nArchivedLogDestId -> (integer)\n\nSpecifies the ID of the destination for the archived redo logs. This value should be the same as a number in the dest_id column of the v$archived_log view. If you work with an additional redo log destination, use the AdditionalArchivedLogDestId option to specify the additional destination ID. Doing this improves performance by ensuring that the correct logs are accessed from the outset.\n\nAdditionalArchivedLogDestId -> (integer)\n\nSet this attribute with ArchivedLogDestId in a primary/ standby setup. This attribute is useful in the case of a switchover. In this case, DMS needs to know which destination to get archive redo logs from to read changes. This need arises because the previous primary instance is now a standby instance after switchover.\n\nAlthough DMS supports the use of the Oracle RESETLOGS option to open the database, never use RESETLOGS unless necessary. For additional information about RESETLOGS , see RMAN Data Repair Concepts in the Oracle Database Backup and Recovery User’s Guide .\n\nExtraArchivedLogDestIds -> (list)\n\nSpecifies the IDs of one more destinations for one or more archived redo logs. These IDs are the values of the dest_id column in the v$archived_log view. Use this setting with the archivedLogDestId extra connection attribute in a primary-to-single setup or a primary-to-multiple-standby setup.\n\nThis setting is useful in a switchover when you use an Oracle Data Guard database as a source. In this case, DMS needs information about what destination to get archive redo logs from to read changes. DMS needs this because after the switchover the previous primary is a standby instance. For example, in a primary-to-single standby setup you might apply the following settings.\n\narchivedLogDestId=1; ExtraArchivedLogDestIds=[2]\n\nIn a primary-to-multiple-standby setup, you might apply the following settings.\n\narchivedLogDestId=1; ExtraArchivedLogDestIds=[2,3,4]\n\nAlthough DMS supports the use of the Oracle RESETLOGS option to open the database, never use RESETLOGS unless it’s necessary. For more information about RESETLOGS , see RMAN Data Repair Concepts in the Oracle Database Backup and Recovery User’s Guide .\n\n(integer)\n\nAllowSelectNestedTables -> (boolean)\n\nSet this attribute to true to enable replication of Oracle tables containing columns that are nested tables or defined types.\n\nParallelAsmReadThreads -> (integer)\n\nSet this attribute to change the number of threads that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 2 (the default) and 8 (the maximum). Use this attribute together with the readAheadBlocks attribute.\n\nReadAheadBlocks -> (integer)\n\nSet this attribute to change the number of read-ahead blocks that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 1000 (the default) and 200,000 (the maximum).\n\nAccessAlternateDirectly -> (boolean)\n\nSet this attribute to false in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to not access redo logs through any specified path prefix replacement using direct file access.\n\nUseAlternateFolderForOnline -> (boolean)\n\nSet this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to use any specified prefix replacement to access all online redo logs.\n\nOraclePathPrefix -> (string)\n\nSet this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the default Oracle root used to access the redo logs.\n\nUsePathPrefix -> (string)\n\nSet this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the path prefix used to replace the default Oracle root to access the redo logs.\n\nReplacePathPrefix -> (boolean)\n\nSet this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This setting tells DMS instance to replace the default Oracle root with the specified usePathPrefix setting to access the redo logs.\n\nEnableHomogenousTablespace -> (boolean)\n\nSet this attribute to enable homogenous tablespace replication and create existing tables or indexes under the same tablespace on the target.\n\nDirectPathNoLog -> (boolean)\n\nWhen set to true , this attribute helps to increase the commit rate on the Oracle target database by writing directly to tables and not writing a trail to database logs.\n\nArchivedLogsOnly -> (boolean)\n\nWhen this field is set to Y , DMS only accesses the archived redo logs. If the archived redo logs are stored on Oracle ASM only, the DMS user account needs to be granted ASM privileges.\n\nAsmPassword -> (string)\n\nFor an Oracle source endpoint, your Oracle Automatic Storage Management (ASM) password. You can set this value from the `` asm_user_password `` value. You set this value as part of the comma-separated value that you set to the Password request parameter when you create the endpoint to access transaction logs using Binary Reader. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nAsmServer -> (string)\n\nFor an Oracle source endpoint, your ASM server address. You can set this value from the asm_server value. You set asm_server as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nAsmUser -> (string)\n\nFor an Oracle source endpoint, your ASM user name. You can set this value from the asm_user value. You set asm_user as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nCharLengthSemantics -> (string)\n\nSpecifies whether the length of a character column is in bytes or in characters. To indicate that the character column length is in characters, set this attribute to CHAR . Otherwise, the character column length is in bytes.\n\nExample: charLengthSemantics=CHAR;\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nDirectPathParallelLoad -> (boolean)\n\nWhen set to true , this attribute specifies a parallel load when useDirectPathFullLoad is set to Y . This attribute also only applies when you use the DMS parallel load feature. Note that the target table cannot have any constraints or indexes.\n\nFailTasksOnLobTruncation -> (boolean)\n\nWhen set to true , this attribute causes a task to fail if the actual size of an LOB column is greater than the specified LobMaxSize .\n\nIf a task is set to limited LOB mode and this option is set to true , the task fails instead of truncating the LOB data.\n\nNumberDatatypeScale -> (integer)\n\nSpecifies the number scale. You can select a scale up to 38, or you can select FLOAT. By default, the NUMBER data type is converted to precision 38, scale 10.\n\nExample: numberDataTypeScale=12\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nReadTableSpaceName -> (boolean)\n\nWhen set to true , this attribute supports tablespace replication.\n\nRetryInterval -> (integer)\n\nSpecifies the number of seconds that the system waits before resending a query.\n\nExample: retryInterval=6;\n\nSecurityDbEncryption -> (string)\n\nFor an Oracle source endpoint, the transparent data encryption (TDE) password required by AWM DMS to access Oracle redo logs encrypted by TDE using Binary Reader. It is also the `` TDE_Password `` part of the comma-separated value you set to the Password request parameter when you create the endpoint. The SecurityDbEncryptian setting is related to this SecurityDbEncryptionName setting. For more information, see Supported encryption methods for using Oracle as a source for DMS in the Database Migration Service User Guide .\n\nSecurityDbEncryptionName -> (string)\n\nFor an Oracle source endpoint, the name of a key used for the transparent data encryption (TDE) of the columns and tablespaces in an Oracle source database that is encrypted using TDE. The key value is the value of the SecurityDbEncryption setting. For more information on setting the key name value of SecurityDbEncryptionName , see the information and example for setting the securityDbEncryptionName extra connection attribute in Supported encryption methods for using Oracle as a source for DMS in the Database Migration Service User Guide .\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nSpatialDataOptionToGeoJsonFunctionName -> (string)\n\nUse this attribute to convert SDO_GEOMETRY to GEOJSON format. By default, DMS calls the SDO2GEOJSON custom function if present and accessible. Or you can create your own custom function that mimics the operation of SDOGEOJSON and set SpatialDataOptionToGeoJsonFunctionName to call it instead.\n\nStandbyDelayTime -> (integer)\n\nUse this attribute to specify a time in minutes for the delay in standby sync. If the source is an Oracle Active Data Guard standby database, use this attribute to specify the time lag between primary and standby databases.\n\nIn DMS, you can create an Oracle CDC task that uses an Active Data Guard standby instance as a source for replicating ongoing changes. Doing this eliminates the need to connect to an active database that might be in production.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nUseBFile -> (boolean)\n\nSet this attribute to Y to capture change data using the Binary Reader utility. Set UseLogminerReader to N to set this attribute to Y. To use Binary Reader with Amazon RDS for Oracle as the source, you set additional attributes. For more information about using this setting with Oracle Automatic Storage Management (ASM), see Using Oracle LogMiner or DMS Binary Reader for CDC .\n\nUseDirectPathFullLoad -> (boolean)\n\nSet this attribute to Y to have DMS use a direct path full load. Specify this value to use the direct path protocol in the Oracle Call Interface (OCI). By using this OCI protocol, you can bulk-load Oracle target tables during a full load.\n\nUseLogminerReader -> (boolean)\n\nSet this attribute to Y to capture change data using the Oracle LogMiner utility (the default). Set this attribute to N if you want to access the redo logs as a binary file. When you set UseLogminerReader to N, also set UseBfile to Y. For more information on this setting and using Oracle ASM, see Using Oracle LogMiner or DMS Binary Reader for CDC in the DMS User Guide .\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Oracle endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Oracle endpoint connection details.\n\nSecretsManagerOracleAsmAccessRoleArn -> (string)\n\nRequired only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the SecretsManagerOracleAsmSecret . This SecretsManagerOracleAsmSecret has the secret value that allows access to the Oracle ASM of the endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerOracleAsmSecretId . Or you can specify clear-text values for AsmUserName , AsmPassword , and AsmServerName . You can’t specify both. For more information on creating this SecretsManagerOracleAsmSecret and the SecretsManagerOracleAsmAccessRoleArn and SecretsManagerOracleAsmSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerOracleAsmSecretId -> (string)\n\nRequired only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN, partial ARN, or friendly name of the SecretsManagerOracleAsmSecret that contains the Oracle ASM connection details for the Oracle endpoint.\n\nShorthand Syntax:\n\nAddSupplementalLogging=boolean,ArchivedLogDestId=integer,AdditionalArchivedLogDestId=integer,ExtraArchivedLogDestIds=integer,integer,AllowSelectNestedTables=boolean,ParallelAsmReadThreads=integer,ReadAheadBlocks=integer,AccessAlternateDirectly=boolean,UseAlternateFolderForOnline=boolean,OraclePathPrefix=string,UsePathPrefix=string,ReplacePathPrefix=boolean,EnableHomogenousTablespace=boolean,DirectPathNoLog=boolean,ArchivedLogsOnly=boolean,AsmPassword=string,AsmServer=string,AsmUser=string,CharLengthSemantics=string,DatabaseName=string,DirectPathParallelLoad=boolean,FailTasksOnLobTruncation=boolean,NumberDatatypeScale=integer,Password=string,Port=integer,ReadTableSpaceName=boolean,RetryInterval=integer,SecurityDbEncryption=string,SecurityDbEncryptionName=string,ServerName=string,SpatialDataOptionToGeoJsonFunctionName=string,StandbyDelayTime=integer,Username=string,UseBFile=boolean,UseDirectPathFullLoad=boolean,UseLogminerReader=boolean,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string,SecretsManagerOracleAsmAccessRoleArn=string,SecretsManagerOracleAsmSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"AddSupplementalLogging\": true|false,\n  \"ArchivedLogDestId\": integer,\n  \"AdditionalArchivedLogDestId\": integer,\n  \"ExtraArchivedLogDestIds\": [integer, ...],\n  \"AllowSelectNestedTables\": true|false,\n  \"ParallelAsmReadThreads\": integer,\n  \"ReadAheadBlocks\": integer,\n  \"AccessAlternateDirectly\": true|false,\n  \"UseAlternateFolderForOnline\": true|false,\n  \"OraclePathPrefix\": \"string\",\n  \"UsePathPrefix\": \"string\",\n  \"ReplacePathPrefix\": true|false,\n  \"EnableHomogenousTablespace\": true|false,\n  \"DirectPathNoLog\": true|false,\n  \"ArchivedLogsOnly\": true|false,\n  \"AsmPassword\": \"string\",\n  \"AsmServer\": \"string\",\n  \"AsmUser\": \"string\",\n  \"CharLengthSemantics\": \"default\"|\"char\"|\"byte\",\n  \"DatabaseName\": \"string\",\n  \"DirectPathParallelLoad\": true|false,\n  \"FailTasksOnLobTruncation\": true|false,\n  \"NumberDatatypeScale\": integer,\n  \"Password\": \"string\",\n  \"Port\": integer,\n  \"ReadTableSpaceName\": true|false,\n  \"RetryInterval\": integer,\n  \"SecurityDbEncryption\": \"string\",\n  \"SecurityDbEncryptionName\": \"string\",\n  \"ServerName\": \"string\",\n  \"SpatialDataOptionToGeoJsonFunctionName\": \"string\",\n  \"StandbyDelayTime\": integer,\n  \"Username\": \"string\",\n  \"UseBFile\": true|false,\n  \"UseDirectPathFullLoad\": true|false,\n  \"UseLogminerReader\": true|false,\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\",\n  \"SecretsManagerOracleAsmAccessRoleArn\": \"string\",\n  \"SecretsManagerOracleAsmSecretId\": \"string\"\n}\n\n\n--sybase-settings (structure)\n\nSettings in JSON format for the source and target SAP ASE endpoint. For information about other available settings, see Extra connection attributes when using SAP ASE as a source for DMS and Extra connection attributes when using SAP ASE as a target for DMS in the Database Migration Service User Guide.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the SAP ASE endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the SAP SAE endpoint connection details.\n\nShorthand Syntax:\n\nDatabaseName=string,Password=string,Port=integer,ServerName=string,Username=string,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"DatabaseName\": \"string\",\n  \"Password\": \"string\",\n  \"Port\": integer,\n  \"ServerName\": \"string\",\n  \"Username\": \"string\",\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--microsoft-sql-server-settings (structure)\n\nSettings in JSON format for the source and target Microsoft SQL Server endpoint. For information about other available settings, see Extra connection attributes when using SQL Server as a source for DMS and Extra connection attributes when using SQL Server as a target for DMS in the Database Migration Service User Guide.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nBcpPacketSize -> (integer)\n\nThe maximum size of the packets (in bytes) used to transfer data using BCP.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nControlTablesFileGroup -> (string)\n\nSpecifies a file group for the DMS internal tables. When the replication task starts, all the internal DMS control tables (awsdms_ apply_exception, awsdms_apply, awsdms_changes) are created for the specified file group.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nQuerySingleAlwaysOnNode -> (boolean)\n\nCleans and recreates table metadata information on the replication instance when a mismatch occurs. An example is a situation where running an alter DDL statement on a table might result in different information about the table cached in the replication instance.\n\nReadBackupOnly -> (boolean)\n\nWhen this attribute is set to Y , DMS only reads changes from transaction log backups and doesn’t read from the active transaction log file during ongoing replication. Setting this parameter to Y enables you to control active transaction log file growth during full load and ongoing replication tasks. However, it can add some source latency to ongoing replication.\n\nSafeguardPolicy -> (string)\n\nUse this attribute to minimize the need to access the backup log and enable DMS to prevent truncation using one of the following two methods.\n\nStart transactions in the database: This is the default method. When this method is used, DMS prevents TLOG truncation by mimicking a transaction in the database. As long as such a transaction is open, changes that appear after the transaction started aren’t truncated. If you need Microsoft Replication to be enabled in your database, then you must choose this method.\n\nExclusively use sp_repldone within a single task : When this method is used, DMS reads the changes and then uses sp_repldone to mark the TLOG transactions as ready for truncation. Although this method doesn’t involve any transactional activities, it can only be used when Microsoft Replication isn’t running. Also, when using this method, only one DMS task can access the database at any given time. Therefore, if you need to run parallel DMS tasks against the same database, use the default method.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nUseBcpFullLoad -> (boolean)\n\nUse this to attribute to transfer data for full-load operations using BCP. When the target table contains an identity column that does not exist in the source table, you must disable the use BCP for loading table option.\n\nUseThirdPartyBackupDevice -> (boolean)\n\nWhen this attribute is set to Y , DMS processes third-party transaction log backups if they are created in native format.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the SQL Server endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the SQL Server endpoint connection details.\n\nShorthand Syntax:\n\nPort=integer,BcpPacketSize=integer,DatabaseName=string,ControlTablesFileGroup=string,Password=string,QuerySingleAlwaysOnNode=boolean,ReadBackupOnly=boolean,SafeguardPolicy=string,ServerName=string,Username=string,UseBcpFullLoad=boolean,UseThirdPartyBackupDevice=boolean,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"Port\": integer,\n  \"BcpPacketSize\": integer,\n  \"DatabaseName\": \"string\",\n  \"ControlTablesFileGroup\": \"string\",\n  \"Password\": \"string\",\n  \"QuerySingleAlwaysOnNode\": true|false,\n  \"ReadBackupOnly\": true|false,\n  \"SafeguardPolicy\": \"rely-on-sql-server-replication-agent\"|\"exclusive-automatic-truncation\"|\"shared-automatic-truncation\",\n  \"ServerName\": \"string\",\n  \"Username\": \"string\",\n  \"UseBcpFullLoad\": true|false,\n  \"UseThirdPartyBackupDevice\": true|false,\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--ibm-db2-settings (structure)\n\nSettings in JSON format for the source IBM Db2 LUW endpoint. For information about other available settings, see Extra connection attributes when using Db2 LUW as a source for DMS in the Database Migration Service User Guide.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port. The default value is 50000.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nSetDataCaptureChanges -> (boolean)\n\nEnables ongoing replication (CDC) as a BOOLEAN value. The default is true.\n\nCurrentLsn -> (string)\n\nFor ongoing replication (CDC), use CurrentLSN to specify a log sequence number (LSN) where you want the replication to start.\n\nMaxKBytesPerRead -> (integer)\n\nMaximum number of bytes per read, as a NUMBER value. The default is 64 KB.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Db2 LUW endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Db2 LUW endpoint connection details.\n\nShorthand Syntax:\n\nDatabaseName=string,Password=string,Port=integer,ServerName=string,SetDataCaptureChanges=boolean,CurrentLsn=string,MaxKBytesPerRead=integer,Username=string,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"DatabaseName\": \"string\",\n  \"Password\": \"string\",\n  \"Port\": integer,\n  \"ServerName\": \"string\",\n  \"SetDataCaptureChanges\": true|false,\n  \"CurrentLsn\": \"string\",\n  \"MaxKBytesPerRead\": integer,\n  \"Username\": \"string\",\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--resource-identifier (string)\n\nA friendly name for the resource identifier at the end of the EndpointArn response parameter that is returned in the created Endpoint object. The value for this parameter can have up to 31 characters. It can contain only ASCII letters, digits, and hyphen (‘-‘). Also, it can’t end with a hyphen or contain two consecutive hyphens, and can only begin with a letter, such as Example-App-ARN1 . For example, this value might result in the EndpointArn value arn:aws:dms:eu-west-1:012345678901:rep:Example-App-ARN1 . If you don’t specify a ResourceIdentifier value, DMS generates a default identifier value for the end of EndpointArn .\n\n--doc-db-settings (structure)\n\nProvides information that defines a DocumentDB endpoint.\n\nUsername -> (string)\n\nThe user name you use to access the DocumentDB source endpoint.\n\nPassword -> (string)\n\nThe password for the user account you use to access the DocumentDB source endpoint.\n\nServerName -> (string)\n\nThe name of the server on the DocumentDB source endpoint.\n\nPort -> (integer)\n\nThe port value for the DocumentDB source endpoint.\n\nDatabaseName -> (string)\n\nThe database name on the DocumentDB source endpoint.\n\nNestingLevel -> (string)\n\nSpecifies either document or table mode.\n\nDefault value is \"none\" . Specify \"none\" to use document mode. Specify \"one\" to use table mode.\n\nExtractDocId -> (boolean)\n\nSpecifies the document ID. Use this setting when NestingLevel is set to \"none\" .\n\nDefault value is \"false\" .\n\nDocsToInvestigate -> (integer)\n\nIndicates the number of documents to preview to determine the document organization. Use this setting when NestingLevel is set to \"one\" .\n\nMust be a positive value greater than 0 . Default value is 1000 .\n\nKmsKeyId -> (string)\n\nThe KMS key identifier that is used to encrypt the content on the replication instance. If you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key. KMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the DocumentDB endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the DocumentDB endpoint connection details.\n\nShorthand Syntax:\n\nUsername=string,Password=string,ServerName=string,Port=integer,DatabaseName=string,NestingLevel=string,ExtractDocId=boolean,DocsToInvestigate=integer,KmsKeyId=string,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"Username\": \"string\",\n  \"Password\": \"string\",\n  \"ServerName\": \"string\",\n  \"Port\": integer,\n  \"DatabaseName\": \"string\",\n  \"NestingLevel\": \"none\"|\"one\",\n  \"ExtractDocId\": true|false,\n  \"DocsToInvestigate\": integer,\n  \"KmsKeyId\": \"string\",\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--redis-settings (structure)\n\nSettings in JSON format for the target Redis endpoint.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nPort -> (integer)\n\nTransmission Control Protocol (TCP) port for the endpoint.\n\nSslSecurityProtocol -> (string)\n\nThe connection to a Redis target endpoint using Transport Layer Security (TLS). Valid values include plaintext and ssl-encryption . The default is ssl-encryption . The ssl-encryption option makes an encrypted connection. Optionally, you can identify an Amazon Resource Name (ARN) for an SSL certificate authority (CA) using the SslCaCertificateArn setting. If an ARN isn’t given for a CA, DMS uses the Amazon root CA.\n\nThe plaintext option doesn’t provide Transport Layer Security (TLS) encryption for traffic between endpoint and database.\n\nAuthType -> (string)\n\nThe type of authentication to perform when connecting to a Redis target. Options include none , auth-token , and auth-role . The auth-token option requires an AuthPassword value to be provided. The auth-role option requires AuthUserName and AuthPassword values to be provided.\n\nAuthUserName -> (string)\n\nThe user name provided with the auth-role option of the AuthType setting for a Redis target endpoint.\n\nAuthPassword -> (string)\n\nThe password provided with the auth-role and auth-token options of the AuthType setting for a Redis target endpoint.\n\nSslCaCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the certificate authority (CA) that DMS uses to connect to your Redis target endpoint.\n\nShorthand Syntax:\n\nServerName=string,Port=integer,SslSecurityProtocol=string,AuthType=string,AuthUserName=string,AuthPassword=string,SslCaCertificateArn=string\n\n\nJSON Syntax:\n\n{\n  \"ServerName\": \"string\",\n  \"Port\": integer,\n  \"SslSecurityProtocol\": \"plaintext\"|\"ssl-encryption\",\n  \"AuthType\": \"none\"|\"auth-role\"|\"auth-token\",\n  \"AuthUserName\": \"string\",\n  \"AuthPassword\": \"string\",\n  \"SslCaCertificateArn\": \"string\"\n}\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nEndpoint -> (structure)\n\nThe endpoint that was created.\n\nEndpointIdentifier -> (string)\n\nThe database endpoint identifier. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen or contain two consecutive hyphens.\n\nEndpointType -> (string)\n\nThe type of endpoint. Valid values are source and target .\n\nEngineName -> (string)\n\nThe database engine name. Valid values, depending on the EndpointType, include \"mysql\" , \"oracle\" , \"postgres\" , \"mariadb\" , \"aurora\" , \"aurora-postgresql\" , \"redshift\" , \"s3\" , \"db2\" , \"azuredb\" , \"sybase\" , \"dynamodb\" , \"mongodb\" , \"kinesis\" , \"kafka\" , \"elasticsearch\" , \"documentdb\" , \"sqlserver\" , and \"neptune\" .\n\nEngineDisplayName -> (string)\n\nThe expanded name for the engine name. For example, if the EngineName parameter is “aurora,” this value would be “Amazon Aurora MySQL.”\n\nUsername -> (string)\n\nThe user name used to connect to the endpoint.\n\nServerName -> (string)\n\nThe name of the server at the endpoint.\n\nPort -> (integer)\n\nThe port value used to access the endpoint.\n\nDatabaseName -> (string)\n\nThe name of the database at the endpoint.\n\nExtraConnectionAttributes -> (string)\n\nAdditional connection attributes used to connect to the endpoint.\n\nStatus -> (string)\n\nThe status of the endpoint.\n\nKmsKeyId -> (string)\n\nAn KMS key identifier that is used to encrypt the connection parameters for the endpoint.\n\nIf you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key.\n\nKMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\nCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) used for SSL connection to the endpoint.\n\nSslMode -> (string)\n\nThe SSL mode used to connect to the endpoint. The default value is none .\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nExternalTableDefinition -> (string)\n\nThe external table definition.\n\nExternalId -> (string)\n\nValue returned by a call to CreateEndpoint that can be used for cross-account validation. Use it on a subsequent call to CreateEndpoint to create the endpoint with a cross-account.\n\nDynamoDbSettings -> (structure)\n\nThe settings for the DynamoDB target endpoint. For more information, see the DynamoDBSettings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nS3Settings -> (structure)\n\nThe settings for the S3 target endpoint. For more information, see the S3Settings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action. It is a required parameter that enables DMS to write and read objects from an S3 bucket.\n\nExternalTableDefinition -> (string)\n\nSpecifies how tables are defined in the S3 source files only.\n\nCsvRowDelimiter -> (string)\n\nThe delimiter used to separate rows in the .csv file for both source and target. The default is a carriage return (\\n ).\n\nCsvDelimiter -> (string)\n\nThe delimiter used to separate columns in the .csv file for both source and target. The default is a comma.\n\nBucketFolder -> (string)\n\nAn optional parameter to set a folder name in the S3 bucket. If provided, tables are created in the path `` bucketFolder /schema_name /table_name /`` . If this parameter isn’t specified, then the path used is `` schema_name /table_name /`` .\n\nBucketName -> (string)\n\nThe name of the S3 bucket.\n\nCompressionType -> (string)\n\nAn optional parameter to use GZIP to compress the target files. Set to GZIP to compress the target files. Either set this parameter to NONE (the default) or don’t use it to leave the files uncompressed. This parameter applies to both .csv and .parquet file formats.\n\nEncryptionMode -> (string)\n\nThe type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either SSE_S3 (the default) or SSE_KMS .\n\nNote\n\nFor the ModifyEndpoint operation, you can change the existing value of the EncryptionMode parameter from SSE_KMS to SSE_S3 . But you can’t change the existing value from SSE_S3 to SSE_KMS .\n\nTo use SSE_S3 , you need an Identity and Access Management (IAM) role with permission to allow \"arn:aws:s3:::dms-*\" to use the following actions:\n\ns3:CreateBucket\n\ns3:ListBucket\n\ns3:DeleteBucket\n\ns3:GetBucketLocation\n\ns3:GetObject\n\ns3:PutObject\n\ns3:DeleteObject\n\ns3:GetObjectVersion\n\ns3:GetBucketPolicy\n\ns3:PutBucketPolicy\n\ns3:DeleteBucketPolicy\n\nServerSideEncryptionKmsKeyId -> (string)\n\nIf you are using SSE_KMS for the EncryptionMode , provide the KMS key ID. The key that you use needs an attached policy that enables Identity and Access Management (IAM) user permissions and allows use of the key.\n\nHere is a CLI example: ``aws dms create-endpoint –endpoint-identifier value –endpoint-type target –engine-name s3 –s3-settings ServiceAccessRoleArn=*value* ,BucketFolder=*value* ,BucketName=*value* ,EncryptionMode=SSE_KMS,ServerSideEncryptionKmsKeyId=*value* ``\n\nDataFormat -> (string)\n\nThe format of the data that you want to use for output. You can choose one of the following:\n\ncsv : This is a row-based file format with comma-separated values (.csv).\n\nparquet : Apache Parquet (.parquet) is a columnar storage file format that features efficient compression and provides faster query response.\n\nEncodingType -> (string)\n\nThe type of encoding you are using:\n\nRLE_DICTIONARY uses a combination of bit-packing and run-length encoding to store repeated values more efficiently. This is the default.\n\nPLAIN doesn’t use encoding at all. Values are stored as they are.\n\nPLAIN_DICTIONARY builds a dictionary of the values encountered in a given column. The dictionary is stored in a dictionary page for each column chunk.\n\nDictPageSizeLimit -> (integer)\n\nThe maximum size of an encoded dictionary page of a column. If the dictionary page exceeds this, this column is stored using an encoding type of PLAIN . This parameter defaults to 1024 * 1024 bytes (1 MiB), the maximum size of a dictionary page before it reverts to PLAIN encoding. This size is used for .parquet file format only.\n\nRowGroupLength -> (integer)\n\nThe number of rows in a row group. A smaller row group size provides faster reads. But as the number of row groups grows, the slower writes become. This parameter defaults to 10,000 rows. This number is used for .parquet file format only.\n\nIf you choose a value larger than the maximum, RowGroupLength is set to the max row group length in bytes (64 * 1024 * 1024).\n\nDataPageSize -> (integer)\n\nThe size of one data page in bytes. This parameter defaults to 1024 * 1024 bytes (1 MiB). This number is used for .parquet file format only.\n\nParquetVersion -> (string)\n\nThe version of the Apache Parquet format that you want to use: parquet_1_0 (the default) or parquet_2_0 .\n\nEnableStatistics -> (boolean)\n\nA value that enables statistics for Parquet pages and row groups. Choose true to enable statistics, false to disable. Statistics include NULL , DISTINCT , MAX , and MIN values. This parameter defaults to true . This value is used for .parquet file format only.\n\nIncludeOpForFullLoad -> (boolean)\n\nA value that enables a full load to write INSERT operations to the comma-separated value (.csv) output files only to indicate how the rows were added to the source database.\n\nNote\n\nDMS supports the IncludeOpForFullLoad parameter in versions 3.1.4 and later.\n\nFor full load, records can only be inserted. By default (the false setting), no information is recorded in these output files for a full load to indicate that the rows were inserted at the source database. If IncludeOpForFullLoad is set to true or y , the INSERT is recorded as an I annotation in the first field of the .csv file. This allows the format of your target records from a full load to be consistent with the target records from a CDC load.\n\nNote\n\nThis setting works together with the CdcInsertsOnly and the CdcInsertsAndUpdates parameters for output to .csv files only. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nCdcInsertsOnly -> (boolean)\n\nA value that enables a change data capture (CDC) load to write only INSERT operations to .csv or columnar storage (.parquet) output files. By default (the false setting), the first field in a .csv or .parquet record contains the letter I (INSERT), U (UPDATE), or D (DELETE). These values indicate whether the row was inserted, updated, or deleted at the source database for a CDC load to the target.\n\nIf CdcInsertsOnly is set to true or y , only INSERTs from the source database are migrated to the .csv or .parquet file. For .csv format only, how these INSERTs are recorded depends on the value of IncludeOpForFullLoad . If IncludeOpForFullLoad is set to true , the first field of every CDC record is set to I to indicate the INSERT operation at the source. If IncludeOpForFullLoad is set to false , every CDC record is written without a first field to indicate the INSERT operation at the source. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nNote\n\nDMS supports the interaction described preceding between the CdcInsertsOnly and IncludeOpForFullLoad parameters in versions 3.1.4 and later.\n\nCdcInsertsOnly and CdcInsertsAndUpdates can’t both be set to true for the same endpoint. Set either CdcInsertsOnly or CdcInsertsAndUpdates to true for the same endpoint, but not both.\n\nTimestampColumnName -> (string)\n\nA value that when nonblank causes DMS to add a column with timestamp information to the endpoint data for an Amazon S3 target.\n\nNote\n\nDMS supports the TimestampColumnName parameter in versions 3.1.4 and later.\n\nDMS includes an additional STRING column in the .csv or .parquet object files of your migrated data when you set TimestampColumnName to a nonblank value.\n\nFor a full load, each row of this timestamp column contains a timestamp for when the data was transferred from the source to the target by DMS.\n\nFor a change data capture (CDC) load, each row of the timestamp column contains the timestamp for the commit of that row in the source database.\n\nThe string format for this timestamp column value is yyyy-MM-dd HH:mm:ss.SSSSSS . By default, the precision of this value is in microseconds. For a CDC load, the rounding of the precision depends on the commit timestamp supported by DMS for the source database.\n\nWhen the AddColumnName parameter is set to true , DMS also includes a name for the timestamp column that you set with TimestampColumnName .\n\nParquetTimestampInMillisecond -> (boolean)\n\nA value that specifies the precision of any TIMESTAMP column values that are written to an Amazon S3 object file in .parquet format.\n\nNote\n\nDMS supports the ParquetTimestampInMillisecond parameter in versions 3.1.4 and later.\n\nWhen ParquetTimestampInMillisecond is set to true or y , DMS writes all TIMESTAMP columns in a .parquet formatted file with millisecond precision. Otherwise, DMS writes them with microsecond precision.\n\nCurrently, Amazon Athena and Glue can handle only millisecond precision for TIMESTAMP values. Set this parameter to true for S3 endpoint object files that are .parquet formatted only if you plan to query or process the data with Athena or Glue.\n\nNote\n\nDMS writes any TIMESTAMP column values written to an S3 file in .csv format with microsecond precision.\n\nSetting ParquetTimestampInMillisecond has no effect on the string format of the timestamp column value that is inserted by setting the TimestampColumnName parameter.\n\nCdcInsertsAndUpdates -> (boolean)\n\nA value that enables a change data capture (CDC) load to write INSERT and UPDATE operations to .csv or .parquet (columnar storage) output files. The default setting is false , but when CdcInsertsAndUpdates is set to true or y , only INSERTs and UPDATEs from the source database are migrated to the .csv or .parquet file.\n\nFor .csv file format only, how these INSERTs and UPDATEs are recorded depends on the value of the IncludeOpForFullLoad parameter. If IncludeOpForFullLoad is set to true , the first field of every CDC record is set to either I or U to indicate INSERT and UPDATE operations at the source. But if IncludeOpForFullLoad is set to false , CDC records are written without an indication of INSERT or UPDATE operations at the source. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nNote\n\nDMS supports the use of the CdcInsertsAndUpdates parameter in versions 3.3.1 and later.\n\nCdcInsertsOnly and CdcInsertsAndUpdates can’t both be set to true for the same endpoint. Set either CdcInsertsOnly or CdcInsertsAndUpdates to true for the same endpoint, but not both.\n\nDatePartitionEnabled -> (boolean)\n\nWhen set to true , this parameter partitions S3 bucket folders based on transaction commit dates. The default value is false . For more information about date-based folder partitioning, see Using date-based folder partitioning .\n\nDatePartitionSequence -> (string)\n\nIdentifies the sequence of the date format to use during folder partitioning. The default value is YYYYMMDD . Use this parameter when DatePartitionedEnabled is set to true .\n\nDatePartitionDelimiter -> (string)\n\nSpecifies a date separating delimiter to use during folder partitioning. The default value is SLASH . Use this parameter when DatePartitionedEnabled is set to true .\n\nUseCsvNoSupValue -> (boolean)\n\nThis setting applies if the S3 output files during a change data capture (CDC) load are written in .csv format. If set to true for columns not included in the supplemental log, DMS uses the value specified by ` CsvNoSupValue https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CsvNoSupValue`__ . If not set or set to false , DMS uses the null value for these columns.\n\nNote\n\nThis setting is supported in DMS versions 3.4.1 and later.\n\nCsvNoSupValue -> (string)\n\nThis setting only applies if your Amazon S3 output files during a change data capture (CDC) load are written in .csv format. If ` UseCsvNoSupValue https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-UseCsvNoSupValue`__ is set to true, specify a string value that you want DMS to use for all columns not included in the supplemental log. If you do not specify a string value, DMS uses the null value for these columns regardless of the UseCsvNoSupValue setting.\n\nNote\n\nThis setting is supported in DMS versions 3.4.1 and later.\n\nPreserveTransactions -> (boolean)\n\nIf set to true , DMS saves the transaction order for a change data capture (CDC) load on the Amazon S3 target specified by ` CdcPath https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CdcPath`__ . For more information, see Capturing data changes (CDC) including transaction order on the S3 target .\n\nNote\n\nThis setting is supported in DMS versions 3.4.2 and later.\n\nCdcPath -> (string)\n\nSpecifies the folder path of CDC files. For an S3 source, this setting is required if a task captures change data; otherwise, it’s optional. If CdcPath is set, DMS reads CDC files from this path and replicates the data changes to the target endpoint. For an S3 target if you set ` PreserveTransactions https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-PreserveTransactions`__ to true , DMS verifies that you have set this parameter to a folder path on your S3 target where DMS can save the transaction order for the CDC load. DMS creates this CDC folder path in either your S3 target working directory or the S3 target location specified by ` BucketFolder https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketFolder`__ and ` BucketName https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketName`__ .\n\nFor example, if you specify CdcPath as MyChangedData , and you specify BucketName as MyTargetBucket but do not specify BucketFolder , DMS creates the CDC folder path following: MyTargetBucket/MyChangedData .\n\nIf you specify the same CdcPath , and you specify BucketName as MyTargetBucket and BucketFolder as MyTargetData , DMS creates the CDC folder path following: MyTargetBucket/MyTargetData/MyChangedData .\n\nFor more information on CDC including transaction order on an S3 target, see Capturing data changes (CDC) including transaction order on the S3 target .\n\nNote\n\nThis setting is supported in DMS versions 3.4.2 and later.\n\nCannedAclForObjects -> (string)\n\nA value that enables DMS to specify a predefined (canned) access control list for objects created in an Amazon S3 bucket as .csv or .parquet files. For more information about Amazon S3 canned ACLs, see Canned ACL in the Amazon S3 Developer Guide.\n\nThe default value is NONE. Valid values include NONE, PRIVATE, PUBLIC_READ, PUBLIC_READ_WRITE, AUTHENTICATED_READ, AWS_EXEC_READ, BUCKET_OWNER_READ, and BUCKET_OWNER_FULL_CONTROL.\n\nAddColumnName -> (boolean)\n\nAn optional parameter that, when set to true or y , you can use to add column name information to the .csv output file.\n\nThe default value is false . Valid values are true , false , y , and n .\n\nCdcMaxBatchInterval -> (integer)\n\nMaximum length of the interval, defined in seconds, after which to output a file to Amazon S3.\n\nWhen CdcMaxBatchInterval and CdcMinFileSize are both specified, the file write is triggered by whichever parameter condition is met first within an DMS CloudFormation template.\n\nThe default value is 60 seconds.\n\nCdcMinFileSize -> (integer)\n\nMinimum file size, defined in megabytes, to reach for a file output to Amazon S3.\n\nWhen CdcMinFileSize and CdcMaxBatchInterval are both specified, the file write is triggered by whichever parameter condition is met first within an DMS CloudFormation template.\n\nThe default value is 32 MB.\n\nCsvNullValue -> (string)\n\nAn optional parameter that specifies how DMS treats null values. While handling the null value, you can use this parameter to pass a user-defined string as null when writing to the target. For example, when target columns are not nullable, you can use this option to differentiate between the empty string value and the null value. So, if you set this parameter value to the empty string (“” or ‘’), DMS treats the empty string as the null value instead of NULL .\n\nThe default value is NULL . Valid values include any valid string.\n\nIgnoreHeaderRows -> (integer)\n\nWhen this value is set to 1, DMS ignores the first row header in a .csv file. A value of 1 turns on the feature; a value of 0 turns off the feature.\n\nThe default is 0.\n\nMaxFileSize -> (integer)\n\nA value that specifies the maximum size (in KB) of any .csv file to be created while migrating to an S3 target during full load.\n\nThe default value is 1,048,576 KB (1 GB). Valid values include 1 to 1,048,576.\n\nRfc4180 -> (boolean)\n\nFor an S3 source, when this value is set to true or y , each leading double quotation mark has to be followed by an ending double quotation mark. This formatting complies with RFC 4180. When this value is set to false or n , string literals are copied to the target as is. In this case, a delimiter (row or column) signals the end of the field. Thus, you can’t use a delimiter as part of the string, because it signals the end of the value.\n\nFor an S3 target, an optional parameter used to set behavior to comply with RFC 4180 for data migrated to Amazon S3 using .csv file format only. When this value is set to true or y using Amazon S3 as a target, if the data has quotation marks or newline characters in it, DMS encloses the entire column with an additional pair of double quotation marks (“). Every quotation mark within the data is repeated twice.\n\nThe default value is true . Valid values include true , false , y , and n .\n\nDmsTransferSettings -> (structure)\n\nThe settings in JSON format for the DMS transfer type of source endpoint.\n\nPossible settings include the following:\n\nServiceAccessRoleArn - - The Amazon Resource Name (ARN) used by the service access IAM role. The role must allow the iam:PassRole action.\n\nBucketName - The name of the S3 bucket to use.\n\nShorthand syntax for these settings is as follows: ServiceAccessRoleArn=string,BucketName=string,\n\nJSON syntax for these settings is as follows: { \"ServiceAccessRoleArn\": \"string\", \"BucketName\": \"string\"}\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service access IAM role. The role must allow the iam:PassRole action.\n\nBucketName -> (string)\n\nThe name of the S3 bucket to use.\n\nMongoDbSettings -> (structure)\n\nThe settings for the MongoDB source endpoint. For more information, see the MongoDbSettings structure.\n\nUsername -> (string)\n\nThe user name you use to access the MongoDB source endpoint.\n\nPassword -> (string)\n\nThe password for the user account you use to access the MongoDB source endpoint.\n\nServerName -> (string)\n\nThe name of the server on the MongoDB source endpoint.\n\nPort -> (integer)\n\nThe port value for the MongoDB source endpoint.\n\nDatabaseName -> (string)\n\nThe database name on the MongoDB source endpoint.\n\nAuthType -> (string)\n\nThe authentication type you use to access the MongoDB source endpoint.\n\nWhen when set to \"no\" , user name and password parameters are not used and can be empty.\n\nAuthMechanism -> (string)\n\nThe authentication mechanism you use to access the MongoDB source endpoint.\n\nFor the default value, in MongoDB version 2.x, \"default\" is \"mongodb_cr\" . For MongoDB version 3.x or later, \"default\" is \"scram_sha_1\" . This setting isn’t used when AuthType is set to \"no\" .\n\nNestingLevel -> (string)\n\nSpecifies either document or table mode.\n\nDefault value is \"none\" . Specify \"none\" to use document mode. Specify \"one\" to use table mode.\n\nExtractDocId -> (string)\n\nSpecifies the document ID. Use this setting when NestingLevel is set to \"none\" .\n\nDefault value is \"false\" .\n\nDocsToInvestigate -> (string)\n\nIndicates the number of documents to preview to determine the document organization. Use this setting when NestingLevel is set to \"one\" .\n\nMust be a positive value greater than 0 . Default value is 1000 .\n\nAuthSource -> (string)\n\nThe MongoDB database name. This setting isn’t used when AuthType is set to \"no\" .\n\nThe default is \"admin\" .\n\nKmsKeyId -> (string)\n\nThe KMS key identifier that is used to encrypt the content on the replication instance. If you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key. KMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the MongoDB endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the MongoDB endpoint connection details.\n\nKinesisSettings -> (structure)\n\nThe settings for the Amazon Kinesis target endpoint. For more information, see the KinesisSettings structure.\n\nStreamArn -> (string)\n\nThe Amazon Resource Name (ARN) for the Amazon Kinesis Data Streams endpoint.\n\nMessageFormat -> (string)\n\nThe output format for the records created on the endpoint. The message format is JSON (default) or JSON_UNFORMATTED (a single line with no tab).\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) for the IAM role that DMS uses to write to the Kinesis data stream. The role must allow the iam:PassRole action.\n\nIncludeTransactionDetails -> (boolean)\n\nProvides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for transaction_id , previous transaction_id , and transaction_record_id (the record offset within a transaction). The default is false .\n\nIncludePartitionValue -> (boolean)\n\nShows the partition value within the Kinesis message output, unless the partition type is schema-table-type . The default is false .\n\nPartitionIncludeSchemaTable -> (boolean)\n\nPrefixes schema and table names to partition values, when the partition type is primary-key-type . Doing this increases data distribution among Kinesis shards. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same shard, which causes throttling. The default is false .\n\nIncludeTableAlterOperations -> (boolean)\n\nIncludes any data definition language (DDL) operations that change the table in the control data, such as rename-table , drop-table , add-column , drop-column , and rename-column . The default is false .\n\nIncludeControlDetails -> (boolean)\n\nShows detailed control information for table definition, column definition, and table and column changes in the Kinesis message output. The default is false .\n\nIncludeNullAndEmpty -> (boolean)\n\nInclude NULL and empty columns for records migrated to the endpoint. The default is false .\n\nNoHexPrefix -> (boolean)\n\nSet this optional parameter to true to avoid adding a ‘0x’ prefix to raw data in hexadecimal format. For example, by default, DMS adds a ‘0x’ prefix to the LOB column type in hexadecimal format moving from an Oracle source to an Amazon Kinesis target. Use the NoHexPrefix endpoint setting to enable migration of RAW data type columns without adding the ‘0x’ prefix.\n\nKafkaSettings -> (structure)\n\nThe settings for the Apache Kafka target endpoint. For more information, see the KafkaSettings structure.\n\nBroker -> (string)\n\nA comma-separated list of one or more broker locations in your Kafka cluster that host your Kafka instance. Specify each broker location in the form `` broker-hostname-or-ip :port `` . For example, \"ec2-12-345-678-901.compute-1.amazonaws.com:2345\" . For more information and examples of specifying a list of broker locations, see Using Apache Kafka as a target for Database Migration Service in the Database Migration Service User Guide .\n\nTopic -> (string)\n\nThe topic to which you migrate the data. If you don’t specify a topic, DMS specifies \"kafka-default-topic\" as the migration topic.\n\nMessageFormat -> (string)\n\nThe output format for the records created on the endpoint. The message format is JSON (default) or JSON_UNFORMATTED (a single line with no tab).\n\nIncludeTransactionDetails -> (boolean)\n\nProvides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for transaction_id , previous transaction_id , and transaction_record_id (the record offset within a transaction). The default is false .\n\nIncludePartitionValue -> (boolean)\n\nShows the partition value within the Kafka message output unless the partition type is schema-table-type . The default is false .\n\nPartitionIncludeSchemaTable -> (boolean)\n\nPrefixes schema and table names to partition values, when the partition type is primary-key-type . Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. The default is false .\n\nIncludeTableAlterOperations -> (boolean)\n\nIncludes any data definition language (DDL) operations that change the table in the control data, such as rename-table , drop-table , add-column , drop-column , and rename-column . The default is false .\n\nIncludeControlDetails -> (boolean)\n\nShows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. The default is false .\n\nMessageMaxBytes -> (integer)\n\nThe maximum size in bytes for records created on the endpoint The default is 1,000,000.\n\nIncludeNullAndEmpty -> (boolean)\n\nInclude NULL and empty columns for records migrated to the endpoint. The default is false .\n\nSecurityProtocol -> (string)\n\nSet secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include ssl-encryption , ssl-authentication , and sasl-ssl . sasl-ssl requires SaslUsername and SaslPassword .\n\nSslClientCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) of the client certificate used to securely connect to a Kafka target endpoint.\n\nSslClientKeyArn -> (string)\n\nThe Amazon Resource Name (ARN) for the client private key used to securely connect to a Kafka target endpoint.\n\nSslClientKeyPassword -> (string)\n\nThe password for the client private key used to securely connect to a Kafka target endpoint.\n\nSslCaCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the private certificate authority (CA) cert that DMS uses to securely connect to your Kafka target endpoint.\n\nSaslUsername -> (string)\n\nThe secure user name you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n\nSaslPassword -> (string)\n\nThe secure password you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n\nNoHexPrefix -> (boolean)\n\nSet this optional parameter to true to avoid adding a ‘0x’ prefix to raw data in hexadecimal format. For example, by default, DMS adds a ‘0x’ prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the NoHexPrefix endpoint setting to enable migration of RAW data type columns without adding the ‘0x’ prefix.\n\nElasticsearchSettings -> (structure)\n\nThe settings for the Elasticsearch source endpoint. For more information, see the ElasticsearchSettings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nEndpointUri -> (string)\n\nThe endpoint for the Elasticsearch cluster. DMS uses HTTPS if a transport protocol (http/https) is not specified.\n\nFullLoadErrorPercentage -> (integer)\n\nThe maximum percentage of records that can fail to be written before a full load operation stops.\n\nTo avoid early failure, this counter is only effective after 1000 records are transferred. Elasticsearch also has the concept of error monitoring during the last 10 minutes of an Observation Window. If transfer of all records fail in the last 10 minutes, the full load operation stops.\n\nErrorRetryDuration -> (integer)\n\nThe maximum number of seconds for which DMS retries failed API requests to the Elasticsearch cluster.\n\nNeptuneSettings -> (structure)\n\nThe settings for the Amazon Neptune target endpoint. For more information, see the NeptuneSettings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the service role that you created for the Neptune target endpoint. The role must allow the iam:PassRole action. For more information, see Creating an IAM Service Role for Accessing Amazon Neptune as a Target in the Database Migration Service User Guide.\n\nS3BucketName -> (string)\n\nThe name of the Amazon S3 bucket where DMS can temporarily store migrated graph data in .csv files before bulk-loading it to the Neptune target database. DMS maps the SQL source data to graph data before storing it in these .csv files.\n\nS3BucketFolder -> (string)\n\nA folder path where you want DMS to store migrated graph data in the S3 bucket specified by S3BucketName\n\nErrorRetryDuration -> (integer)\n\nThe number of milliseconds for DMS to wait to retry a bulk-load of migrated graph data to the Neptune target database before raising an error. The default is 250.\n\nMaxFileSize -> (integer)\n\nThe maximum size in kilobytes of migrated graph data stored in a .csv file before DMS bulk-loads the data to the Neptune target database. The default is 1,048,576 KB. If the bulk load is successful, DMS clears the bucket, ready to store the next batch of migrated graph data.\n\nMaxRetryCount -> (integer)\n\nThe number of times for DMS to retry a bulk load of migrated graph data to the Neptune target database before raising an error. The default is 5.\n\nIamAuthEnabled -> (boolean)\n\nIf you want Identity and Access Management (IAM) authorization enabled for this endpoint, set this parameter to true . Then attach the appropriate IAM policy document to your service role specified by ServiceAccessRoleArn . The default is false .\n\nRedshiftSettings -> (structure)\n\nSettings for the Amazon Redshift endpoint.\n\nAcceptAnyDate -> (boolean)\n\nA value that indicates to allow any date format, including invalid formats such as 00/00/00 00:00:00, to be loaded without generating an error. You can choose true or false (the default).\n\nThis parameter applies only to TIMESTAMP and DATE columns. Always use ACCEPTANYDATE with the DATEFORMAT parameter. If the date format for the data doesn’t match the DATEFORMAT specification, Amazon Redshift inserts a NULL value into that field.\n\nAfterConnectScript -> (string)\n\nCode to run after connecting. This parameter should contain the code itself, not the name of a file containing the code.\n\nBucketFolder -> (string)\n\nAn S3 folder where the comma-separated-value (.csv) files are stored before being uploaded to the target Redshift cluster.\n\nFor full load mode, DMS converts source records into .csv files and loads them to the BucketFolder/TableID path. DMS uses the Redshift COPY command to upload the .csv files to the target table. The files are deleted once the COPY operation has finished. For more information, see COPY in the Amazon Redshift Database Developer Guide .\n\nFor change-data-capture (CDC) mode, DMS creates a NetChanges table, and loads the .csv files to this BucketFolder/NetChangesTableID path.\n\nBucketName -> (string)\n\nThe name of the intermediate S3 bucket used to store .csv files before uploading data to Redshift.\n\nCaseSensitiveNames -> (boolean)\n\nIf Amazon Redshift is configured to support case sensitive schema names, set CaseSensitiveNames to true . The default is false .\n\nCompUpdate -> (boolean)\n\nIf you set CompUpdate to true Amazon Redshift applies automatic compression if the table is empty. This applies even if the table columns already have encodings other than RAW . If you set CompUpdate to false , automatic compression is disabled and existing column encodings aren’t changed. The default is true .\n\nConnectionTimeout -> (integer)\n\nA value that sets the amount of time to wait (in milliseconds) before timing out, beginning from when you initially establish a connection.\n\nDatabaseName -> (string)\n\nThe name of the Amazon Redshift data warehouse (service) that you are working with.\n\nDateFormat -> (string)\n\nThe date format that you are using. Valid values are auto (case-sensitive), your date format string enclosed in quotes, or NULL. If this parameter is left unset (NULL), it defaults to a format of ‘YYYY-MM-DD’. Using auto recognizes most strings, even some that aren’t supported when you use a date format string.\n\nIf your date and time values use formats different from each other, set this to auto .\n\nEmptyAsNull -> (boolean)\n\nA value that specifies whether DMS should migrate empty CHAR and VARCHAR fields as NULL. A value of true sets empty CHAR and VARCHAR fields to null. The default is false .\n\nEncryptionMode -> (string)\n\nThe type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either SSE_S3 (the default) or SSE_KMS .\n\nNote\n\nFor the ModifyEndpoint operation, you can change the existing value of the EncryptionMode parameter from SSE_KMS to SSE_S3 . But you can’t change the existing value from SSE_S3 to SSE_KMS .\n\nTo use SSE_S3 , create an Identity and Access Management (IAM) role with a policy that allows \"arn:aws:s3:::*\" to use the following actions: \"s3:PutObject\", \"s3:ListBucket\"\n\nExplicitIds -> (boolean)\n\nThis setting is only valid for a full-load migration task. Set ExplicitIds to true to have tables with IDENTITY columns override their auto-generated values with explicit values loaded from the source data files used to populate the tables. The default is false .\n\nFileTransferUploadStreams -> (integer)\n\nThe number of threads used to upload a single file. This parameter accepts a value from 1 through 64. It defaults to 10.\n\nThe number of parallel streams used to upload a single .csv file to an S3 bucket using S3 Multipart Upload. For more information, see Multipart upload overview .\n\nFileTransferUploadStreams accepts a value from 1 through 64. It defaults to 10.\n\nLoadTimeout -> (integer)\n\nThe amount of time to wait (in milliseconds) before timing out of operations performed by DMS on a Redshift cluster, such as Redshift COPY, INSERT, DELETE, and UPDATE.\n\nMaxFileSize -> (integer)\n\nThe maximum size (in KB) of any .csv file used to load data on an S3 bucket and transfer data to Amazon Redshift. It defaults to 1048576KB (1 GB).\n\nPassword -> (string)\n\nThe password for the user named in the username property.\n\nPort -> (integer)\n\nThe port number for Amazon Redshift. The default value is 5439.\n\nRemoveQuotes -> (boolean)\n\nA value that specifies to remove surrounding quotation marks from strings in the incoming data. All characters within the quotation marks, including delimiters, are retained. Choose true to remove quotation marks. The default is false .\n\nReplaceInvalidChars -> (string)\n\nA list of characters that you want to replace. Use with ReplaceChars .\n\nReplaceChars -> (string)\n\nA value that specifies to replaces the invalid characters specified in ReplaceInvalidChars , substituting the specified characters instead. The default is \"?\" .\n\nServerName -> (string)\n\nThe name of the Amazon Redshift cluster you are using.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the IAM role that has access to the Amazon Redshift service. The role must allow the iam:PassRole action.\n\nServerSideEncryptionKmsKeyId -> (string)\n\nThe KMS key ID. If you are using SSE_KMS for the EncryptionMode , provide this key ID. The key that you use needs an attached policy that enables IAM user permissions and allows use of the key.\n\nTimeFormat -> (string)\n\nThe time format that you want to use. Valid values are auto (case-sensitive), 'timeformat_string' , 'epochsecs' , or 'epochmillisecs' . It defaults to 10. Using auto recognizes most strings, even some that aren’t supported when you use a time format string.\n\nIf your date and time values use formats different from each other, set this parameter to auto .\n\nTrimBlanks -> (boolean)\n\nA value that specifies to remove the trailing white space characters from a VARCHAR string. This parameter applies only to columns with a VARCHAR data type. Choose true to remove unneeded white space. The default is false .\n\nTruncateColumns -> (boolean)\n\nA value that specifies to truncate data in columns to the appropriate number of characters, so that the data fits in the column. This parameter applies only to columns with a VARCHAR or CHAR data type, and rows with a size of 4 MB or less. Choose true to truncate data. The default is false .\n\nUsername -> (string)\n\nAn Amazon Redshift user name for a registered user.\n\nWriteBufferSize -> (integer)\n\nThe size (in KB) of the in-memory file write buffer used when generating .csv files on the local disk at the DMS replication instance. The default value is 1000 (buffer size is 1000KB).\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Amazon Redshift endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Amazon Redshift endpoint connection details.\n\nPostgreSQLSettings -> (structure)\n\nThe settings for the PostgreSQL source and target endpoint. For more information, see the PostgreSQLSettings structure.\n\nAfterConnectScript -> (string)\n\nFor use with change data capture (CDC) only, this attribute has DMS bypass foreign keys and user triggers to reduce the time it takes to bulk load data.\n\nExample: afterConnectScript=SET session_replication_role='replica'\n\nCaptureDdls -> (boolean)\n\nTo capture DDL events, DMS creates various artifacts in the PostgreSQL database when the task starts. You can later remove these artifacts.\n\nIf this value is set to N , you don’t have to create tables or triggers on the source database.\n\nMaxFileSize -> (integer)\n\nSpecifies the maximum size (in KB) of any .csv file used to transfer data to PostgreSQL.\n\nExample: maxFileSize=512\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nDdlArtifactsSchema -> (string)\n\nThe schema in which the operational DDL database artifacts are created.\n\nExample: ddlArtifactsSchema=xyzddlschema;\n\nExecuteTimeout -> (integer)\n\nSets the client statement timeout for the PostgreSQL instance, in seconds. The default value is 60 seconds.\n\nExample: executeTimeout=100;\n\nFailTasksOnLobTruncation -> (boolean)\n\nWhen set to true , this value causes a task to fail if the actual size of a LOB column is greater than the specified LobMaxSize .\n\nIf task is set to Limited LOB mode and this option is set to true, the task fails instead of truncating the LOB data.\n\nHeartbeatEnable -> (boolean)\n\nThe write-ahead log (WAL) heartbeat feature mimics a dummy transaction. By doing this, it prevents idle logical replication slots from holding onto old WAL logs, which can result in storage full situations on the source. This heartbeat keeps restart_lsn moving and prevents storage full scenarios.\n\nHeartbeatSchema -> (string)\n\nSets the schema in which the heartbeat artifacts are created.\n\nHeartbeatFrequency -> (integer)\n\nSets the WAL heartbeat frequency (in minutes).\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSlotName -> (string)\n\nSets the name of a previously created logical replication slot for a change data capture (CDC) load of the PostgreSQL source instance.\n\nWhen used with the CdcStartPosition request parameter for the DMS API , this attribute also makes it possible to use native CDC start points. DMS verifies that the specified logical replication slot exists before starting the CDC load task. It also verifies that the task was created with a valid setting of CdcStartPosition . If the specified slot doesn’t exist or the task doesn’t have a valid CdcStartPosition setting, DMS raises an error.\n\nFor more information about setting the CdcStartPosition request parameter, see Determining a CDC native start point in the Database Migration Service User Guide . For more information about using CdcStartPosition , see CreateReplicationTask , StartReplicationTask , and ModifyReplicationTask .\n\nPluginName -> (string)\n\nSpecifies the plugin to use to create a replication slot.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the PostgreSQL endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the PostgreSQL endpoint connection details.\n\nMySQLSettings -> (structure)\n\nThe settings for the MySQL source and target endpoint. For more information, see the MySQLSettings structure.\n\nAfterConnectScript -> (string)\n\nSpecifies a script to run immediately after DMS connects to the endpoint. The migration task continues running regardless if the SQL statement succeeds or fails.\n\nFor this parameter, provide the code of the script itself, not the name of a file containing the script.\n\nCleanSourceMetadataOnMismatch -> (boolean)\n\nAdjusts the behavior of DMS when migrating from an SQL Server source database that is hosted as part of an Always On availability group cluster. If you need DMS to poll all the nodes in the Always On cluster for transaction backups, set this attribute to false .\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint. For a MySQL source or target endpoint, don’t explicitly specify the database using the DatabaseName request parameter on either the CreateEndpoint or ModifyEndpoint API call. Specifying DatabaseName when you create or modify a MySQL endpoint replicates all the task tables to this single database. For MySQL endpoints, you specify the database only when you specify the schema in the table-mapping rules of the DMS task.\n\nEventsPollInterval -> (integer)\n\nSpecifies how often to check the binary log for new changes/events when the database is idle.\n\nExample: eventsPollInterval=5;\n\nIn the example, DMS checks for changes in the binary logs every five seconds.\n\nTargetDbType -> (string)\n\nSpecifies where to migrate source tables on the target, either to a single database or multiple databases.\n\nExample: targetDbType=MULTIPLE_DATABASES\n\nMaxFileSize -> (integer)\n\nSpecifies the maximum size (in KB) of any .csv file used to transfer data to a MySQL-compatible database.\n\nExample: maxFileSize=512\n\nParallelLoadThreads -> (integer)\n\nImproves performance when loading data into the MySQL-compatible target database. Specifies how many threads to use to load the data into the MySQL-compatible target database. Setting a large number of threads can have an adverse effect on database performance, because a separate connection is required for each thread.\n\nExample: parallelLoadThreads=1\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nServerTimezone -> (string)\n\nSpecifies the time zone for the source MySQL database.\n\nExample: serverTimezone=US/Pacific;\n\nNote: Do not enclose time zones in single quotes.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the MySQL endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the MySQL endpoint connection details.\n\nOracleSettings -> (structure)\n\nThe settings for the Oracle source and target endpoint. For more information, see the OracleSettings structure.\n\nAddSupplementalLogging -> (boolean)\n\nSet this attribute to set up table-level supplemental logging for the Oracle database. This attribute enables PRIMARY KEY supplemental logging on all tables selected for a migration task.\n\nIf you use this option, you still need to enable database-level supplemental logging.\n\nArchivedLogDestId -> (integer)\n\nSpecifies the ID of the destination for the archived redo logs. This value should be the same as a number in the dest_id column of the v$archived_log view. If you work with an additional redo log destination, use the AdditionalArchivedLogDestId option to specify the additional destination ID. Doing this improves performance by ensuring that the correct logs are accessed from the outset.\n\nAdditionalArchivedLogDestId -> (integer)\n\nSet this attribute with ArchivedLogDestId in a primary/ standby setup. This attribute is useful in the case of a switchover. In this case, DMS needs to know which destination to get archive redo logs from to read changes. This need arises because the previous primary instance is now a standby instance after switchover.\n\nAlthough DMS supports the use of the Oracle RESETLOGS option to open the database, never use RESETLOGS unless necessary. For additional information about RESETLOGS , see RMAN Data Repair Concepts in the Oracle Database Backup and Recovery User’s Guide .\n\nExtraArchivedLogDestIds -> (list)\n\nSpecifies the IDs of one more destinations for one or more archived redo logs. These IDs are the values of the dest_id column in the v$archived_log view. Use this setting with the archivedLogDestId extra connection attribute in a primary-to-single setup or a primary-to-multiple-standby setup.\n\nThis setting is useful in a switchover when you use an Oracle Data Guard database as a source. In this case, DMS needs information about what destination to get archive redo logs from to read changes. DMS needs this because after the switchover the previous primary is a standby instance. For example, in a primary-to-single standby setup you might apply the following settings.\n\narchivedLogDestId=1; ExtraArchivedLogDestIds=[2]\n\nIn a primary-to-multiple-standby setup, you might apply the following settings.\n\narchivedLogDestId=1; ExtraArchivedLogDestIds=[2,3,4]\n\nAlthough DMS supports the use of the Oracle RESETLOGS option to open the database, never use RESETLOGS unless it’s necessary. For more information about RESETLOGS , see RMAN Data Repair Concepts in the Oracle Database Backup and Recovery User’s Guide .\n\n(integer)\n\nAllowSelectNestedTables -> (boolean)\n\nSet this attribute to true to enable replication of Oracle tables containing columns that are nested tables or defined types.\n\nParallelAsmReadThreads -> (integer)\n\nSet this attribute to change the number of threads that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 2 (the default) and 8 (the maximum). Use this attribute together with the readAheadBlocks attribute.\n\nReadAheadBlocks -> (integer)\n\nSet this attribute to change the number of read-ahead blocks that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 1000 (the default) and 200,000 (the maximum).\n\nAccessAlternateDirectly -> (boolean)\n\nSet this attribute to false in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to not access redo logs through any specified path prefix replacement using direct file access.\n\nUseAlternateFolderForOnline -> (boolean)\n\nSet this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to use any specified prefix replacement to access all online redo logs.\n\nOraclePathPrefix -> (string)\n\nSet this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the default Oracle root used to access the redo logs.\n\nUsePathPrefix -> (string)\n\nSet this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the path prefix used to replace the default Oracle root to access the redo logs.\n\nReplacePathPrefix -> (boolean)\n\nSet this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This setting tells DMS instance to replace the default Oracle root with the specified usePathPrefix setting to access the redo logs.\n\nEnableHomogenousTablespace -> (boolean)\n\nSet this attribute to enable homogenous tablespace replication and create existing tables or indexes under the same tablespace on the target.\n\nDirectPathNoLog -> (boolean)\n\nWhen set to true , this attribute helps to increase the commit rate on the Oracle target database by writing directly to tables and not writing a trail to database logs.\n\nArchivedLogsOnly -> (boolean)\n\nWhen this field is set to Y , DMS only accesses the archived redo logs. If the archived redo logs are stored on Oracle ASM only, the DMS user account needs to be granted ASM privileges.\n\nAsmPassword -> (string)\n\nFor an Oracle source endpoint, your Oracle Automatic Storage Management (ASM) password. You can set this value from the `` asm_user_password `` value. You set this value as part of the comma-separated value that you set to the Password request parameter when you create the endpoint to access transaction logs using Binary Reader. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nAsmServer -> (string)\n\nFor an Oracle source endpoint, your ASM server address. You can set this value from the asm_server value. You set asm_server as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nAsmUser -> (string)\n\nFor an Oracle source endpoint, your ASM user name. You can set this value from the asm_user value. You set asm_user as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nCharLengthSemantics -> (string)\n\nSpecifies whether the length of a character column is in bytes or in characters. To indicate that the character column length is in characters, set this attribute to CHAR . Otherwise, the character column length is in bytes.\n\nExample: charLengthSemantics=CHAR;\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nDirectPathParallelLoad -> (boolean)\n\nWhen set to true , this attribute specifies a parallel load when useDirectPathFullLoad is set to Y . This attribute also only applies when you use the DMS parallel load feature. Note that the target table cannot have any constraints or indexes.\n\nFailTasksOnLobTruncation -> (boolean)\n\nWhen set to true , this attribute causes a task to fail if the actual size of an LOB column is greater than the specified LobMaxSize .\n\nIf a task is set to limited LOB mode and this option is set to true , the task fails instead of truncating the LOB data.\n\nNumberDatatypeScale -> (integer)\n\nSpecifies the number scale. You can select a scale up to 38, or you can select FLOAT. By default, the NUMBER data type is converted to precision 38, scale 10.\n\nExample: numberDataTypeScale=12\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nReadTableSpaceName -> (boolean)\n\nWhen set to true , this attribute supports tablespace replication.\n\nRetryInterval -> (integer)\n\nSpecifies the number of seconds that the system waits before resending a query.\n\nExample: retryInterval=6;\n\nSecurityDbEncryption -> (string)\n\nFor an Oracle source endpoint, the transparent data encryption (TDE) password required by AWM DMS to access Oracle redo logs encrypted by TDE using Binary Reader. It is also the `` TDE_Password `` part of the comma-separated value you set to the Password request parameter when you create the endpoint. The SecurityDbEncryptian setting is related to this SecurityDbEncryptionName setting. For more information, see Supported encryption methods for using Oracle as a source for DMS in the Database Migration Service User Guide .\n\nSecurityDbEncryptionName -> (string)\n\nFor an Oracle source endpoint, the name of a key used for the transparent data encryption (TDE) of the columns and tablespaces in an Oracle source database that is encrypted using TDE. The key value is the value of the SecurityDbEncryption setting. For more information on setting the key name value of SecurityDbEncryptionName , see the information and example for setting the securityDbEncryptionName extra connection attribute in Supported encryption methods for using Oracle as a source for DMS in the Database Migration Service User Guide .\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nSpatialDataOptionToGeoJsonFunctionName -> (string)\n\nUse this attribute to convert SDO_GEOMETRY to GEOJSON format. By default, DMS calls the SDO2GEOJSON custom function if present and accessible. Or you can create your own custom function that mimics the operation of SDOGEOJSON and set SpatialDataOptionToGeoJsonFunctionName to call it instead.\n\nStandbyDelayTime -> (integer)\n\nUse this attribute to specify a time in minutes for the delay in standby sync. If the source is an Oracle Active Data Guard standby database, use this attribute to specify the time lag between primary and standby databases.\n\nIn DMS, you can create an Oracle CDC task that uses an Active Data Guard standby instance as a source for replicating ongoing changes. Doing this eliminates the need to connect to an active database that might be in production.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nUseBFile -> (boolean)\n\nSet this attribute to Y to capture change data using the Binary Reader utility. Set UseLogminerReader to N to set this attribute to Y. To use Binary Reader with Amazon RDS for Oracle as the source, you set additional attributes. For more information about using this setting with Oracle Automatic Storage Management (ASM), see Using Oracle LogMiner or DMS Binary Reader for CDC .\n\nUseDirectPathFullLoad -> (boolean)\n\nSet this attribute to Y to have DMS use a direct path full load. Specify this value to use the direct path protocol in the Oracle Call Interface (OCI). By using this OCI protocol, you can bulk-load Oracle target tables during a full load.\n\nUseLogminerReader -> (boolean)\n\nSet this attribute to Y to capture change data using the Oracle LogMiner utility (the default). Set this attribute to N if you want to access the redo logs as a binary file. When you set UseLogminerReader to N, also set UseBfile to Y. For more information on this setting and using Oracle ASM, see Using Oracle LogMiner or DMS Binary Reader for CDC in the DMS User Guide .\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Oracle endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Oracle endpoint connection details.\n\nSecretsManagerOracleAsmAccessRoleArn -> (string)\n\nRequired only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the SecretsManagerOracleAsmSecret . This SecretsManagerOracleAsmSecret has the secret value that allows access to the Oracle ASM of the endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerOracleAsmSecretId . Or you can specify clear-text values for AsmUserName , AsmPassword , and AsmServerName . You can’t specify both. For more information on creating this SecretsManagerOracleAsmSecret and the SecretsManagerOracleAsmAccessRoleArn and SecretsManagerOracleAsmSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerOracleAsmSecretId -> (string)\n\nRequired only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN, partial ARN, or friendly name of the SecretsManagerOracleAsmSecret that contains the Oracle ASM connection details for the Oracle endpoint.\n\nSybaseSettings -> (structure)\n\nThe settings for the SAP ASE source and target endpoint. For more information, see the SybaseSettings structure.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the SAP ASE endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the SAP SAE endpoint connection details.\n\nMicrosoftSQLServerSettings -> (structure)\n\nThe settings for the Microsoft SQL Server source and target endpoint. For more information, see the MicrosoftSQLServerSettings structure.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nBcpPacketSize -> (integer)\n\nThe maximum size of the packets (in bytes) used to transfer data using BCP.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nControlTablesFileGroup -> (string)\n\nSpecifies a file group for the DMS internal tables. When the replication task starts, all the internal DMS control tables (awsdms_ apply_exception, awsdms_apply, awsdms_changes) are created for the specified file group.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nQuerySingleAlwaysOnNode -> (boolean)\n\nCleans and recreates table metadata information on the replication instance when a mismatch occurs. An example is a situation where running an alter DDL statement on a table might result in different information about the table cached in the replication instance.\n\nReadBackupOnly -> (boolean)\n\nWhen this attribute is set to Y , DMS only reads changes from transaction log backups and doesn’t read from the active transaction log file during ongoing replication. Setting this parameter to Y enables you to control active transaction log file growth during full load and ongoing replication tasks. However, it can add some source latency to ongoing replication.\n\nSafeguardPolicy -> (string)\n\nUse this attribute to minimize the need to access the backup log and enable DMS to prevent truncation using one of the following two methods.\n\nStart transactions in the database: This is the default method. When this method is used, DMS prevents TLOG truncation by mimicking a transaction in the database. As long as such a transaction is open, changes that appear after the transaction started aren’t truncated. If you need Microsoft Replication to be enabled in your database, then you must choose this method.\n\nExclusively use sp_repldone within a single task : When this method is used, DMS reads the changes and then uses sp_repldone to mark the TLOG transactions as ready for truncation. Although this method doesn’t involve any transactional activities, it can only be used when Microsoft Replication isn’t running. Also, when using this method, only one DMS task can access the database at any given time. Therefore, if you need to run parallel DMS tasks against the same database, use the default method.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nUseBcpFullLoad -> (boolean)\n\nUse this to attribute to transfer data for full-load operations using BCP. When the target table contains an identity column that does not exist in the source table, you must disable the use BCP for loading table option.\n\nUseThirdPartyBackupDevice -> (boolean)\n\nWhen this attribute is set to Y , DMS processes third-party transaction log backups if they are created in native format.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the SQL Server endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the SQL Server endpoint connection details.\n\nIBMDb2Settings -> (structure)\n\nThe settings for the IBM Db2 LUW source endpoint. For more information, see the IBMDb2Settings structure.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port. The default value is 50000.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nSetDataCaptureChanges -> (boolean)\n\nEnables ongoing replication (CDC) as a BOOLEAN value. The default is true.\n\nCurrentLsn -> (string)\n\nFor ongoing replication (CDC), use CurrentLSN to specify a log sequence number (LSN) where you want the replication to start.\n\nMaxKBytesPerRead -> (integer)\n\nMaximum number of bytes per read, as a NUMBER value. The default is 64 KB.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Db2 LUW endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Db2 LUW endpoint connection details.\n\nDocDbSettings -> (structure)\n\nProvides information that defines a DocumentDB endpoint.\n\nUsername -> (string)\n\nThe user name you use to access the DocumentDB source endpoint.\n\nPassword -> (string)\n\nThe password for the user account you use to access the DocumentDB source endpoint.\n\nServerName -> (string)\n\nThe name of the server on the DocumentDB source endpoint.\n\nPort -> (integer)\n\nThe port value for the DocumentDB source endpoint.\n\nDatabaseName -> (string)\n\nThe database name on the DocumentDB source endpoint.\n\nNestingLevel -> (string)\n\nSpecifies either document or table mode.\n\nDefault value is \"none\" . Specify \"none\" to use document mode. Specify \"one\" to use table mode.\n\nExtractDocId -> (boolean)\n\nSpecifies the document ID. Use this setting when NestingLevel is set to \"none\" .\n\nDefault value is \"false\" .\n\nDocsToInvestigate -> (integer)\n\nIndicates the number of documents to preview to determine the document organization. Use this setting when NestingLevel is set to \"one\" .\n\nMust be a positive value greater than 0 . Default value is 1000 .\n\nKmsKeyId -> (string)\n\nThe KMS key identifier that is used to encrypt the content on the replication instance. If you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key. KMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the DocumentDB endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the DocumentDB endpoint connection details.\n\nRedisSettings -> (structure)\n\nThe settings for the Redis target endpoint. For more information, see the RedisSettings structure.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nPort -> (integer)\n\nTransmission Control Protocol (TCP) port for the endpoint.\n\nSslSecurityProtocol -> (string)\n\nThe connection to a Redis target endpoint using Transport Layer Security (TLS). Valid values include plaintext and ssl-encryption . The default is ssl-encryption . The ssl-encryption option makes an encrypted connection. Optionally, you can identify an Amazon Resource Name (ARN) for an SSL certificate authority (CA) using the SslCaCertificateArn setting. If an ARN isn’t given for a CA, DMS uses the Amazon root CA.\n\nThe plaintext option doesn’t provide Transport Layer Security (TLS) encryption for traffic between endpoint and database.\n\nAuthType -> (string)\n\nThe type of authentication to perform when connecting to a Redis target. Options include none , auth-token , and auth-role . The auth-token option requires an AuthPassword value to be provided. The auth-role option requires AuthUserName and AuthPassword values to be provided.\n\nAuthUserName -> (string)\n\nThe user name provided with the auth-role option of the AuthType setting for a Redis target endpoint.\n\nAuthPassword -> (string)\n\nThe password provided with the auth-role and auth-token options of the AuthType setting for a Redis target endpoint.\n\nSslCaCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the certificate authority (CA) that DMS uses to connect to your Redis target endpoint.",
      "command_examples": "Examples\n\nTo create an endpoint\n\nThe following create-endpoint example creates an endpoint for an Amazon S3 source.\n\naws dms create-endpoint \\\n    --endpoint-type source \\\n    --engine-name s3 \\\n    --endpoint-identifier src-endpoint \\\n    --s3-settings file://s3-settings.json\n\n\nContents of s3-settings.json:\n\n{\n    \"BucketName\":\"my-corp-data\",\n    \"BucketFolder\":\"sourcedata\",\n    \"ServiceAccessRoleArn\":\"arn:aws:iam::123456789012:role/my-s3-access-role\"\n}\n\n\nOutput:\n\n{\n    \"Endpoint\": {\n        \"EndpointIdentifier\": \"src-endpoint\",\n        \"EndpointType\": \"SOURCE\",\n        \"EngineName\": \"s3\",\n        \"EngineDisplayName\": \"Amazon S3\",\n        \"ExtraConnectionAttributes\": \"bucketFolder=sourcedata;bucketName=my-corp-data;compressionType=NONE;csvDelimiter=,;csvRowDelimiter=\\\\n;\",\n        \"Status\": \"active\",\n        \"EndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:GUVAFG34EECUOJ6QVZ56DAHT3U\",\n        \"SslMode\": \"none\",\n        \"ServiceAccessRoleArn\": \"arn:aws:iam::123456789012:role/my-s3-access-role\",\n        \"S3Settings\": {\n            \"ServiceAccessRoleArn\": \"arn:aws:iam::123456789012:role/my-s3-access-role\",\n            \"CsvRowDelimiter\": \"\\\\n\",\n            \"CsvDelimiter\": \",\",\n            \"BucketFolder\": \"sourcedata\",\n            \"BucketName\": \"my-corp-data\",\n            \"CompressionType\": \"NONE\",\n            \"EnableStatistics\": true\n        }\n    }\n}\n\n\nFor more information, see Working with AWS DMS Endpoints in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "create-event-subscription",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/create-event-subscription.html",
      "command_description": "Description\n\nCreates an DMS event notification subscription.\n\nYou can specify the type of source (SourceType ) you want to be notified of, provide a list of DMS source IDs (SourceIds ) that triggers the events, and provide a list of event categories (EventCategories ) for events you want to be notified of. If you specify both the SourceType and SourceIds , such as SourceType = replication-instance and SourceIdentifier = my-replinstance , you will be notified of all the replication instance events for the specified source. If you specify a SourceType but don’t specify a SourceIdentifier , you receive notice of the events for that source type for all your DMS sources. If you don’t specify either SourceType nor SourceIdentifier , you will be notified of events generated from all DMS sources belonging to your customer account.\n\nFor more information about DMS events, see Working with Events and Notifications in the Database Migration Service User Guide.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  create-event-subscription\n--subscription-name <value>\n--sns-topic-arn <value>\n[--source-type <value>]\n[--event-categories <value>]\n[--source-ids <value>]\n[--enabled | --no-enabled]\n[--tags <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--subscription-name <value>",
        "--sns-topic-arn <value>",
        "[--source-type <value>]",
        "[--event-categories <value>]",
        "[--source-ids <value>]",
        "[--enabled | --no-enabled]",
        "[--tags <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--subscription-name (string)\n\nThe name of the DMS event notification subscription. This name must be less than 255 characters.\n\n--sns-topic-arn (string)\n\nThe Amazon Resource Name (ARN) of the Amazon SNS topic created for event notification. The ARN is created by Amazon SNS when you create a topic and subscribe to it.\n\n--source-type (string)\n\nThe type of DMS resource that generates the events. For example, if you want to be notified of events generated by a replication instance, you set this parameter to replication-instance . If this value isn’t specified, all events are returned.\n\nValid values: replication-instance | replication-task\n\n--event-categories (list)\n\nA list of event categories for a source type that you want to subscribe to. For more information, see Working with Events and Notifications in the Database Migration Service User Guide.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--source-ids (list)\n\nA list of identifiers for which DMS provides notification events.\n\nIf you don’t specify a value, notifications are provided for all sources.\n\nIf you specify multiple values, they must be of the same type. For example, if you specify a database instance ID, then all of the other values must be database instance IDs.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--enabled | --no-enabled (boolean)\n\nA Boolean value; set to true to activate the subscription, or set to false to create the subscription but not activate it.\n\n--tags (list)\n\nOne or more tags to be assigned to the event subscription.\n\n(structure)\n\nA user-defined key-value pair that describes metadata added to an DMS resource and that is used by operations such as the following:\n\nAddTagsToResource\n\nListTagsForResource\n\nRemoveTagsFromResource\n\nKey -> (string)\n\nA key is the required name of the tag. The string value can be 1-128 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nValue -> (string)\n\nA value is the optional value of the tag. The string value can be 1-256 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the resource for which the tag is created.\n\nShorthand Syntax:\n\nKey=string,Value=string,ResourceArn=string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Key\": \"string\",\n    \"Value\": \"string\",\n    \"ResourceArn\": \"string\"\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nEventSubscription -> (structure)\n\nThe event subscription that was created.\n\nCustomerAwsId -> (string)\n\nThe Amazon Web Services customer account associated with the DMS event notification subscription.\n\nCustSubscriptionId -> (string)\n\nThe DMS event notification subscription Id.\n\nSnsTopicArn -> (string)\n\nThe topic ARN of the DMS event notification subscription.\n\nStatus -> (string)\n\nThe status of the DMS event notification subscription.\n\nConstraints:\n\nCan be one of the following: creating | modifying | deleting | active | no-permission | topic-not-exist\n\nThe status “no-permission” indicates that DMS no longer has permission to post to the SNS topic. The status “topic-not-exist” indicates that the topic was deleted after the subscription was created.\n\nSubscriptionCreationTime -> (string)\n\nThe time the DMS event notification subscription was created.\n\nSourceType -> (string)\n\nThe type of DMS resource that generates events.\n\nValid values: replication-instance | replication-server | security-group | replication-task\n\nSourceIdsList -> (list)\n\nA list of source Ids for the event subscription.\n\n(string)\n\nEventCategoriesList -> (list)\n\nA lists of event categories.\n\n(string)\n\nEnabled -> (boolean)\n\nBoolean value that indicates if the event subscription is enabled.",
      "command_examples": "Examples\n\nTo list event subscriptions\n\nThe following create-event-subscription example creates an event subscription to an Amazon SNS topic (my-sns-topic).\n\naws dms create-event-subscription \\\n    --subscription-name my-dms-events \\\n    --sns-topic-arn arn:aws:sns:us-east-1:123456789012:my-sns-topic\n\n\nOutput:\n\n{\n    \"EventSubscription\": {\n        \"CustomerAwsId\": \"123456789012\",\n        \"CustSubscriptionId\": \"my-dms-events\",\n        \"SnsTopicArn\": \"arn:aws:sns:us-east-1:123456789012:my-sns-topic\",\n        \"Status\": \"creating\",\n        \"SubscriptionCreationTime\": \"2020-05-21 21:58:38.598\",\n        \"Enabled\": true\n    }\n}\n\n\nFor more information, see Working with Events and Notifications in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "create-replication-instance",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/create-replication-instance.html",
      "command_description": "Description\n\nCreates the replication instance using the specified parameters.\n\nDMS requires that your account have certain roles with appropriate permissions before you can create a replication instance. For information on the required roles, see Creating the IAM Roles to Use With the CLI and DMS API . For information on the required permissions, see IAM Permissions Needed to Use DMS .\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  create-replication-instance\n--replication-instance-identifier <value>\n[--allocated-storage <value>]\n--replication-instance-class <value>\n[--vpc-security-group-ids <value>]\n[--availability-zone <value>]\n[--replication-subnet-group-identifier <value>]\n[--preferred-maintenance-window <value>]\n[--multi-az | --no-multi-az]\n[--engine-version <value>]\n[--auto-minor-version-upgrade | --no-auto-minor-version-upgrade]\n[--tags <value>]\n[--kms-key-id <value>]\n[--publicly-accessible | --no-publicly-accessible]\n[--dns-name-servers <value>]\n[--resource-identifier <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-instance-identifier <value>",
        "[--allocated-storage <value>]",
        "--replication-instance-class <value>",
        "[--vpc-security-group-ids <value>]",
        "[--availability-zone <value>]",
        "[--replication-subnet-group-identifier <value>]",
        "[--preferred-maintenance-window <value>]",
        "[--multi-az | --no-multi-az]",
        "[--engine-version <value>]",
        "[--auto-minor-version-upgrade | --no-auto-minor-version-upgrade]",
        "[--tags <value>]",
        "[--kms-key-id <value>]",
        "[--publicly-accessible | --no-publicly-accessible]",
        "[--dns-name-servers <value>]",
        "[--resource-identifier <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-instance-identifier (string)\n\nThe replication instance identifier. This parameter is stored as a lowercase string.\n\nConstraints:\n\nMust contain 1-63 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCan’t end with a hyphen or contain two consecutive hyphens.\n\nExample: myrepinstance\n\n--allocated-storage (integer)\n\nThe amount of storage (in gigabytes) to be initially allocated for the replication instance.\n\n--replication-instance-class (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class. For example to specify the instance class dms.c4.large, set this parameter to \"dms.c4.large\" .\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\n--vpc-security-group-ids (list)\n\nSpecifies the VPC security group to be used with the replication instance. The VPC security group must work with the VPC containing the replication instance.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--availability-zone (string)\n\nThe Availability Zone where the replication instance will be created. The default value is a random, system-chosen Availability Zone in the endpoint’s Amazon Web Services Region, for example: us-east-1d\n\n--replication-subnet-group-identifier (string)\n\nA subnet group to associate with the replication instance.\n\n--preferred-maintenance-window (string)\n\nThe weekly time range during which system maintenance can occur, in Universal Coordinated Time (UTC).\n\nFormat: ddd:hh24:mi-ddd:hh24:mi\n\nDefault: A 30-minute window selected at random from an 8-hour block of time per Amazon Web Services Region, occurring on a random day of the week.\n\nValid Days: Mon, Tue, Wed, Thu, Fri, Sat, Sun\n\nConstraints: Minimum 30-minute window.\n\n--multi-az | --no-multi-az (boolean)\n\nSpecifies whether the replication instance is a Multi-AZ deployment. You can’t set the AvailabilityZone parameter if the Multi-AZ parameter is set to true .\n\n--engine-version (string)\n\nThe engine version number of the replication instance.\n\nIf an engine version number is not specified when a replication instance is created, the default is the latest engine version available.\n\n--auto-minor-version-upgrade | --no-auto-minor-version-upgrade (boolean)\n\nA value that indicates whether minor engine upgrades are applied automatically to the replication instance during the maintenance window. This parameter defaults to true .\n\nDefault: true\n\n--tags (list)\n\nOne or more tags to be assigned to the replication instance.\n\n(structure)\n\nA user-defined key-value pair that describes metadata added to an DMS resource and that is used by operations such as the following:\n\nAddTagsToResource\n\nListTagsForResource\n\nRemoveTagsFromResource\n\nKey -> (string)\n\nA key is the required name of the tag. The string value can be 1-128 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nValue -> (string)\n\nA value is the optional value of the tag. The string value can be 1-256 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the resource for which the tag is created.\n\nShorthand Syntax:\n\nKey=string,Value=string,ResourceArn=string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Key\": \"string\",\n    \"Value\": \"string\",\n    \"ResourceArn\": \"string\"\n  }\n  ...\n]\n\n\n--kms-key-id (string)\n\nAn KMS key identifier that is used to encrypt the data on the replication instance.\n\nIf you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key.\n\nKMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\n--publicly-accessible | --no-publicly-accessible (boolean)\n\nSpecifies the accessibility options for the replication instance. A value of true represents an instance with a public IP address. A value of false represents an instance with a private IP address. The default value is true .\n\n--dns-name-servers (string)\n\nA list of custom DNS name servers supported for the replication instance to access your on-premise source or target database. This list overrides the default name servers supported by the replication instance. You can specify a comma-separated list of internet addresses for up to four on-premise DNS name servers. For example: \"1.1.1.1,2.2.2.2,3.3.3.3,4.4.4.4\"\n\n--resource-identifier (string)\n\nA friendly name for the resource identifier at the end of the EndpointArn response parameter that is returned in the created Endpoint object. The value for this parameter can have up to 31 characters. It can contain only ASCII letters, digits, and hyphen (‘-‘). Also, it can’t end with a hyphen or contain two consecutive hyphens, and can only begin with a letter, such as Example-App-ARN1 . For example, this value might result in the EndpointArn value arn:aws:dms:eu-west-1:012345678901:rep:Example-App-ARN1 . If you don’t specify a ResourceIdentifier value, DMS generates a default identifier value for the end of EndpointArn .\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationInstance -> (structure)\n\nThe replication instance that was created.\n\nReplicationInstanceIdentifier -> (string)\n\nThe replication instance identifier is a required parameter. This parameter is stored as a lowercase string.\n\nConstraints:\n\nMust contain 1-63 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nExample: myrepinstance\n\nReplicationInstanceClass -> (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class. It is a required parameter, although a default value is pre-selected in the DMS console.\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\nReplicationInstanceStatus -> (string)\n\nThe status of the replication instance. The possible return values include:\n\n\"available\"\n\n\"creating\"\n\n\"deleted\"\n\n\"deleting\"\n\n\"failed\"\n\n\"modifying\"\n\n\"upgrading\"\n\n\"rebooting\"\n\n\"resetting-master-credentials\"\n\n\"storage-full\"\n\n\"incompatible-credentials\"\n\n\"incompatible-network\"\n\n\"maintenance\"\n\nAllocatedStorage -> (integer)\n\nThe amount of storage (in gigabytes) that is allocated for the replication instance.\n\nInstanceCreateTime -> (timestamp)\n\nThe time the replication instance was created.\n\nVpcSecurityGroups -> (list)\n\nThe VPC security group for the instance.\n\n(structure)\n\nDescribes the status of a security group associated with the virtual private cloud (VPC) hosting your replication and DB instances.\n\nVpcSecurityGroupId -> (string)\n\nThe VPC security group ID.\n\nStatus -> (string)\n\nThe status of the VPC security group.\n\nAvailabilityZone -> (string)\n\nThe Availability Zone for the instance.\n\nReplicationSubnetGroup -> (structure)\n\nThe subnet group for the replication instance.\n\nReplicationSubnetGroupIdentifier -> (string)\n\nThe identifier of the replication instance subnet group.\n\nReplicationSubnetGroupDescription -> (string)\n\nA description for the replication subnet group.\n\nVpcId -> (string)\n\nThe ID of the VPC.\n\nSubnetGroupStatus -> (string)\n\nThe status of the subnet group.\n\nSubnets -> (list)\n\nThe subnets that are in the subnet group.\n\n(structure)\n\nIn response to a request by the DescribeReplicationSubnetGroups operation, this object identifies a subnet by its given Availability Zone, subnet identifier, and status.\n\nSubnetIdentifier -> (string)\n\nThe subnet identifier.\n\nSubnetAvailabilityZone -> (structure)\n\nThe Availability Zone of the subnet.\n\nName -> (string)\n\nThe name of the Availability Zone.\n\nSubnetStatus -> (string)\n\nThe status of the subnet.\n\nPreferredMaintenanceWindow -> (string)\n\nThe maintenance window times for the replication instance. Any pending upgrades to the replication instance are performed during this time.\n\nPendingModifiedValues -> (structure)\n\nThe pending modification values.\n\nReplicationInstanceClass -> (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class.\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\nAllocatedStorage -> (integer)\n\nThe amount of storage (in gigabytes) that is allocated for the replication instance.\n\nMultiAZ -> (boolean)\n\nSpecifies whether the replication instance is a Multi-AZ deployment. You can’t set the AvailabilityZone parameter if the Multi-AZ parameter is set to true .\n\nEngineVersion -> (string)\n\nThe engine version number of the replication instance.\n\nMultiAZ -> (boolean)\n\nSpecifies whether the replication instance is a Multi-AZ deployment. You can’t set the AvailabilityZone parameter if the Multi-AZ parameter is set to true .\n\nEngineVersion -> (string)\n\nThe engine version number of the replication instance.\n\nIf an engine version number is not specified when a replication instance is created, the default is the latest engine version available.\n\nWhen modifying a major engine version of an instance, also set AllowMajorVersionUpgrade to true .\n\nAutoMinorVersionUpgrade -> (boolean)\n\nBoolean value indicating if minor version upgrades will be automatically applied to the instance.\n\nKmsKeyId -> (string)\n\nAn KMS key identifier that is used to encrypt the data on the replication instance.\n\nIf you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key.\n\nKMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nReplicationInstanceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\nReplicationInstancePublicIpAddress -> (string)\n\nThe public IP address of the replication instance.\n\nReplicationInstancePrivateIpAddress -> (string)\n\nThe private IP address of the replication instance.\n\nReplicationInstancePublicIpAddresses -> (list)\n\nOne or more public IP addresses for the replication instance.\n\n(string)\n\nReplicationInstancePrivateIpAddresses -> (list)\n\nOne or more private IP addresses for the replication instance.\n\n(string)\n\nPubliclyAccessible -> (boolean)\n\nSpecifies the accessibility options for the replication instance. A value of true represents an instance with a public IP address. A value of false represents an instance with a private IP address. The default value is true .\n\nSecondaryAvailabilityZone -> (string)\n\nThe Availability Zone of the standby replication instance in a Multi-AZ deployment.\n\nFreeUntil -> (timestamp)\n\nThe expiration date of the free replication instance that is part of the Free DMS program.\n\nDnsNameServers -> (string)\n\nThe DNS name servers supported for the replication instance to access your on-premise source or target database.",
      "command_examples": "Examples\n\nTo create a replication instance\n\nThe following create-replication-instance example creates a replication instance.\n\naws dms create-replication-instance \\\n    --replication-instance-identifier my-repl-instance \\\n    --replication-instance-class dms.t2.micro \\\n    --allocated-storage 5\n\n\nOutput:\n\n{\n    \"ReplicationInstance\": {\n        \"ReplicationInstanceIdentifier\": \"my-repl-instance\",\n        \"ReplicationInstanceClass\": \"dms.t2.micro\",\n        \"ReplicationInstanceStatus\": \"creating\",\n        \"AllocatedStorage\": 5,\n        \"VpcSecurityGroups\": [\n            {\n                \"VpcSecurityGroupId\": \"sg-f839b688\",\n                \"Status\": \"active\"\n            }\n        ],\n        \"ReplicationSubnetGroup\": {\n            \"ReplicationSubnetGroupIdentifier\": \"default\",\n            \"ReplicationSubnetGroupDescription\": \"default\",\n            \"VpcId\": \"vpc-136a4c6a\",\n            \"SubnetGroupStatus\": \"Complete\",\n            \"Subnets\": [\n                {\n                    \"SubnetIdentifier\": \"subnet-da327bf6\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1a\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                },\n                {\n                    \"SubnetIdentifier\": \"subnet-42599426\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1d\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                },\n                {\n                    \"SubnetIdentifier\": \"subnet-bac383e0\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1c\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                },\n                {\n                    \"SubnetIdentifier\": \"subnet-6746046b\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1f\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                },\n                {\n                    \"SubnetIdentifier\": \"subnet-d7c825e8\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1e\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                },\n                {\n                    \"SubnetIdentifier\": \"subnet-cbfff283\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1b\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                }\n            ]\n        },\n        \"PreferredMaintenanceWindow\": \"sat:12:35-sat:13:05\",\n        \"PendingModifiedValues\": {},\n        \"MultiAZ\": false,\n        \"EngineVersion\": \"3.3.2\",\n        \"AutoMinorVersionUpgrade\": true,\n        \"KmsKeyId\": \"arn:aws:kms:us-east-1:123456789012:key/f7bc0f8e-1a3a-4ace-9faa-e8494fa3921a\",\n        \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:ZK2VQBUWFDBAWHIXHAYG5G2PKY\",\n        \"PubliclyAccessible\": true\n    }\n}\n\n\nFor more information, see Working with an AWS DMS Replication Instance in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "create-replication-subnet-group",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/create-replication-subnet-group.html",
      "command_description": "Description\n\nCreates a replication subnet group given a list of the subnet IDs in a VPC.\n\nThe VPC needs to have at least one subnet in at least two availability zones in the Amazon Web Services Region, otherwise the service will throw a ReplicationSubnetGroupDoesNotCoverEnoughAZs exception.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  create-replication-subnet-group\n--replication-subnet-group-identifier <value>\n--replication-subnet-group-description <value>\n--subnet-ids <value>\n[--tags <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-subnet-group-identifier <value>",
        "--replication-subnet-group-description <value>",
        "--subnet-ids <value>",
        "[--tags <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-subnet-group-identifier (string)\n\nThe name for the replication subnet group. This value is stored as a lowercase string.\n\nConstraints: Must contain no more than 255 alphanumeric characters, periods, spaces, underscores, or hyphens. Must not be “default”.\n\nExample: mySubnetgroup\n\n--replication-subnet-group-description (string)\n\nThe description for the subnet group.\n\n--subnet-ids (list)\n\nOne or more subnet IDs to be assigned to the subnet group.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--tags (list)\n\nOne or more tags to be assigned to the subnet group.\n\n(structure)\n\nA user-defined key-value pair that describes metadata added to an DMS resource and that is used by operations such as the following:\n\nAddTagsToResource\n\nListTagsForResource\n\nRemoveTagsFromResource\n\nKey -> (string)\n\nA key is the required name of the tag. The string value can be 1-128 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nValue -> (string)\n\nA value is the optional value of the tag. The string value can be 1-256 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the resource for which the tag is created.\n\nShorthand Syntax:\n\nKey=string,Value=string,ResourceArn=string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Key\": \"string\",\n    \"Value\": \"string\",\n    \"ResourceArn\": \"string\"\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationSubnetGroup -> (structure)\n\nThe replication subnet group that was created.\n\nReplicationSubnetGroupIdentifier -> (string)\n\nThe identifier of the replication instance subnet group.\n\nReplicationSubnetGroupDescription -> (string)\n\nA description for the replication subnet group.\n\nVpcId -> (string)\n\nThe ID of the VPC.\n\nSubnetGroupStatus -> (string)\n\nThe status of the subnet group.\n\nSubnets -> (list)\n\nThe subnets that are in the subnet group.\n\n(structure)\n\nIn response to a request by the DescribeReplicationSubnetGroups operation, this object identifies a subnet by its given Availability Zone, subnet identifier, and status.\n\nSubnetIdentifier -> (string)\n\nThe subnet identifier.\n\nSubnetAvailabilityZone -> (structure)\n\nThe Availability Zone of the subnet.\n\nName -> (string)\n\nThe name of the Availability Zone.\n\nSubnetStatus -> (string)\n\nThe status of the subnet.",
      "command_examples": "Examples\n\nTo create a subnet group\n\nThe following create-replication-subnet-group example creates a group consisting of 3 subnets.\n\naws dms create-replication-subnet-group \\\n    --replication-subnet-group-identifier my-subnet-group \\\n    --replication-subnet-group-description \"my subnet group\" \\\n    --subnet-ids subnet-da327bf6 subnet-bac383e0 subnet-d7c825e8\n\n\nOutput:\n\n{\n    \"ReplicationSubnetGroup\": {\n        \"ReplicationSubnetGroupIdentifier\": \"my-subnet-group\",\n        \"ReplicationSubnetGroupDescription\": \"my subnet group\",\n        \"VpcId\": \"vpc-136a4c6a\",\n        \"SubnetGroupStatus\": \"Complete\",\n        \"Subnets\": [\n            {\n                \"SubnetIdentifier\": \"subnet-da327bf6\",\n                \"SubnetAvailabilityZone\": {\n                    \"Name\": \"us-east-1a\"\n                },\n                \"SubnetStatus\": \"Active\"\n            },\n            {\n                \"SubnetIdentifier\": \"subnet-bac383e0\",\n                \"SubnetAvailabilityZone\": {\n                    \"Name\": \"us-east-1c\"\n                },\n                \"SubnetStatus\": \"Active\"\n            },\n            {\n                \"SubnetIdentifier\": \"subnet-d7c825e8\",\n                \"SubnetAvailabilityZone\": {\n                    \"Name\": \"us-east-1e\"\n                },\n                \"SubnetStatus\": \"Active\"\n            }\n        ]\n    }\n}\n\n\nFor more information, see Setting Up a Network for a Replication Instance in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "create-replication-task",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/create-replication-task.html",
      "command_description": "Description\n\nCreates a replication task using the specified parameters.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  create-replication-task\n--replication-task-identifier <value>\n--source-endpoint-arn <value>\n--target-endpoint-arn <value>\n--replication-instance-arn <value>\n--migration-type <value>\n--table-mappings <value>\n[--replication-task-settings <value>]\n[--cdc-start-time <value>]\n[--cdc-start-position <value>]\n[--cdc-stop-position <value>]\n[--tags <value>]\n[--task-data <value>]\n[--resource-identifier <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-task-identifier <value>",
        "--source-endpoint-arn <value>",
        "--target-endpoint-arn <value>",
        "--replication-instance-arn <value>",
        "--migration-type <value>",
        "--table-mappings <value>",
        "[--replication-task-settings <value>]",
        "[--cdc-start-time <value>]",
        "[--cdc-start-position <value>]",
        "[--cdc-stop-position <value>]",
        "[--tags <value>]",
        "[--task-data <value>]",
        "[--resource-identifier <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-identifier (string)\n\nAn identifier for the replication task.\n\nConstraints:\n\nMust contain 1-255 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\n--source-endpoint-arn (string)\n\nAn Amazon Resource Name (ARN) that uniquely identifies the source endpoint.\n\n--target-endpoint-arn (string)\n\nAn Amazon Resource Name (ARN) that uniquely identifies the target endpoint.\n\n--replication-instance-arn (string)\n\nThe Amazon Resource Name (ARN) of a replication instance.\n\n--migration-type (string)\n\nThe migration type. Valid values: full-load | cdc | full-load-and-cdc\n\nPossible values:\n\nfull-load\n\ncdc\n\nfull-load-and-cdc\n\n--table-mappings (string)\n\nThe table mappings for the task, in JSON format. For more information, see Using Table Mapping to Specify Task Settings in the Database Migration Service User Guide.\n\n--replication-task-settings (string)\n\nOverall settings for the task, in JSON format. For more information, see Specifying Task Settings for Database Migration Service Tasks in the Database Migration Service User Guide.\n\n--cdc-start-time (timestamp)\n\nIndicates the start time for a change data capture (CDC) operation. Use either CdcStartTime or CdcStartPosition to specify when you want a CDC operation to start. Specifying both values results in an error.\n\nTimestamp Example: –cdc-start-time “2018-03-08T12:12:12”\n\n--cdc-start-position (string)\n\nIndicates when you want a change data capture (CDC) operation to start. Use either CdcStartPosition or CdcStartTime to specify when you want a CDC operation to start. Specifying both values results in an error.\n\nThe value can be in date, checkpoint, or LSN/SCN format.\n\nDate Example: –cdc-start-position “2018-03-08T12:12:12”\n\nCheckpoint Example: –cdc-start-position “checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93”\n\nLSN Example: –cdc-start-position “mysql-bin-changelog.000024:373”\n\nNote\n\nWhen you use this task setting with a source PostgreSQL database, a logical replication slot should already be created and associated with the source endpoint. You can verify this by setting the slotName extra connection attribute to the name of this logical replication slot. For more information, see Extra Connection Attributes When Using PostgreSQL as a Source for DMS .\n\n--cdc-stop-position (string)\n\nIndicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time.\n\nServer time example: –cdc-stop-position “server_time:2018-02-09T12:12:12”\n\nCommit time example: –cdc-stop-position “commit_time: 2018-02-09T12:12:12 “\n\n--tags (list)\n\nOne or more tags to be assigned to the replication task.\n\n(structure)\n\nA user-defined key-value pair that describes metadata added to an DMS resource and that is used by operations such as the following:\n\nAddTagsToResource\n\nListTagsForResource\n\nRemoveTagsFromResource\n\nKey -> (string)\n\nA key is the required name of the tag. The string value can be 1-128 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nValue -> (string)\n\nA value is the optional value of the tag. The string value can be 1-256 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the resource for which the tag is created.\n\nShorthand Syntax:\n\nKey=string,Value=string,ResourceArn=string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Key\": \"string\",\n    \"Value\": \"string\",\n    \"ResourceArn\": \"string\"\n  }\n  ...\n]\n\n\n--task-data (string)\n\nSupplemental information that the task requires to migrate the data for certain source and target endpoints. For more information, see Specifying Supplemental Data for Task Settings in the Database Migration Service User Guide.\n\n--resource-identifier (string)\n\nA friendly name for the resource identifier at the end of the EndpointArn response parameter that is returned in the created Endpoint object. The value for this parameter can have up to 31 characters. It can contain only ASCII letters, digits, and hyphen (‘-‘). Also, it can’t end with a hyphen or contain two consecutive hyphens, and can only begin with a letter, such as Example-App-ARN1 . For example, this value might result in the EndpointArn value arn:aws:dms:eu-west-1:012345678901:rep:Example-App-ARN1 . If you don’t specify a ResourceIdentifier value, DMS generates a default identifier value for the end of EndpointArn .\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationTask -> (structure)\n\nThe replication task that was created.\n\nReplicationTaskIdentifier -> (string)\n\nThe user-assigned replication task identifier or name.\n\nConstraints:\n\nMust contain 1-255 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nSourceEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) that uniquely identifies the endpoint.\n\nTargetEndpointArn -> (string)\n\nThe ARN that uniquely identifies the endpoint.\n\nReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance.\n\nMigrationType -> (string)\n\nThe type of migration.\n\nTableMappings -> (string)\n\nTable mappings specified in the task.\n\nReplicationTaskSettings -> (string)\n\nThe settings for the replication task.\n\nStatus -> (string)\n\nThe status of the replication task. This response parameter can return one of the following values:\n\n\"moving\" – The task is being moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"creating\" – The task is being created in response to running the ` CreateReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_CreateReplicationTask.html`__ operation.\n\n\"deleting\" – The task is being deleted in response to running the ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ operation.\n\n\"failed\" – The task failed to successfully complete the database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"failed-move\" – The task failed to move in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"modifying\" – The task definition is being modified in response to running the ` ModifyReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_ModifyReplicationTask.html`__ operation.\n\n\"ready\" – The task is in a ready state where it can respond to other task operations, such as ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ or ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ .\n\n\"running\" – The task is performing a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"starting\" – The task is preparing to perform a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"stopped\" – The task has stopped in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"stopping\" – The task is preparing to stop in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"testing\" – The database migration specified for this task is being tested in response to running either the ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ or the ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation.\n\nNote\n\n` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ is an improved premigration task assessment operation. The ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation assesses data type compatibility only between the source and target database of a given migration task. In contrast, ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ enables you to specify a variety of premigration task assessments in addition to data type compatibility. These assessments include ones for the validity of primary key definitions and likely issues with database migration performance, among others.\n\nLastFailureMessage -> (string)\n\nThe last error (failure) message generated for the replication task.\n\nStopReason -> (string)\n\nThe reason the replication task was stopped. This response parameter can return one of the following values:\n\n\"STOP_REASON_FULL_LOAD_COMPLETED\" – Full-load migration completed.\n\n\"STOP_REASON_CACHED_CHANGES_APPLIED\" – Change data capture (CDC) load completed.\n\n\"STOP_REASON_CACHED_CHANGES_NOT_APPLIED\" – In a full-load and CDC migration, the full load stopped as specified before starting the CDC migration.\n\n\"STOP_REASON_SERVER_TIME\" – The migration stopped at the specified server time.\n\nReplicationTaskCreationDate -> (timestamp)\n\nThe date the replication task was created.\n\nReplicationTaskStartDate -> (timestamp)\n\nThe date the replication task is scheduled to start.\n\nCdcStartPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to start. Use either CdcStartPosition or CdcStartTime to specify when you want the CDC operation to start. Specifying both values results in an error.\n\nThe value can be in date, checkpoint, or LSN/SCN format.\n\nDate Example: –cdc-start-position “2018-03-08T12:12:12”\n\nCheckpoint Example: –cdc-start-position “checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93”\n\nLSN Example: –cdc-start-position “mysql-bin-changelog.000024:373”\n\nCdcStopPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time.\n\nServer time example: –cdc-stop-position “server_time:2018-02-09T12:12:12”\n\nCommit time example: –cdc-stop-position “commit_time: 2018-02-09T12:12:12 “\n\nRecoveryCheckpoint -> (string)\n\nIndicates the last checkpoint that occurred during a change data capture (CDC) operation. You can provide this value to the CdcStartPosition parameter to start a CDC operation that begins at that checkpoint.\n\nReplicationTaskArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\nReplicationTaskStats -> (structure)\n\nThe statistics for the task, including elapsed time, tables loaded, and table errors.\n\nFullLoadProgressPercent -> (integer)\n\nThe percent complete for the full load migration task.\n\nElapsedTimeMillis -> (long)\n\nThe elapsed time of the task, in milliseconds.\n\nTablesLoaded -> (integer)\n\nThe number of tables loaded for this task.\n\nTablesLoading -> (integer)\n\nThe number of tables currently loading for this task.\n\nTablesQueued -> (integer)\n\nThe number of tables queued for this task.\n\nTablesErrored -> (integer)\n\nThe number of errors that have occurred during this task.\n\nFreshStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a target reload.\n\nStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a resume. For more information, see StartReplicationTaskType .\n\nStopDate -> (timestamp)\n\nThe date the replication task was stopped.\n\nFullLoadStartDate -> (timestamp)\n\nThe date the replication task full load was started.\n\nFullLoadFinishDate -> (timestamp)\n\nThe date the replication task full load was completed.\n\nTaskData -> (string)\n\nSupplemental information that the task requires to migrate the data for certain source and target endpoints. For more information, see Specifying Supplemental Data for Task Settings in the Database Migration Service User Guide.\n\nTargetReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance to which this task is moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation. Otherwise, this response parameter isn’t a member of the ReplicationTask object.",
      "command_examples": "Examples\n\nTo create a replication task\n\nThe following create-replication-task example creates a replication task.\n\naws dms create-replication-task \\\n    --replication-task-identifier movedata \\\n    --source-endpoint-arn arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA \\\n    --target-endpoint-arn arn:aws:dms:us-east-1:123456789012:endpoint:EOM4SFKCZEYHZBFGAGZT3QEC5U \\\n    --replication-instance-arn $RI_ARN \\\n    --migration-type full-load \\\n    --table-mappings file://table-mappings.json\n\n\nContents of table-mappings.json:\n\n{\n    \"rules\": [\n        {\n            \"rule-type\": \"selection\",\n            \"rule-id\": \"1\",\n            \"rule-name\": \"1\",\n            \"object-locator\": {\n                \"schema-name\": \"prodrep\",\n                \"table-name\": \"%\"\n            },\n            \"rule-action\": \"include\",\n            \"filters\": []\n        }\n    ]\n}\n\n\nOutput:\n\n{\n    \"ReplicationTask\": {\n        \"ReplicationTaskIdentifier\": \"moveit2\",\n        \"SourceEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\",\n        \"TargetEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:EOM4SFKCZEYHZBFGAGZT3QEC5U\",\n        \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n        \"MigrationType\": \"full-load\",\n        \"TableMappings\": ...output omitted... ,\n        \"ReplicationTaskSettings\": ...output omitted... ,\n        \"Status\": \"creating\",\n        \"ReplicationTaskCreationDate\": 1590524772.505,\n        \"ReplicationTaskArn\": \"arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\"\n    }\n}\n\n\nFor more information, see Working with AWS DMS Tasks in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "delete-certificate",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/delete-certificate.html",
      "command_description": "Description\n\nDeletes the specified certificate.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-certificate\n--certificate-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--certificate-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--certificate-arn (string)\n\nThe Amazon Resource Name (ARN) of the deleted certificate.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nCertificate -> (structure)\n\nThe Secure Sockets Layer (SSL) certificate.\n\nCertificateIdentifier -> (string)\n\nA customer-assigned name for the certificate. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen or contain two consecutive hyphens.\n\nCertificateCreationDate -> (timestamp)\n\nThe date that the certificate was created.\n\nCertificatePem -> (string)\n\nThe contents of a .pem file, which contains an X.509 certificate.\n\nCertificateWallet -> (blob)\n\nThe location of an imported Oracle Wallet certificate for use with SSL.\n\nCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the certificate.\n\nCertificateOwner -> (string)\n\nThe owner of the certificate.\n\nValidFromDate -> (timestamp)\n\nThe beginning date that the certificate is valid.\n\nValidToDate -> (timestamp)\n\nThe final date that the certificate is valid.\n\nSigningAlgorithm -> (string)\n\nThe signing algorithm for the certificate.\n\nKeyLength -> (integer)\n\nThe key length of the cryptographic algorithm being used."
    },
    {
      "command_name": "delete-connection",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/delete-connection.html",
      "command_description": "Description\n\nDeletes the connection between a replication instance and an endpoint.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-connection\n--endpoint-arn <value>\n--replication-instance-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--endpoint-arn <value>",
        "--replication-instance-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--endpoint-arn (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\n--replication-instance-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nConnection -> (structure)\n\nThe connection that is being deleted.\n\nReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance.\n\nEndpointArn -> (string)\n\nThe ARN string that uniquely identifies the endpoint.\n\nStatus -> (string)\n\nThe connection status. This parameter can return one of the following values:\n\n\"successful\"\n\n\"testing\"\n\n\"failed\"\n\n\"deleting\"\n\nLastFailureMessage -> (string)\n\nThe error message when the connection last failed.\n\nEndpointIdentifier -> (string)\n\nThe identifier of the endpoint. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen or contain two consecutive hyphens.\n\nReplicationInstanceIdentifier -> (string)\n\nThe replication instance identifier. This parameter is stored as a lowercase string.",
      "command_examples": "Examples\n\nTo delete a connection\n\nThe following delete-connection example disassociates an endpoint from a replication instance.\n\naws dms delete-connection \\\n    --endpoint-arn arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA \\\n    --replication-instance-arn arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\n\n\nOutput:\n\n{\n    \"Connection\": {\n        \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n        \"EndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\",\n        \"Status\": \"deleting\",\n        \"EndpointIdentifier\": \"src-database-1\",\n        \"ReplicationInstanceIdentifier\": \"my-repl-instance\"\n    }\n}\n\n\nFor more information, see https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Endpoints.Creating.html in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "delete-endpoint",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/delete-endpoint.html",
      "command_description": "Description\n\nDeletes the specified endpoint.\n\nNote\n\nAll tasks associated with the endpoint must be deleted before you can delete the endpoint.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-endpoint\n--endpoint-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--endpoint-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--endpoint-arn (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nEndpoint -> (structure)\n\nThe endpoint that was deleted.\n\nEndpointIdentifier -> (string)\n\nThe database endpoint identifier. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen or contain two consecutive hyphens.\n\nEndpointType -> (string)\n\nThe type of endpoint. Valid values are source and target .\n\nEngineName -> (string)\n\nThe database engine name. Valid values, depending on the EndpointType, include \"mysql\" , \"oracle\" , \"postgres\" , \"mariadb\" , \"aurora\" , \"aurora-postgresql\" , \"redshift\" , \"s3\" , \"db2\" , \"azuredb\" , \"sybase\" , \"dynamodb\" , \"mongodb\" , \"kinesis\" , \"kafka\" , \"elasticsearch\" , \"documentdb\" , \"sqlserver\" , and \"neptune\" .\n\nEngineDisplayName -> (string)\n\nThe expanded name for the engine name. For example, if the EngineName parameter is “aurora,” this value would be “Amazon Aurora MySQL.”\n\nUsername -> (string)\n\nThe user name used to connect to the endpoint.\n\nServerName -> (string)\n\nThe name of the server at the endpoint.\n\nPort -> (integer)\n\nThe port value used to access the endpoint.\n\nDatabaseName -> (string)\n\nThe name of the database at the endpoint.\n\nExtraConnectionAttributes -> (string)\n\nAdditional connection attributes used to connect to the endpoint.\n\nStatus -> (string)\n\nThe status of the endpoint.\n\nKmsKeyId -> (string)\n\nAn KMS key identifier that is used to encrypt the connection parameters for the endpoint.\n\nIf you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key.\n\nKMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\nCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) used for SSL connection to the endpoint.\n\nSslMode -> (string)\n\nThe SSL mode used to connect to the endpoint. The default value is none .\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nExternalTableDefinition -> (string)\n\nThe external table definition.\n\nExternalId -> (string)\n\nValue returned by a call to CreateEndpoint that can be used for cross-account validation. Use it on a subsequent call to CreateEndpoint to create the endpoint with a cross-account.\n\nDynamoDbSettings -> (structure)\n\nThe settings for the DynamoDB target endpoint. For more information, see the DynamoDBSettings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nS3Settings -> (structure)\n\nThe settings for the S3 target endpoint. For more information, see the S3Settings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action. It is a required parameter that enables DMS to write and read objects from an S3 bucket.\n\nExternalTableDefinition -> (string)\n\nSpecifies how tables are defined in the S3 source files only.\n\nCsvRowDelimiter -> (string)\n\nThe delimiter used to separate rows in the .csv file for both source and target. The default is a carriage return (\\n ).\n\nCsvDelimiter -> (string)\n\nThe delimiter used to separate columns in the .csv file for both source and target. The default is a comma.\n\nBucketFolder -> (string)\n\nAn optional parameter to set a folder name in the S3 bucket. If provided, tables are created in the path `` bucketFolder /schema_name /table_name /`` . If this parameter isn’t specified, then the path used is `` schema_name /table_name /`` .\n\nBucketName -> (string)\n\nThe name of the S3 bucket.\n\nCompressionType -> (string)\n\nAn optional parameter to use GZIP to compress the target files. Set to GZIP to compress the target files. Either set this parameter to NONE (the default) or don’t use it to leave the files uncompressed. This parameter applies to both .csv and .parquet file formats.\n\nEncryptionMode -> (string)\n\nThe type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either SSE_S3 (the default) or SSE_KMS .\n\nNote\n\nFor the ModifyEndpoint operation, you can change the existing value of the EncryptionMode parameter from SSE_KMS to SSE_S3 . But you can’t change the existing value from SSE_S3 to SSE_KMS .\n\nTo use SSE_S3 , you need an Identity and Access Management (IAM) role with permission to allow \"arn:aws:s3:::dms-*\" to use the following actions:\n\ns3:CreateBucket\n\ns3:ListBucket\n\ns3:DeleteBucket\n\ns3:GetBucketLocation\n\ns3:GetObject\n\ns3:PutObject\n\ns3:DeleteObject\n\ns3:GetObjectVersion\n\ns3:GetBucketPolicy\n\ns3:PutBucketPolicy\n\ns3:DeleteBucketPolicy\n\nServerSideEncryptionKmsKeyId -> (string)\n\nIf you are using SSE_KMS for the EncryptionMode , provide the KMS key ID. The key that you use needs an attached policy that enables Identity and Access Management (IAM) user permissions and allows use of the key.\n\nHere is a CLI example: ``aws dms create-endpoint –endpoint-identifier value –endpoint-type target –engine-name s3 –s3-settings ServiceAccessRoleArn=*value* ,BucketFolder=*value* ,BucketName=*value* ,EncryptionMode=SSE_KMS,ServerSideEncryptionKmsKeyId=*value* ``\n\nDataFormat -> (string)\n\nThe format of the data that you want to use for output. You can choose one of the following:\n\ncsv : This is a row-based file format with comma-separated values (.csv).\n\nparquet : Apache Parquet (.parquet) is a columnar storage file format that features efficient compression and provides faster query response.\n\nEncodingType -> (string)\n\nThe type of encoding you are using:\n\nRLE_DICTIONARY uses a combination of bit-packing and run-length encoding to store repeated values more efficiently. This is the default.\n\nPLAIN doesn’t use encoding at all. Values are stored as they are.\n\nPLAIN_DICTIONARY builds a dictionary of the values encountered in a given column. The dictionary is stored in a dictionary page for each column chunk.\n\nDictPageSizeLimit -> (integer)\n\nThe maximum size of an encoded dictionary page of a column. If the dictionary page exceeds this, this column is stored using an encoding type of PLAIN . This parameter defaults to 1024 * 1024 bytes (1 MiB), the maximum size of a dictionary page before it reverts to PLAIN encoding. This size is used for .parquet file format only.\n\nRowGroupLength -> (integer)\n\nThe number of rows in a row group. A smaller row group size provides faster reads. But as the number of row groups grows, the slower writes become. This parameter defaults to 10,000 rows. This number is used for .parquet file format only.\n\nIf you choose a value larger than the maximum, RowGroupLength is set to the max row group length in bytes (64 * 1024 * 1024).\n\nDataPageSize -> (integer)\n\nThe size of one data page in bytes. This parameter defaults to 1024 * 1024 bytes (1 MiB). This number is used for .parquet file format only.\n\nParquetVersion -> (string)\n\nThe version of the Apache Parquet format that you want to use: parquet_1_0 (the default) or parquet_2_0 .\n\nEnableStatistics -> (boolean)\n\nA value that enables statistics for Parquet pages and row groups. Choose true to enable statistics, false to disable. Statistics include NULL , DISTINCT , MAX , and MIN values. This parameter defaults to true . This value is used for .parquet file format only.\n\nIncludeOpForFullLoad -> (boolean)\n\nA value that enables a full load to write INSERT operations to the comma-separated value (.csv) output files only to indicate how the rows were added to the source database.\n\nNote\n\nDMS supports the IncludeOpForFullLoad parameter in versions 3.1.4 and later.\n\nFor full load, records can only be inserted. By default (the false setting), no information is recorded in these output files for a full load to indicate that the rows were inserted at the source database. If IncludeOpForFullLoad is set to true or y , the INSERT is recorded as an I annotation in the first field of the .csv file. This allows the format of your target records from a full load to be consistent with the target records from a CDC load.\n\nNote\n\nThis setting works together with the CdcInsertsOnly and the CdcInsertsAndUpdates parameters for output to .csv files only. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nCdcInsertsOnly -> (boolean)\n\nA value that enables a change data capture (CDC) load to write only INSERT operations to .csv or columnar storage (.parquet) output files. By default (the false setting), the first field in a .csv or .parquet record contains the letter I (INSERT), U (UPDATE), or D (DELETE). These values indicate whether the row was inserted, updated, or deleted at the source database for a CDC load to the target.\n\nIf CdcInsertsOnly is set to true or y , only INSERTs from the source database are migrated to the .csv or .parquet file. For .csv format only, how these INSERTs are recorded depends on the value of IncludeOpForFullLoad . If IncludeOpForFullLoad is set to true , the first field of every CDC record is set to I to indicate the INSERT operation at the source. If IncludeOpForFullLoad is set to false , every CDC record is written without a first field to indicate the INSERT operation at the source. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nNote\n\nDMS supports the interaction described preceding between the CdcInsertsOnly and IncludeOpForFullLoad parameters in versions 3.1.4 and later.\n\nCdcInsertsOnly and CdcInsertsAndUpdates can’t both be set to true for the same endpoint. Set either CdcInsertsOnly or CdcInsertsAndUpdates to true for the same endpoint, but not both.\n\nTimestampColumnName -> (string)\n\nA value that when nonblank causes DMS to add a column with timestamp information to the endpoint data for an Amazon S3 target.\n\nNote\n\nDMS supports the TimestampColumnName parameter in versions 3.1.4 and later.\n\nDMS includes an additional STRING column in the .csv or .parquet object files of your migrated data when you set TimestampColumnName to a nonblank value.\n\nFor a full load, each row of this timestamp column contains a timestamp for when the data was transferred from the source to the target by DMS.\n\nFor a change data capture (CDC) load, each row of the timestamp column contains the timestamp for the commit of that row in the source database.\n\nThe string format for this timestamp column value is yyyy-MM-dd HH:mm:ss.SSSSSS . By default, the precision of this value is in microseconds. For a CDC load, the rounding of the precision depends on the commit timestamp supported by DMS for the source database.\n\nWhen the AddColumnName parameter is set to true , DMS also includes a name for the timestamp column that you set with TimestampColumnName .\n\nParquetTimestampInMillisecond -> (boolean)\n\nA value that specifies the precision of any TIMESTAMP column values that are written to an Amazon S3 object file in .parquet format.\n\nNote\n\nDMS supports the ParquetTimestampInMillisecond parameter in versions 3.1.4 and later.\n\nWhen ParquetTimestampInMillisecond is set to true or y , DMS writes all TIMESTAMP columns in a .parquet formatted file with millisecond precision. Otherwise, DMS writes them with microsecond precision.\n\nCurrently, Amazon Athena and Glue can handle only millisecond precision for TIMESTAMP values. Set this parameter to true for S3 endpoint object files that are .parquet formatted only if you plan to query or process the data with Athena or Glue.\n\nNote\n\nDMS writes any TIMESTAMP column values written to an S3 file in .csv format with microsecond precision.\n\nSetting ParquetTimestampInMillisecond has no effect on the string format of the timestamp column value that is inserted by setting the TimestampColumnName parameter.\n\nCdcInsertsAndUpdates -> (boolean)\n\nA value that enables a change data capture (CDC) load to write INSERT and UPDATE operations to .csv or .parquet (columnar storage) output files. The default setting is false , but when CdcInsertsAndUpdates is set to true or y , only INSERTs and UPDATEs from the source database are migrated to the .csv or .parquet file.\n\nFor .csv file format only, how these INSERTs and UPDATEs are recorded depends on the value of the IncludeOpForFullLoad parameter. If IncludeOpForFullLoad is set to true , the first field of every CDC record is set to either I or U to indicate INSERT and UPDATE operations at the source. But if IncludeOpForFullLoad is set to false , CDC records are written without an indication of INSERT or UPDATE operations at the source. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nNote\n\nDMS supports the use of the CdcInsertsAndUpdates parameter in versions 3.3.1 and later.\n\nCdcInsertsOnly and CdcInsertsAndUpdates can’t both be set to true for the same endpoint. Set either CdcInsertsOnly or CdcInsertsAndUpdates to true for the same endpoint, but not both.\n\nDatePartitionEnabled -> (boolean)\n\nWhen set to true , this parameter partitions S3 bucket folders based on transaction commit dates. The default value is false . For more information about date-based folder partitioning, see Using date-based folder partitioning .\n\nDatePartitionSequence -> (string)\n\nIdentifies the sequence of the date format to use during folder partitioning. The default value is YYYYMMDD . Use this parameter when DatePartitionedEnabled is set to true .\n\nDatePartitionDelimiter -> (string)\n\nSpecifies a date separating delimiter to use during folder partitioning. The default value is SLASH . Use this parameter when DatePartitionedEnabled is set to true .\n\nUseCsvNoSupValue -> (boolean)\n\nThis setting applies if the S3 output files during a change data capture (CDC) load are written in .csv format. If set to true for columns not included in the supplemental log, DMS uses the value specified by ` CsvNoSupValue https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CsvNoSupValue`__ . If not set or set to false , DMS uses the null value for these columns.\n\nNote\n\nThis setting is supported in DMS versions 3.4.1 and later.\n\nCsvNoSupValue -> (string)\n\nThis setting only applies if your Amazon S3 output files during a change data capture (CDC) load are written in .csv format. If ` UseCsvNoSupValue https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-UseCsvNoSupValue`__ is set to true, specify a string value that you want DMS to use for all columns not included in the supplemental log. If you do not specify a string value, DMS uses the null value for these columns regardless of the UseCsvNoSupValue setting.\n\nNote\n\nThis setting is supported in DMS versions 3.4.1 and later.\n\nPreserveTransactions -> (boolean)\n\nIf set to true , DMS saves the transaction order for a change data capture (CDC) load on the Amazon S3 target specified by ` CdcPath https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CdcPath`__ . For more information, see Capturing data changes (CDC) including transaction order on the S3 target .\n\nNote\n\nThis setting is supported in DMS versions 3.4.2 and later.\n\nCdcPath -> (string)\n\nSpecifies the folder path of CDC files. For an S3 source, this setting is required if a task captures change data; otherwise, it’s optional. If CdcPath is set, DMS reads CDC files from this path and replicates the data changes to the target endpoint. For an S3 target if you set ` PreserveTransactions https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-PreserveTransactions`__ to true , DMS verifies that you have set this parameter to a folder path on your S3 target where DMS can save the transaction order for the CDC load. DMS creates this CDC folder path in either your S3 target working directory or the S3 target location specified by ` BucketFolder https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketFolder`__ and ` BucketName https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketName`__ .\n\nFor example, if you specify CdcPath as MyChangedData , and you specify BucketName as MyTargetBucket but do not specify BucketFolder , DMS creates the CDC folder path following: MyTargetBucket/MyChangedData .\n\nIf you specify the same CdcPath , and you specify BucketName as MyTargetBucket and BucketFolder as MyTargetData , DMS creates the CDC folder path following: MyTargetBucket/MyTargetData/MyChangedData .\n\nFor more information on CDC including transaction order on an S3 target, see Capturing data changes (CDC) including transaction order on the S3 target .\n\nNote\n\nThis setting is supported in DMS versions 3.4.2 and later.\n\nCannedAclForObjects -> (string)\n\nA value that enables DMS to specify a predefined (canned) access control list for objects created in an Amazon S3 bucket as .csv or .parquet files. For more information about Amazon S3 canned ACLs, see Canned ACL in the Amazon S3 Developer Guide.\n\nThe default value is NONE. Valid values include NONE, PRIVATE, PUBLIC_READ, PUBLIC_READ_WRITE, AUTHENTICATED_READ, AWS_EXEC_READ, BUCKET_OWNER_READ, and BUCKET_OWNER_FULL_CONTROL.\n\nAddColumnName -> (boolean)\n\nAn optional parameter that, when set to true or y , you can use to add column name information to the .csv output file.\n\nThe default value is false . Valid values are true , false , y , and n .\n\nCdcMaxBatchInterval -> (integer)\n\nMaximum length of the interval, defined in seconds, after which to output a file to Amazon S3.\n\nWhen CdcMaxBatchInterval and CdcMinFileSize are both specified, the file write is triggered by whichever parameter condition is met first within an DMS CloudFormation template.\n\nThe default value is 60 seconds.\n\nCdcMinFileSize -> (integer)\n\nMinimum file size, defined in megabytes, to reach for a file output to Amazon S3.\n\nWhen CdcMinFileSize and CdcMaxBatchInterval are both specified, the file write is triggered by whichever parameter condition is met first within an DMS CloudFormation template.\n\nThe default value is 32 MB.\n\nCsvNullValue -> (string)\n\nAn optional parameter that specifies how DMS treats null values. While handling the null value, you can use this parameter to pass a user-defined string as null when writing to the target. For example, when target columns are not nullable, you can use this option to differentiate between the empty string value and the null value. So, if you set this parameter value to the empty string (“” or ‘’), DMS treats the empty string as the null value instead of NULL .\n\nThe default value is NULL . Valid values include any valid string.\n\nIgnoreHeaderRows -> (integer)\n\nWhen this value is set to 1, DMS ignores the first row header in a .csv file. A value of 1 turns on the feature; a value of 0 turns off the feature.\n\nThe default is 0.\n\nMaxFileSize -> (integer)\n\nA value that specifies the maximum size (in KB) of any .csv file to be created while migrating to an S3 target during full load.\n\nThe default value is 1,048,576 KB (1 GB). Valid values include 1 to 1,048,576.\n\nRfc4180 -> (boolean)\n\nFor an S3 source, when this value is set to true or y , each leading double quotation mark has to be followed by an ending double quotation mark. This formatting complies with RFC 4180. When this value is set to false or n , string literals are copied to the target as is. In this case, a delimiter (row or column) signals the end of the field. Thus, you can’t use a delimiter as part of the string, because it signals the end of the value.\n\nFor an S3 target, an optional parameter used to set behavior to comply with RFC 4180 for data migrated to Amazon S3 using .csv file format only. When this value is set to true or y using Amazon S3 as a target, if the data has quotation marks or newline characters in it, DMS encloses the entire column with an additional pair of double quotation marks (“). Every quotation mark within the data is repeated twice.\n\nThe default value is true . Valid values include true , false , y , and n .\n\nDmsTransferSettings -> (structure)\n\nThe settings in JSON format for the DMS transfer type of source endpoint.\n\nPossible settings include the following:\n\nServiceAccessRoleArn - - The Amazon Resource Name (ARN) used by the service access IAM role. The role must allow the iam:PassRole action.\n\nBucketName - The name of the S3 bucket to use.\n\nShorthand syntax for these settings is as follows: ServiceAccessRoleArn=string,BucketName=string,\n\nJSON syntax for these settings is as follows: { \"ServiceAccessRoleArn\": \"string\", \"BucketName\": \"string\"}\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service access IAM role. The role must allow the iam:PassRole action.\n\nBucketName -> (string)\n\nThe name of the S3 bucket to use.\n\nMongoDbSettings -> (structure)\n\nThe settings for the MongoDB source endpoint. For more information, see the MongoDbSettings structure.\n\nUsername -> (string)\n\nThe user name you use to access the MongoDB source endpoint.\n\nPassword -> (string)\n\nThe password for the user account you use to access the MongoDB source endpoint.\n\nServerName -> (string)\n\nThe name of the server on the MongoDB source endpoint.\n\nPort -> (integer)\n\nThe port value for the MongoDB source endpoint.\n\nDatabaseName -> (string)\n\nThe database name on the MongoDB source endpoint.\n\nAuthType -> (string)\n\nThe authentication type you use to access the MongoDB source endpoint.\n\nWhen when set to \"no\" , user name and password parameters are not used and can be empty.\n\nAuthMechanism -> (string)\n\nThe authentication mechanism you use to access the MongoDB source endpoint.\n\nFor the default value, in MongoDB version 2.x, \"default\" is \"mongodb_cr\" . For MongoDB version 3.x or later, \"default\" is \"scram_sha_1\" . This setting isn’t used when AuthType is set to \"no\" .\n\nNestingLevel -> (string)\n\nSpecifies either document or table mode.\n\nDefault value is \"none\" . Specify \"none\" to use document mode. Specify \"one\" to use table mode.\n\nExtractDocId -> (string)\n\nSpecifies the document ID. Use this setting when NestingLevel is set to \"none\" .\n\nDefault value is \"false\" .\n\nDocsToInvestigate -> (string)\n\nIndicates the number of documents to preview to determine the document organization. Use this setting when NestingLevel is set to \"one\" .\n\nMust be a positive value greater than 0 . Default value is 1000 .\n\nAuthSource -> (string)\n\nThe MongoDB database name. This setting isn’t used when AuthType is set to \"no\" .\n\nThe default is \"admin\" .\n\nKmsKeyId -> (string)\n\nThe KMS key identifier that is used to encrypt the content on the replication instance. If you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key. KMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the MongoDB endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the MongoDB endpoint connection details.\n\nKinesisSettings -> (structure)\n\nThe settings for the Amazon Kinesis target endpoint. For more information, see the KinesisSettings structure.\n\nStreamArn -> (string)\n\nThe Amazon Resource Name (ARN) for the Amazon Kinesis Data Streams endpoint.\n\nMessageFormat -> (string)\n\nThe output format for the records created on the endpoint. The message format is JSON (default) or JSON_UNFORMATTED (a single line with no tab).\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) for the IAM role that DMS uses to write to the Kinesis data stream. The role must allow the iam:PassRole action.\n\nIncludeTransactionDetails -> (boolean)\n\nProvides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for transaction_id , previous transaction_id , and transaction_record_id (the record offset within a transaction). The default is false .\n\nIncludePartitionValue -> (boolean)\n\nShows the partition value within the Kinesis message output, unless the partition type is schema-table-type . The default is false .\n\nPartitionIncludeSchemaTable -> (boolean)\n\nPrefixes schema and table names to partition values, when the partition type is primary-key-type . Doing this increases data distribution among Kinesis shards. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same shard, which causes throttling. The default is false .\n\nIncludeTableAlterOperations -> (boolean)\n\nIncludes any data definition language (DDL) operations that change the table in the control data, such as rename-table , drop-table , add-column , drop-column , and rename-column . The default is false .\n\nIncludeControlDetails -> (boolean)\n\nShows detailed control information for table definition, column definition, and table and column changes in the Kinesis message output. The default is false .\n\nIncludeNullAndEmpty -> (boolean)\n\nInclude NULL and empty columns for records migrated to the endpoint. The default is false .\n\nNoHexPrefix -> (boolean)\n\nSet this optional parameter to true to avoid adding a ‘0x’ prefix to raw data in hexadecimal format. For example, by default, DMS adds a ‘0x’ prefix to the LOB column type in hexadecimal format moving from an Oracle source to an Amazon Kinesis target. Use the NoHexPrefix endpoint setting to enable migration of RAW data type columns without adding the ‘0x’ prefix.\n\nKafkaSettings -> (structure)\n\nThe settings for the Apache Kafka target endpoint. For more information, see the KafkaSettings structure.\n\nBroker -> (string)\n\nA comma-separated list of one or more broker locations in your Kafka cluster that host your Kafka instance. Specify each broker location in the form `` broker-hostname-or-ip :port `` . For example, \"ec2-12-345-678-901.compute-1.amazonaws.com:2345\" . For more information and examples of specifying a list of broker locations, see Using Apache Kafka as a target for Database Migration Service in the Database Migration Service User Guide .\n\nTopic -> (string)\n\nThe topic to which you migrate the data. If you don’t specify a topic, DMS specifies \"kafka-default-topic\" as the migration topic.\n\nMessageFormat -> (string)\n\nThe output format for the records created on the endpoint. The message format is JSON (default) or JSON_UNFORMATTED (a single line with no tab).\n\nIncludeTransactionDetails -> (boolean)\n\nProvides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for transaction_id , previous transaction_id , and transaction_record_id (the record offset within a transaction). The default is false .\n\nIncludePartitionValue -> (boolean)\n\nShows the partition value within the Kafka message output unless the partition type is schema-table-type . The default is false .\n\nPartitionIncludeSchemaTable -> (boolean)\n\nPrefixes schema and table names to partition values, when the partition type is primary-key-type . Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. The default is false .\n\nIncludeTableAlterOperations -> (boolean)\n\nIncludes any data definition language (DDL) operations that change the table in the control data, such as rename-table , drop-table , add-column , drop-column , and rename-column . The default is false .\n\nIncludeControlDetails -> (boolean)\n\nShows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. The default is false .\n\nMessageMaxBytes -> (integer)\n\nThe maximum size in bytes for records created on the endpoint The default is 1,000,000.\n\nIncludeNullAndEmpty -> (boolean)\n\nInclude NULL and empty columns for records migrated to the endpoint. The default is false .\n\nSecurityProtocol -> (string)\n\nSet secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include ssl-encryption , ssl-authentication , and sasl-ssl . sasl-ssl requires SaslUsername and SaslPassword .\n\nSslClientCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) of the client certificate used to securely connect to a Kafka target endpoint.\n\nSslClientKeyArn -> (string)\n\nThe Amazon Resource Name (ARN) for the client private key used to securely connect to a Kafka target endpoint.\n\nSslClientKeyPassword -> (string)\n\nThe password for the client private key used to securely connect to a Kafka target endpoint.\n\nSslCaCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the private certificate authority (CA) cert that DMS uses to securely connect to your Kafka target endpoint.\n\nSaslUsername -> (string)\n\nThe secure user name you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n\nSaslPassword -> (string)\n\nThe secure password you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n\nNoHexPrefix -> (boolean)\n\nSet this optional parameter to true to avoid adding a ‘0x’ prefix to raw data in hexadecimal format. For example, by default, DMS adds a ‘0x’ prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the NoHexPrefix endpoint setting to enable migration of RAW data type columns without adding the ‘0x’ prefix.\n\nElasticsearchSettings -> (structure)\n\nThe settings for the Elasticsearch source endpoint. For more information, see the ElasticsearchSettings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nEndpointUri -> (string)\n\nThe endpoint for the Elasticsearch cluster. DMS uses HTTPS if a transport protocol (http/https) is not specified.\n\nFullLoadErrorPercentage -> (integer)\n\nThe maximum percentage of records that can fail to be written before a full load operation stops.\n\nTo avoid early failure, this counter is only effective after 1000 records are transferred. Elasticsearch also has the concept of error monitoring during the last 10 minutes of an Observation Window. If transfer of all records fail in the last 10 minutes, the full load operation stops.\n\nErrorRetryDuration -> (integer)\n\nThe maximum number of seconds for which DMS retries failed API requests to the Elasticsearch cluster.\n\nNeptuneSettings -> (structure)\n\nThe settings for the Amazon Neptune target endpoint. For more information, see the NeptuneSettings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the service role that you created for the Neptune target endpoint. The role must allow the iam:PassRole action. For more information, see Creating an IAM Service Role for Accessing Amazon Neptune as a Target in the Database Migration Service User Guide.\n\nS3BucketName -> (string)\n\nThe name of the Amazon S3 bucket where DMS can temporarily store migrated graph data in .csv files before bulk-loading it to the Neptune target database. DMS maps the SQL source data to graph data before storing it in these .csv files.\n\nS3BucketFolder -> (string)\n\nA folder path where you want DMS to store migrated graph data in the S3 bucket specified by S3BucketName\n\nErrorRetryDuration -> (integer)\n\nThe number of milliseconds for DMS to wait to retry a bulk-load of migrated graph data to the Neptune target database before raising an error. The default is 250.\n\nMaxFileSize -> (integer)\n\nThe maximum size in kilobytes of migrated graph data stored in a .csv file before DMS bulk-loads the data to the Neptune target database. The default is 1,048,576 KB. If the bulk load is successful, DMS clears the bucket, ready to store the next batch of migrated graph data.\n\nMaxRetryCount -> (integer)\n\nThe number of times for DMS to retry a bulk load of migrated graph data to the Neptune target database before raising an error. The default is 5.\n\nIamAuthEnabled -> (boolean)\n\nIf you want Identity and Access Management (IAM) authorization enabled for this endpoint, set this parameter to true . Then attach the appropriate IAM policy document to your service role specified by ServiceAccessRoleArn . The default is false .\n\nRedshiftSettings -> (structure)\n\nSettings for the Amazon Redshift endpoint.\n\nAcceptAnyDate -> (boolean)\n\nA value that indicates to allow any date format, including invalid formats such as 00/00/00 00:00:00, to be loaded without generating an error. You can choose true or false (the default).\n\nThis parameter applies only to TIMESTAMP and DATE columns. Always use ACCEPTANYDATE with the DATEFORMAT parameter. If the date format for the data doesn’t match the DATEFORMAT specification, Amazon Redshift inserts a NULL value into that field.\n\nAfterConnectScript -> (string)\n\nCode to run after connecting. This parameter should contain the code itself, not the name of a file containing the code.\n\nBucketFolder -> (string)\n\nAn S3 folder where the comma-separated-value (.csv) files are stored before being uploaded to the target Redshift cluster.\n\nFor full load mode, DMS converts source records into .csv files and loads them to the BucketFolder/TableID path. DMS uses the Redshift COPY command to upload the .csv files to the target table. The files are deleted once the COPY operation has finished. For more information, see COPY in the Amazon Redshift Database Developer Guide .\n\nFor change-data-capture (CDC) mode, DMS creates a NetChanges table, and loads the .csv files to this BucketFolder/NetChangesTableID path.\n\nBucketName -> (string)\n\nThe name of the intermediate S3 bucket used to store .csv files before uploading data to Redshift.\n\nCaseSensitiveNames -> (boolean)\n\nIf Amazon Redshift is configured to support case sensitive schema names, set CaseSensitiveNames to true . The default is false .\n\nCompUpdate -> (boolean)\n\nIf you set CompUpdate to true Amazon Redshift applies automatic compression if the table is empty. This applies even if the table columns already have encodings other than RAW . If you set CompUpdate to false , automatic compression is disabled and existing column encodings aren’t changed. The default is true .\n\nConnectionTimeout -> (integer)\n\nA value that sets the amount of time to wait (in milliseconds) before timing out, beginning from when you initially establish a connection.\n\nDatabaseName -> (string)\n\nThe name of the Amazon Redshift data warehouse (service) that you are working with.\n\nDateFormat -> (string)\n\nThe date format that you are using. Valid values are auto (case-sensitive), your date format string enclosed in quotes, or NULL. If this parameter is left unset (NULL), it defaults to a format of ‘YYYY-MM-DD’. Using auto recognizes most strings, even some that aren’t supported when you use a date format string.\n\nIf your date and time values use formats different from each other, set this to auto .\n\nEmptyAsNull -> (boolean)\n\nA value that specifies whether DMS should migrate empty CHAR and VARCHAR fields as NULL. A value of true sets empty CHAR and VARCHAR fields to null. The default is false .\n\nEncryptionMode -> (string)\n\nThe type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either SSE_S3 (the default) or SSE_KMS .\n\nNote\n\nFor the ModifyEndpoint operation, you can change the existing value of the EncryptionMode parameter from SSE_KMS to SSE_S3 . But you can’t change the existing value from SSE_S3 to SSE_KMS .\n\nTo use SSE_S3 , create an Identity and Access Management (IAM) role with a policy that allows \"arn:aws:s3:::*\" to use the following actions: \"s3:PutObject\", \"s3:ListBucket\"\n\nExplicitIds -> (boolean)\n\nThis setting is only valid for a full-load migration task. Set ExplicitIds to true to have tables with IDENTITY columns override their auto-generated values with explicit values loaded from the source data files used to populate the tables. The default is false .\n\nFileTransferUploadStreams -> (integer)\n\nThe number of threads used to upload a single file. This parameter accepts a value from 1 through 64. It defaults to 10.\n\nThe number of parallel streams used to upload a single .csv file to an S3 bucket using S3 Multipart Upload. For more information, see Multipart upload overview .\n\nFileTransferUploadStreams accepts a value from 1 through 64. It defaults to 10.\n\nLoadTimeout -> (integer)\n\nThe amount of time to wait (in milliseconds) before timing out of operations performed by DMS on a Redshift cluster, such as Redshift COPY, INSERT, DELETE, and UPDATE.\n\nMaxFileSize -> (integer)\n\nThe maximum size (in KB) of any .csv file used to load data on an S3 bucket and transfer data to Amazon Redshift. It defaults to 1048576KB (1 GB).\n\nPassword -> (string)\n\nThe password for the user named in the username property.\n\nPort -> (integer)\n\nThe port number for Amazon Redshift. The default value is 5439.\n\nRemoveQuotes -> (boolean)\n\nA value that specifies to remove surrounding quotation marks from strings in the incoming data. All characters within the quotation marks, including delimiters, are retained. Choose true to remove quotation marks. The default is false .\n\nReplaceInvalidChars -> (string)\n\nA list of characters that you want to replace. Use with ReplaceChars .\n\nReplaceChars -> (string)\n\nA value that specifies to replaces the invalid characters specified in ReplaceInvalidChars , substituting the specified characters instead. The default is \"?\" .\n\nServerName -> (string)\n\nThe name of the Amazon Redshift cluster you are using.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the IAM role that has access to the Amazon Redshift service. The role must allow the iam:PassRole action.\n\nServerSideEncryptionKmsKeyId -> (string)\n\nThe KMS key ID. If you are using SSE_KMS for the EncryptionMode , provide this key ID. The key that you use needs an attached policy that enables IAM user permissions and allows use of the key.\n\nTimeFormat -> (string)\n\nThe time format that you want to use. Valid values are auto (case-sensitive), 'timeformat_string' , 'epochsecs' , or 'epochmillisecs' . It defaults to 10. Using auto recognizes most strings, even some that aren’t supported when you use a time format string.\n\nIf your date and time values use formats different from each other, set this parameter to auto .\n\nTrimBlanks -> (boolean)\n\nA value that specifies to remove the trailing white space characters from a VARCHAR string. This parameter applies only to columns with a VARCHAR data type. Choose true to remove unneeded white space. The default is false .\n\nTruncateColumns -> (boolean)\n\nA value that specifies to truncate data in columns to the appropriate number of characters, so that the data fits in the column. This parameter applies only to columns with a VARCHAR or CHAR data type, and rows with a size of 4 MB or less. Choose true to truncate data. The default is false .\n\nUsername -> (string)\n\nAn Amazon Redshift user name for a registered user.\n\nWriteBufferSize -> (integer)\n\nThe size (in KB) of the in-memory file write buffer used when generating .csv files on the local disk at the DMS replication instance. The default value is 1000 (buffer size is 1000KB).\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Amazon Redshift endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Amazon Redshift endpoint connection details.\n\nPostgreSQLSettings -> (structure)\n\nThe settings for the PostgreSQL source and target endpoint. For more information, see the PostgreSQLSettings structure.\n\nAfterConnectScript -> (string)\n\nFor use with change data capture (CDC) only, this attribute has DMS bypass foreign keys and user triggers to reduce the time it takes to bulk load data.\n\nExample: afterConnectScript=SET session_replication_role='replica'\n\nCaptureDdls -> (boolean)\n\nTo capture DDL events, DMS creates various artifacts in the PostgreSQL database when the task starts. You can later remove these artifacts.\n\nIf this value is set to N , you don’t have to create tables or triggers on the source database.\n\nMaxFileSize -> (integer)\n\nSpecifies the maximum size (in KB) of any .csv file used to transfer data to PostgreSQL.\n\nExample: maxFileSize=512\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nDdlArtifactsSchema -> (string)\n\nThe schema in which the operational DDL database artifacts are created.\n\nExample: ddlArtifactsSchema=xyzddlschema;\n\nExecuteTimeout -> (integer)\n\nSets the client statement timeout for the PostgreSQL instance, in seconds. The default value is 60 seconds.\n\nExample: executeTimeout=100;\n\nFailTasksOnLobTruncation -> (boolean)\n\nWhen set to true , this value causes a task to fail if the actual size of a LOB column is greater than the specified LobMaxSize .\n\nIf task is set to Limited LOB mode and this option is set to true, the task fails instead of truncating the LOB data.\n\nHeartbeatEnable -> (boolean)\n\nThe write-ahead log (WAL) heartbeat feature mimics a dummy transaction. By doing this, it prevents idle logical replication slots from holding onto old WAL logs, which can result in storage full situations on the source. This heartbeat keeps restart_lsn moving and prevents storage full scenarios.\n\nHeartbeatSchema -> (string)\n\nSets the schema in which the heartbeat artifacts are created.\n\nHeartbeatFrequency -> (integer)\n\nSets the WAL heartbeat frequency (in minutes).\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSlotName -> (string)\n\nSets the name of a previously created logical replication slot for a change data capture (CDC) load of the PostgreSQL source instance.\n\nWhen used with the CdcStartPosition request parameter for the DMS API , this attribute also makes it possible to use native CDC start points. DMS verifies that the specified logical replication slot exists before starting the CDC load task. It also verifies that the task was created with a valid setting of CdcStartPosition . If the specified slot doesn’t exist or the task doesn’t have a valid CdcStartPosition setting, DMS raises an error.\n\nFor more information about setting the CdcStartPosition request parameter, see Determining a CDC native start point in the Database Migration Service User Guide . For more information about using CdcStartPosition , see CreateReplicationTask , StartReplicationTask , and ModifyReplicationTask .\n\nPluginName -> (string)\n\nSpecifies the plugin to use to create a replication slot.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the PostgreSQL endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the PostgreSQL endpoint connection details.\n\nMySQLSettings -> (structure)\n\nThe settings for the MySQL source and target endpoint. For more information, see the MySQLSettings structure.\n\nAfterConnectScript -> (string)\n\nSpecifies a script to run immediately after DMS connects to the endpoint. The migration task continues running regardless if the SQL statement succeeds or fails.\n\nFor this parameter, provide the code of the script itself, not the name of a file containing the script.\n\nCleanSourceMetadataOnMismatch -> (boolean)\n\nAdjusts the behavior of DMS when migrating from an SQL Server source database that is hosted as part of an Always On availability group cluster. If you need DMS to poll all the nodes in the Always On cluster for transaction backups, set this attribute to false .\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint. For a MySQL source or target endpoint, don’t explicitly specify the database using the DatabaseName request parameter on either the CreateEndpoint or ModifyEndpoint API call. Specifying DatabaseName when you create or modify a MySQL endpoint replicates all the task tables to this single database. For MySQL endpoints, you specify the database only when you specify the schema in the table-mapping rules of the DMS task.\n\nEventsPollInterval -> (integer)\n\nSpecifies how often to check the binary log for new changes/events when the database is idle.\n\nExample: eventsPollInterval=5;\n\nIn the example, DMS checks for changes in the binary logs every five seconds.\n\nTargetDbType -> (string)\n\nSpecifies where to migrate source tables on the target, either to a single database or multiple databases.\n\nExample: targetDbType=MULTIPLE_DATABASES\n\nMaxFileSize -> (integer)\n\nSpecifies the maximum size (in KB) of any .csv file used to transfer data to a MySQL-compatible database.\n\nExample: maxFileSize=512\n\nParallelLoadThreads -> (integer)\n\nImproves performance when loading data into the MySQL-compatible target database. Specifies how many threads to use to load the data into the MySQL-compatible target database. Setting a large number of threads can have an adverse effect on database performance, because a separate connection is required for each thread.\n\nExample: parallelLoadThreads=1\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nServerTimezone -> (string)\n\nSpecifies the time zone for the source MySQL database.\n\nExample: serverTimezone=US/Pacific;\n\nNote: Do not enclose time zones in single quotes.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the MySQL endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the MySQL endpoint connection details.\n\nOracleSettings -> (structure)\n\nThe settings for the Oracle source and target endpoint. For more information, see the OracleSettings structure.\n\nAddSupplementalLogging -> (boolean)\n\nSet this attribute to set up table-level supplemental logging for the Oracle database. This attribute enables PRIMARY KEY supplemental logging on all tables selected for a migration task.\n\nIf you use this option, you still need to enable database-level supplemental logging.\n\nArchivedLogDestId -> (integer)\n\nSpecifies the ID of the destination for the archived redo logs. This value should be the same as a number in the dest_id column of the v$archived_log view. If you work with an additional redo log destination, use the AdditionalArchivedLogDestId option to specify the additional destination ID. Doing this improves performance by ensuring that the correct logs are accessed from the outset.\n\nAdditionalArchivedLogDestId -> (integer)\n\nSet this attribute with ArchivedLogDestId in a primary/ standby setup. This attribute is useful in the case of a switchover. In this case, DMS needs to know which destination to get archive redo logs from to read changes. This need arises because the previous primary instance is now a standby instance after switchover.\n\nAlthough DMS supports the use of the Oracle RESETLOGS option to open the database, never use RESETLOGS unless necessary. For additional information about RESETLOGS , see RMAN Data Repair Concepts in the Oracle Database Backup and Recovery User’s Guide .\n\nExtraArchivedLogDestIds -> (list)\n\nSpecifies the IDs of one more destinations for one or more archived redo logs. These IDs are the values of the dest_id column in the v$archived_log view. Use this setting with the archivedLogDestId extra connection attribute in a primary-to-single setup or a primary-to-multiple-standby setup.\n\nThis setting is useful in a switchover when you use an Oracle Data Guard database as a source. In this case, DMS needs information about what destination to get archive redo logs from to read changes. DMS needs this because after the switchover the previous primary is a standby instance. For example, in a primary-to-single standby setup you might apply the following settings.\n\narchivedLogDestId=1; ExtraArchivedLogDestIds=[2]\n\nIn a primary-to-multiple-standby setup, you might apply the following settings.\n\narchivedLogDestId=1; ExtraArchivedLogDestIds=[2,3,4]\n\nAlthough DMS supports the use of the Oracle RESETLOGS option to open the database, never use RESETLOGS unless it’s necessary. For more information about RESETLOGS , see RMAN Data Repair Concepts in the Oracle Database Backup and Recovery User’s Guide .\n\n(integer)\n\nAllowSelectNestedTables -> (boolean)\n\nSet this attribute to true to enable replication of Oracle tables containing columns that are nested tables or defined types.\n\nParallelAsmReadThreads -> (integer)\n\nSet this attribute to change the number of threads that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 2 (the default) and 8 (the maximum). Use this attribute together with the readAheadBlocks attribute.\n\nReadAheadBlocks -> (integer)\n\nSet this attribute to change the number of read-ahead blocks that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 1000 (the default) and 200,000 (the maximum).\n\nAccessAlternateDirectly -> (boolean)\n\nSet this attribute to false in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to not access redo logs through any specified path prefix replacement using direct file access.\n\nUseAlternateFolderForOnline -> (boolean)\n\nSet this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to use any specified prefix replacement to access all online redo logs.\n\nOraclePathPrefix -> (string)\n\nSet this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the default Oracle root used to access the redo logs.\n\nUsePathPrefix -> (string)\n\nSet this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the path prefix used to replace the default Oracle root to access the redo logs.\n\nReplacePathPrefix -> (boolean)\n\nSet this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This setting tells DMS instance to replace the default Oracle root with the specified usePathPrefix setting to access the redo logs.\n\nEnableHomogenousTablespace -> (boolean)\n\nSet this attribute to enable homogenous tablespace replication and create existing tables or indexes under the same tablespace on the target.\n\nDirectPathNoLog -> (boolean)\n\nWhen set to true , this attribute helps to increase the commit rate on the Oracle target database by writing directly to tables and not writing a trail to database logs.\n\nArchivedLogsOnly -> (boolean)\n\nWhen this field is set to Y , DMS only accesses the archived redo logs. If the archived redo logs are stored on Oracle ASM only, the DMS user account needs to be granted ASM privileges.\n\nAsmPassword -> (string)\n\nFor an Oracle source endpoint, your Oracle Automatic Storage Management (ASM) password. You can set this value from the `` asm_user_password `` value. You set this value as part of the comma-separated value that you set to the Password request parameter when you create the endpoint to access transaction logs using Binary Reader. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nAsmServer -> (string)\n\nFor an Oracle source endpoint, your ASM server address. You can set this value from the asm_server value. You set asm_server as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nAsmUser -> (string)\n\nFor an Oracle source endpoint, your ASM user name. You can set this value from the asm_user value. You set asm_user as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nCharLengthSemantics -> (string)\n\nSpecifies whether the length of a character column is in bytes or in characters. To indicate that the character column length is in characters, set this attribute to CHAR . Otherwise, the character column length is in bytes.\n\nExample: charLengthSemantics=CHAR;\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nDirectPathParallelLoad -> (boolean)\n\nWhen set to true , this attribute specifies a parallel load when useDirectPathFullLoad is set to Y . This attribute also only applies when you use the DMS parallel load feature. Note that the target table cannot have any constraints or indexes.\n\nFailTasksOnLobTruncation -> (boolean)\n\nWhen set to true , this attribute causes a task to fail if the actual size of an LOB column is greater than the specified LobMaxSize .\n\nIf a task is set to limited LOB mode and this option is set to true , the task fails instead of truncating the LOB data.\n\nNumberDatatypeScale -> (integer)\n\nSpecifies the number scale. You can select a scale up to 38, or you can select FLOAT. By default, the NUMBER data type is converted to precision 38, scale 10.\n\nExample: numberDataTypeScale=12\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nReadTableSpaceName -> (boolean)\n\nWhen set to true , this attribute supports tablespace replication.\n\nRetryInterval -> (integer)\n\nSpecifies the number of seconds that the system waits before resending a query.\n\nExample: retryInterval=6;\n\nSecurityDbEncryption -> (string)\n\nFor an Oracle source endpoint, the transparent data encryption (TDE) password required by AWM DMS to access Oracle redo logs encrypted by TDE using Binary Reader. It is also the `` TDE_Password `` part of the comma-separated value you set to the Password request parameter when you create the endpoint. The SecurityDbEncryptian setting is related to this SecurityDbEncryptionName setting. For more information, see Supported encryption methods for using Oracle as a source for DMS in the Database Migration Service User Guide .\n\nSecurityDbEncryptionName -> (string)\n\nFor an Oracle source endpoint, the name of a key used for the transparent data encryption (TDE) of the columns and tablespaces in an Oracle source database that is encrypted using TDE. The key value is the value of the SecurityDbEncryption setting. For more information on setting the key name value of SecurityDbEncryptionName , see the information and example for setting the securityDbEncryptionName extra connection attribute in Supported encryption methods for using Oracle as a source for DMS in the Database Migration Service User Guide .\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nSpatialDataOptionToGeoJsonFunctionName -> (string)\n\nUse this attribute to convert SDO_GEOMETRY to GEOJSON format. By default, DMS calls the SDO2GEOJSON custom function if present and accessible. Or you can create your own custom function that mimics the operation of SDOGEOJSON and set SpatialDataOptionToGeoJsonFunctionName to call it instead.\n\nStandbyDelayTime -> (integer)\n\nUse this attribute to specify a time in minutes for the delay in standby sync. If the source is an Oracle Active Data Guard standby database, use this attribute to specify the time lag between primary and standby databases.\n\nIn DMS, you can create an Oracle CDC task that uses an Active Data Guard standby instance as a source for replicating ongoing changes. Doing this eliminates the need to connect to an active database that might be in production.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nUseBFile -> (boolean)\n\nSet this attribute to Y to capture change data using the Binary Reader utility. Set UseLogminerReader to N to set this attribute to Y. To use Binary Reader with Amazon RDS for Oracle as the source, you set additional attributes. For more information about using this setting with Oracle Automatic Storage Management (ASM), see Using Oracle LogMiner or DMS Binary Reader for CDC .\n\nUseDirectPathFullLoad -> (boolean)\n\nSet this attribute to Y to have DMS use a direct path full load. Specify this value to use the direct path protocol in the Oracle Call Interface (OCI). By using this OCI protocol, you can bulk-load Oracle target tables during a full load.\n\nUseLogminerReader -> (boolean)\n\nSet this attribute to Y to capture change data using the Oracle LogMiner utility (the default). Set this attribute to N if you want to access the redo logs as a binary file. When you set UseLogminerReader to N, also set UseBfile to Y. For more information on this setting and using Oracle ASM, see Using Oracle LogMiner or DMS Binary Reader for CDC in the DMS User Guide .\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Oracle endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Oracle endpoint connection details.\n\nSecretsManagerOracleAsmAccessRoleArn -> (string)\n\nRequired only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the SecretsManagerOracleAsmSecret . This SecretsManagerOracleAsmSecret has the secret value that allows access to the Oracle ASM of the endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerOracleAsmSecretId . Or you can specify clear-text values for AsmUserName , AsmPassword , and AsmServerName . You can’t specify both. For more information on creating this SecretsManagerOracleAsmSecret and the SecretsManagerOracleAsmAccessRoleArn and SecretsManagerOracleAsmSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerOracleAsmSecretId -> (string)\n\nRequired only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN, partial ARN, or friendly name of the SecretsManagerOracleAsmSecret that contains the Oracle ASM connection details for the Oracle endpoint.\n\nSybaseSettings -> (structure)\n\nThe settings for the SAP ASE source and target endpoint. For more information, see the SybaseSettings structure.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the SAP ASE endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the SAP SAE endpoint connection details.\n\nMicrosoftSQLServerSettings -> (structure)\n\nThe settings for the Microsoft SQL Server source and target endpoint. For more information, see the MicrosoftSQLServerSettings structure.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nBcpPacketSize -> (integer)\n\nThe maximum size of the packets (in bytes) used to transfer data using BCP.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nControlTablesFileGroup -> (string)\n\nSpecifies a file group for the DMS internal tables. When the replication task starts, all the internal DMS control tables (awsdms_ apply_exception, awsdms_apply, awsdms_changes) are created for the specified file group.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nQuerySingleAlwaysOnNode -> (boolean)\n\nCleans and recreates table metadata information on the replication instance when a mismatch occurs. An example is a situation where running an alter DDL statement on a table might result in different information about the table cached in the replication instance.\n\nReadBackupOnly -> (boolean)\n\nWhen this attribute is set to Y , DMS only reads changes from transaction log backups and doesn’t read from the active transaction log file during ongoing replication. Setting this parameter to Y enables you to control active transaction log file growth during full load and ongoing replication tasks. However, it can add some source latency to ongoing replication.\n\nSafeguardPolicy -> (string)\n\nUse this attribute to minimize the need to access the backup log and enable DMS to prevent truncation using one of the following two methods.\n\nStart transactions in the database: This is the default method. When this method is used, DMS prevents TLOG truncation by mimicking a transaction in the database. As long as such a transaction is open, changes that appear after the transaction started aren’t truncated. If you need Microsoft Replication to be enabled in your database, then you must choose this method.\n\nExclusively use sp_repldone within a single task : When this method is used, DMS reads the changes and then uses sp_repldone to mark the TLOG transactions as ready for truncation. Although this method doesn’t involve any transactional activities, it can only be used when Microsoft Replication isn’t running. Also, when using this method, only one DMS task can access the database at any given time. Therefore, if you need to run parallel DMS tasks against the same database, use the default method.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nUseBcpFullLoad -> (boolean)\n\nUse this to attribute to transfer data for full-load operations using BCP. When the target table contains an identity column that does not exist in the source table, you must disable the use BCP for loading table option.\n\nUseThirdPartyBackupDevice -> (boolean)\n\nWhen this attribute is set to Y , DMS processes third-party transaction log backups if they are created in native format.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the SQL Server endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the SQL Server endpoint connection details.\n\nIBMDb2Settings -> (structure)\n\nThe settings for the IBM Db2 LUW source endpoint. For more information, see the IBMDb2Settings structure.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port. The default value is 50000.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nSetDataCaptureChanges -> (boolean)\n\nEnables ongoing replication (CDC) as a BOOLEAN value. The default is true.\n\nCurrentLsn -> (string)\n\nFor ongoing replication (CDC), use CurrentLSN to specify a log sequence number (LSN) where you want the replication to start.\n\nMaxKBytesPerRead -> (integer)\n\nMaximum number of bytes per read, as a NUMBER value. The default is 64 KB.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Db2 LUW endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Db2 LUW endpoint connection details.\n\nDocDbSettings -> (structure)\n\nProvides information that defines a DocumentDB endpoint.\n\nUsername -> (string)\n\nThe user name you use to access the DocumentDB source endpoint.\n\nPassword -> (string)\n\nThe password for the user account you use to access the DocumentDB source endpoint.\n\nServerName -> (string)\n\nThe name of the server on the DocumentDB source endpoint.\n\nPort -> (integer)\n\nThe port value for the DocumentDB source endpoint.\n\nDatabaseName -> (string)\n\nThe database name on the DocumentDB source endpoint.\n\nNestingLevel -> (string)\n\nSpecifies either document or table mode.\n\nDefault value is \"none\" . Specify \"none\" to use document mode. Specify \"one\" to use table mode.\n\nExtractDocId -> (boolean)\n\nSpecifies the document ID. Use this setting when NestingLevel is set to \"none\" .\n\nDefault value is \"false\" .\n\nDocsToInvestigate -> (integer)\n\nIndicates the number of documents to preview to determine the document organization. Use this setting when NestingLevel is set to \"one\" .\n\nMust be a positive value greater than 0 . Default value is 1000 .\n\nKmsKeyId -> (string)\n\nThe KMS key identifier that is used to encrypt the content on the replication instance. If you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key. KMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the DocumentDB endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the DocumentDB endpoint connection details.\n\nRedisSettings -> (structure)\n\nThe settings for the Redis target endpoint. For more information, see the RedisSettings structure.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nPort -> (integer)\n\nTransmission Control Protocol (TCP) port for the endpoint.\n\nSslSecurityProtocol -> (string)\n\nThe connection to a Redis target endpoint using Transport Layer Security (TLS). Valid values include plaintext and ssl-encryption . The default is ssl-encryption . The ssl-encryption option makes an encrypted connection. Optionally, you can identify an Amazon Resource Name (ARN) for an SSL certificate authority (CA) using the SslCaCertificateArn setting. If an ARN isn’t given for a CA, DMS uses the Amazon root CA.\n\nThe plaintext option doesn’t provide Transport Layer Security (TLS) encryption for traffic between endpoint and database.\n\nAuthType -> (string)\n\nThe type of authentication to perform when connecting to a Redis target. Options include none , auth-token , and auth-role . The auth-token option requires an AuthPassword value to be provided. The auth-role option requires AuthUserName and AuthPassword values to be provided.\n\nAuthUserName -> (string)\n\nThe user name provided with the auth-role option of the AuthType setting for a Redis target endpoint.\n\nAuthPassword -> (string)\n\nThe password provided with the auth-role and auth-token options of the AuthType setting for a Redis target endpoint.\n\nSslCaCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the certificate authority (CA) that DMS uses to connect to your Redis target endpoint.",
      "command_examples": "Examples\n\nTo delete an endpoint\n\nThe following delete-endpoint example deletes an endpoint.\n\naws dms delete-endpoint \\\n    --endpoint-arn arn:aws:dms:us-east-1:123456789012:endpoint:OUJJVXO4XZ4CYTSEG5XGMN2R3Y\n\n\nOutput:\n\n{\n    \"Endpoint\": {\n        \"EndpointIdentifier\": \"src-endpoint\",\n        \"EndpointType\": \"SOURCE\",\n        \"EngineName\": \"s3\",\n        \"EngineDisplayName\": \"Amazon S3\",\n        \"ExtraConnectionAttributes\": \"bucketFolder=sourcedata;bucketName=my-corp-data;compressionType=NONE;csvDelimiter=,;csvRowDelimiter=\\\\n;\",\n        \"Status\": \"deleting\",\n        \"EndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:OUJJVXO4XZ4CYTSEG5XGMN2R3Y\",\n        \"SslMode\": \"none\",\n        \"ServiceAccessRoleArn\": \"arn:aws:iam::123456789012:role/my-s3-access-role\",\n        \"S3Settings\": {\n            \"ServiceAccessRoleArn\": \"arn:aws:iam::123456789012:role/my-s3-access-role\",\n            \"CsvRowDelimiter\": \"\\\\n\",\n            \"CsvDelimiter\": \",\",\n            \"BucketFolder\": \"sourcedata\",\n            \"BucketName\": \"my-corp-data\",\n            \"CompressionType\": \"NONE\",\n            \"EnableStatistics\": true\n        }\n    }\n}\n\n\nFor more information, see Working with AWS DMS Endpoints in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "delete-event-subscription",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/delete-event-subscription.html",
      "command_description": "Description\n\nDeletes an DMS event subscription.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-event-subscription\n--subscription-name <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--subscription-name <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--subscription-name (string)\n\nThe name of the DMS event notification subscription to be deleted.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nEventSubscription -> (structure)\n\nThe event subscription that was deleted.\n\nCustomerAwsId -> (string)\n\nThe Amazon Web Services customer account associated with the DMS event notification subscription.\n\nCustSubscriptionId -> (string)\n\nThe DMS event notification subscription Id.\n\nSnsTopicArn -> (string)\n\nThe topic ARN of the DMS event notification subscription.\n\nStatus -> (string)\n\nThe status of the DMS event notification subscription.\n\nConstraints:\n\nCan be one of the following: creating | modifying | deleting | active | no-permission | topic-not-exist\n\nThe status “no-permission” indicates that DMS no longer has permission to post to the SNS topic. The status “topic-not-exist” indicates that the topic was deleted after the subscription was created.\n\nSubscriptionCreationTime -> (string)\n\nThe time the DMS event notification subscription was created.\n\nSourceType -> (string)\n\nThe type of DMS resource that generates events.\n\nValid values: replication-instance | replication-server | security-group | replication-task\n\nSourceIdsList -> (list)\n\nA list of source Ids for the event subscription.\n\n(string)\n\nEventCategoriesList -> (list)\n\nA lists of event categories.\n\n(string)\n\nEnabled -> (boolean)\n\nBoolean value that indicates if the event subscription is enabled.",
      "command_examples": "Examples\n\nTo delete an event subscription\n\nThe following delete-event-subscription example deletes a subscription to an Amaon SNS topic.\n\naws dms delete-event-subscription \\\n    --subscription-name \"my-dms-events\"\n\n\nOutput:\n\n{\n    \"EventSubscription\": {\n        \"CustomerAwsId\": \"123456789012\",\n        \"CustSubscriptionId\": \"my-dms-events\",\n        \"SnsTopicArn\": \"arn:aws:sns:us-east-1:123456789012:my-sns-topic\",\n        \"Status\": \"deleting\",\n        \"SubscriptionCreationTime\": \"2020-05-21 21:58:38.598\",\n        \"Enabled\": true\n    }\n}\n\n\nFor more information, see Working with Events and Notifications in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "delete-replication-instance",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/delete-replication-instance.html",
      "command_description": "Description\n\nDeletes the specified replication instance.\n\nNote\n\nYou must delete any migration tasks that are associated with the replication instance before you can delete it.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-replication-instance\n--replication-instance-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-instance-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-instance-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication instance to be deleted.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationInstance -> (structure)\n\nThe replication instance that was deleted.\n\nReplicationInstanceIdentifier -> (string)\n\nThe replication instance identifier is a required parameter. This parameter is stored as a lowercase string.\n\nConstraints:\n\nMust contain 1-63 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nExample: myrepinstance\n\nReplicationInstanceClass -> (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class. It is a required parameter, although a default value is pre-selected in the DMS console.\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\nReplicationInstanceStatus -> (string)\n\nThe status of the replication instance. The possible return values include:\n\n\"available\"\n\n\"creating\"\n\n\"deleted\"\n\n\"deleting\"\n\n\"failed\"\n\n\"modifying\"\n\n\"upgrading\"\n\n\"rebooting\"\n\n\"resetting-master-credentials\"\n\n\"storage-full\"\n\n\"incompatible-credentials\"\n\n\"incompatible-network\"\n\n\"maintenance\"\n\nAllocatedStorage -> (integer)\n\nThe amount of storage (in gigabytes) that is allocated for the replication instance.\n\nInstanceCreateTime -> (timestamp)\n\nThe time the replication instance was created.\n\nVpcSecurityGroups -> (list)\n\nThe VPC security group for the instance.\n\n(structure)\n\nDescribes the status of a security group associated with the virtual private cloud (VPC) hosting your replication and DB instances.\n\nVpcSecurityGroupId -> (string)\n\nThe VPC security group ID.\n\nStatus -> (string)\n\nThe status of the VPC security group.\n\nAvailabilityZone -> (string)\n\nThe Availability Zone for the instance.\n\nReplicationSubnetGroup -> (structure)\n\nThe subnet group for the replication instance.\n\nReplicationSubnetGroupIdentifier -> (string)\n\nThe identifier of the replication instance subnet group.\n\nReplicationSubnetGroupDescription -> (string)\n\nA description for the replication subnet group.\n\nVpcId -> (string)\n\nThe ID of the VPC.\n\nSubnetGroupStatus -> (string)\n\nThe status of the subnet group.\n\nSubnets -> (list)\n\nThe subnets that are in the subnet group.\n\n(structure)\n\nIn response to a request by the DescribeReplicationSubnetGroups operation, this object identifies a subnet by its given Availability Zone, subnet identifier, and status.\n\nSubnetIdentifier -> (string)\n\nThe subnet identifier.\n\nSubnetAvailabilityZone -> (structure)\n\nThe Availability Zone of the subnet.\n\nName -> (string)\n\nThe name of the Availability Zone.\n\nSubnetStatus -> (string)\n\nThe status of the subnet.\n\nPreferredMaintenanceWindow -> (string)\n\nThe maintenance window times for the replication instance. Any pending upgrades to the replication instance are performed during this time.\n\nPendingModifiedValues -> (structure)\n\nThe pending modification values.\n\nReplicationInstanceClass -> (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class.\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\nAllocatedStorage -> (integer)\n\nThe amount of storage (in gigabytes) that is allocated for the replication instance.\n\nMultiAZ -> (boolean)\n\nSpecifies whether the replication instance is a Multi-AZ deployment. You can’t set the AvailabilityZone parameter if the Multi-AZ parameter is set to true .\n\nEngineVersion -> (string)\n\nThe engine version number of the replication instance.\n\nMultiAZ -> (boolean)\n\nSpecifies whether the replication instance is a Multi-AZ deployment. You can’t set the AvailabilityZone parameter if the Multi-AZ parameter is set to true .\n\nEngineVersion -> (string)\n\nThe engine version number of the replication instance.\n\nIf an engine version number is not specified when a replication instance is created, the default is the latest engine version available.\n\nWhen modifying a major engine version of an instance, also set AllowMajorVersionUpgrade to true .\n\nAutoMinorVersionUpgrade -> (boolean)\n\nBoolean value indicating if minor version upgrades will be automatically applied to the instance.\n\nKmsKeyId -> (string)\n\nAn KMS key identifier that is used to encrypt the data on the replication instance.\n\nIf you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key.\n\nKMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nReplicationInstanceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\nReplicationInstancePublicIpAddress -> (string)\n\nThe public IP address of the replication instance.\n\nReplicationInstancePrivateIpAddress -> (string)\n\nThe private IP address of the replication instance.\n\nReplicationInstancePublicIpAddresses -> (list)\n\nOne or more public IP addresses for the replication instance.\n\n(string)\n\nReplicationInstancePrivateIpAddresses -> (list)\n\nOne or more private IP addresses for the replication instance.\n\n(string)\n\nPubliclyAccessible -> (boolean)\n\nSpecifies the accessibility options for the replication instance. A value of true represents an instance with a public IP address. A value of false represents an instance with a private IP address. The default value is true .\n\nSecondaryAvailabilityZone -> (string)\n\nThe Availability Zone of the standby replication instance in a Multi-AZ deployment.\n\nFreeUntil -> (timestamp)\n\nThe expiration date of the free replication instance that is part of the Free DMS program.\n\nDnsNameServers -> (string)\n\nThe DNS name servers supported for the replication instance to access your on-premise source or target database.",
      "command_examples": "Examples\n\nTo delete a replication instance\n\nThe following delete-replication-instance example deletes a replication instance.\n\naws dms delete-replication-instance \\\n    --replication-instance-arn arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\n\n\nOutput:\n\n{\n    \"ReplicationInstance\": {\n        \"ReplicationInstanceIdentifier\": \"my-repl-instance\",\n        \"ReplicationInstanceClass\": \"dms.t2.micro\",\n        \"ReplicationInstanceStatus\": \"deleting\",\n        \"AllocatedStorage\": 5,\n        \"InstanceCreateTime\": 1590011235.952,\n        \"VpcSecurityGroups\": [\n            {\n                \"VpcSecurityGroupId\": \"sg-f839b688\",\n                \"Status\": \"active\"\n            }\n        ],\n        \"AvailabilityZone\": \"us-east-1e\",\n        \"ReplicationSubnetGroup\": {\n            \"ReplicationSubnetGroupIdentifier\": \"default\",\n            \"ReplicationSubnetGroupDescription\": \"default\",\n            \"VpcId\": \"vpc-136a4c6a\",\n            \"SubnetGroupStatus\": \"Complete\",\n            \"Subnets\": [\n                {\n                    \"SubnetIdentifier\": \"subnet-da327bf6\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1a\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                },\n                {\n                    \"SubnetIdentifier\": \"subnet-42599426\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1d\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                },\n                {\n                    \"SubnetIdentifier\": \"subnet-bac383e0\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1c\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                },\n                {\n                    \"SubnetIdentifier\": \"subnet-6746046b\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1f\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                },\n                {\n                    \"SubnetIdentifier\": \"subnet-d7c825e8\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1e\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                },\n                {\n                    \"SubnetIdentifier\": \"subnet-cbfff283\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1b\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                }\n            ]\n        },\n        \"PreferredMaintenanceWindow\": \"wed:11:42-wed:12:12\",\n        \"PendingModifiedValues\": {},\n        \"MultiAZ\": true,\n        \"EngineVersion\": \"3.3.2\",\n        \"AutoMinorVersionUpgrade\": true,\n        \"KmsKeyId\": \"arn:aws:kms:us-east-1:123456789012:key/f7bc0f8e-1a3a-4ace-9faa-e8494fa3921a\",\n        \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n        \"ReplicationInstancePublicIpAddress\": \"54.225.120.92\",\n        \"ReplicationInstancePrivateIpAddress\": \"172.31.30.121\",\n        \"ReplicationInstancePublicIpAddresses\": [\n            \"54.225.120.92\",\n            \"3.230.18.248\"\n        ],\n        \"ReplicationInstancePrivateIpAddresses\": [\n            \"172.31.30.121\",\n            \"172.31.75.90\"\n        ],\n        \"PubliclyAccessible\": true,\n        \"SecondaryAvailabilityZone\": \"us-east-1b\"\n    }\n}\n\n\nFor more information, see Working with an AWS DMS Replication Instance in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "delete-replication-subnet-group",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/delete-replication-subnet-group.html",
      "command_description": "Description\n\nDeletes a subnet group.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-replication-subnet-group\n--replication-subnet-group-identifier <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-subnet-group-identifier <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-subnet-group-identifier (string)\n\nThe subnet group name of the replication instance.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nNone",
      "command_examples": "Examples\n\nTo delete a subnet group\n\nThe following delete-replication-subnet-group example deletes a subnet group.\n\naws dms delete-replication-subnet-group \\\n--replication-subnet-group-identifier my-subnet-group\n\n\nOutput:\n\n(none)\n\n\nFor more information, see Setting Up a Network for a Replication Instance in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "delete-replication-task",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/delete-replication-task.html",
      "command_description": "Description\n\nDeletes the specified replication task.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-replication-task\n--replication-task-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-task-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication task to be deleted.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationTask -> (structure)\n\nThe deleted replication task.\n\nReplicationTaskIdentifier -> (string)\n\nThe user-assigned replication task identifier or name.\n\nConstraints:\n\nMust contain 1-255 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nSourceEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) that uniquely identifies the endpoint.\n\nTargetEndpointArn -> (string)\n\nThe ARN that uniquely identifies the endpoint.\n\nReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance.\n\nMigrationType -> (string)\n\nThe type of migration.\n\nTableMappings -> (string)\n\nTable mappings specified in the task.\n\nReplicationTaskSettings -> (string)\n\nThe settings for the replication task.\n\nStatus -> (string)\n\nThe status of the replication task. This response parameter can return one of the following values:\n\n\"moving\" – The task is being moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"creating\" – The task is being created in response to running the ` CreateReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_CreateReplicationTask.html`__ operation.\n\n\"deleting\" – The task is being deleted in response to running the ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ operation.\n\n\"failed\" – The task failed to successfully complete the database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"failed-move\" – The task failed to move in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"modifying\" – The task definition is being modified in response to running the ` ModifyReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_ModifyReplicationTask.html`__ operation.\n\n\"ready\" – The task is in a ready state where it can respond to other task operations, such as ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ or ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ .\n\n\"running\" – The task is performing a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"starting\" – The task is preparing to perform a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"stopped\" – The task has stopped in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"stopping\" – The task is preparing to stop in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"testing\" – The database migration specified for this task is being tested in response to running either the ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ or the ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation.\n\nNote\n\n` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ is an improved premigration task assessment operation. The ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation assesses data type compatibility only between the source and target database of a given migration task. In contrast, ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ enables you to specify a variety of premigration task assessments in addition to data type compatibility. These assessments include ones for the validity of primary key definitions and likely issues with database migration performance, among others.\n\nLastFailureMessage -> (string)\n\nThe last error (failure) message generated for the replication task.\n\nStopReason -> (string)\n\nThe reason the replication task was stopped. This response parameter can return one of the following values:\n\n\"STOP_REASON_FULL_LOAD_COMPLETED\" – Full-load migration completed.\n\n\"STOP_REASON_CACHED_CHANGES_APPLIED\" – Change data capture (CDC) load completed.\n\n\"STOP_REASON_CACHED_CHANGES_NOT_APPLIED\" – In a full-load and CDC migration, the full load stopped as specified before starting the CDC migration.\n\n\"STOP_REASON_SERVER_TIME\" – The migration stopped at the specified server time.\n\nReplicationTaskCreationDate -> (timestamp)\n\nThe date the replication task was created.\n\nReplicationTaskStartDate -> (timestamp)\n\nThe date the replication task is scheduled to start.\n\nCdcStartPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to start. Use either CdcStartPosition or CdcStartTime to specify when you want the CDC operation to start. Specifying both values results in an error.\n\nThe value can be in date, checkpoint, or LSN/SCN format.\n\nDate Example: –cdc-start-position “2018-03-08T12:12:12”\n\nCheckpoint Example: –cdc-start-position “checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93”\n\nLSN Example: –cdc-start-position “mysql-bin-changelog.000024:373”\n\nCdcStopPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time.\n\nServer time example: –cdc-stop-position “server_time:2018-02-09T12:12:12”\n\nCommit time example: –cdc-stop-position “commit_time: 2018-02-09T12:12:12 “\n\nRecoveryCheckpoint -> (string)\n\nIndicates the last checkpoint that occurred during a change data capture (CDC) operation. You can provide this value to the CdcStartPosition parameter to start a CDC operation that begins at that checkpoint.\n\nReplicationTaskArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\nReplicationTaskStats -> (structure)\n\nThe statistics for the task, including elapsed time, tables loaded, and table errors.\n\nFullLoadProgressPercent -> (integer)\n\nThe percent complete for the full load migration task.\n\nElapsedTimeMillis -> (long)\n\nThe elapsed time of the task, in milliseconds.\n\nTablesLoaded -> (integer)\n\nThe number of tables loaded for this task.\n\nTablesLoading -> (integer)\n\nThe number of tables currently loading for this task.\n\nTablesQueued -> (integer)\n\nThe number of tables queued for this task.\n\nTablesErrored -> (integer)\n\nThe number of errors that have occurred during this task.\n\nFreshStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a target reload.\n\nStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a resume. For more information, see StartReplicationTaskType .\n\nStopDate -> (timestamp)\n\nThe date the replication task was stopped.\n\nFullLoadStartDate -> (timestamp)\n\nThe date the replication task full load was started.\n\nFullLoadFinishDate -> (timestamp)\n\nThe date the replication task full load was completed.\n\nTaskData -> (string)\n\nSupplemental information that the task requires to migrate the data for certain source and target endpoints. For more information, see Specifying Supplemental Data for Task Settings in the Database Migration Service User Guide.\n\nTargetReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance to which this task is moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation. Otherwise, this response parameter isn’t a member of the ReplicationTask object.",
      "command_examples": "Examples\n\nTo delete a replication task\n\nThe following delete-replication-task example deletes a replication task.\n\naws dms delete-replication-task \\\n    --replication-task-arn arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\n\n\nOutput:\n\n{\n    \"ReplicationTask\": {\n        \"ReplicationTaskIdentifier\": \"moveit2\",\n        \"SourceEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\",\n        \"TargetEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:EOM4SFKCZEYHZBFGAGZT3QEC5U\",\n        \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n        \"MigrationType\": \"full-load\",\n        \"TableMappings\": ...output omitted...,\n        \"ReplicationTaskSettings\": ...output omitted...,\n        \"Status\": \"deleting\",\n        \"StopReason\": \"Stop Reason FULL_LOAD_ONLY_FINISHED\",\n        \"ReplicationTaskCreationDate\": 1590524772.505,\n        \"ReplicationTaskStartDate\": 1590789988.677,\n        \"ReplicationTaskArn\": \"arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\"\n    }\n}\n\n\nFor more information, see Working with AWS DMS Tasks in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "delete-replication-task-assessment-run",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/delete-replication-task-assessment-run.html",
      "command_description": "Description\n\nDeletes the record of a single premigration assessment run.\n\nThis operation removes all metadata that DMS maintains about this assessment run. However, the operation leaves untouched all information about this assessment run that is stored in your Amazon S3 bucket.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  delete-replication-task-assessment-run\n--replication-task-assessment-run-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-task-assessment-run-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-assessment-run-arn (string)\n\nAmazon Resource Name (ARN) of the premigration assessment run to be deleted.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationTaskAssessmentRun -> (structure)\n\nThe ReplicationTaskAssessmentRun object for the deleted assessment run.\n\nReplicationTaskAssessmentRunArn -> (string)\n\nAmazon Resource Name (ARN) of this assessment run.\n\nReplicationTaskArn -> (string)\n\nARN of the migration task associated with this premigration assessment run.\n\nStatus -> (string)\n\nAssessment run status.\n\nThis status can have one of the following values:\n\n\"cancelling\" – The assessment run was canceled by the CancelReplicationTaskAssessmentRun operation.\n\n\"deleting\" – The assessment run was deleted by the DeleteReplicationTaskAssessmentRun operation.\n\n\"failed\" – At least one individual assessment completed with a failed status.\n\n\"error-provisioning\" – An internal error occurred while resources were provisioned (during provisioning status).\n\n\"error-executing\" – An internal error occurred while individual assessments ran (during running status).\n\n\"invalid state\" – The assessment run is in an unknown state.\n\n\"passed\" – All individual assessments have completed, and none has a failed status.\n\n\"provisioning\" – Resources required to run individual assessments are being provisioned.\n\n\"running\" – Individual assessments are being run.\n\n\"starting\" – The assessment run is starting, but resources are not yet being provisioned for individual assessments.\n\nReplicationTaskAssessmentRunCreationDate -> (timestamp)\n\nDate on which the assessment run was created using the StartReplicationTaskAssessmentRun operation.\n\nAssessmentProgress -> (structure)\n\nIndication of the completion progress for the individual assessments specified to run.\n\nIndividualAssessmentCount -> (integer)\n\nThe number of individual assessments that are specified to run.\n\nIndividualAssessmentCompletedCount -> (integer)\n\nThe number of individual assessments that have completed, successfully or not.\n\nLastFailureMessage -> (string)\n\nLast message generated by an individual assessment failure.\n\nServiceAccessRoleArn -> (string)\n\nARN of the service role used to start the assessment run using the StartReplicationTaskAssessmentRun operation. The role must allow the iam:PassRole action.\n\nResultLocationBucket -> (string)\n\nAmazon S3 bucket where DMS stores the results of this assessment run.\n\nResultLocationFolder -> (string)\n\nFolder in an Amazon S3 bucket where DMS stores the results of this assessment run.\n\nResultEncryptionMode -> (string)\n\nEncryption mode used to encrypt the assessment run results.\n\nResultKmsKeyArn -> (string)\n\nARN of the KMS encryption key used to encrypt the assessment run results.\n\nAssessmentRunName -> (string)\n\nUnique name of the assessment run."
    },
    {
      "command_name": "describe-account-attributes",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-account-attributes.html",
      "command_description": "Description\n\nLists all of the DMS attributes for a customer account. These attributes include DMS quotas for the account and a unique account identifier in a particular DMS region. DMS quotas include a list of resource quotas supported by the account, such as the number of replication instances allowed. The description for each resource quota, includes the quota name, current usage toward that quota, and the quota’s maximum value. DMS uses the unique account identifier to name each artifact used by DMS in the given region.\n\nThis command does not take any parameters.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-account-attributes\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nAccountQuotas -> (list)\n\nAccount quota information.\n\n(structure)\n\nDescribes a quota for an Amazon Web Services account, for example the number of replication instances allowed.\n\nAccountQuotaName -> (string)\n\nThe name of the DMS quota for this Amazon Web Services account.\n\nUsed -> (long)\n\nThe amount currently used toward the quota maximum.\n\nMax -> (long)\n\nThe maximum allowed value for the quota.\n\nUniqueAccountIdentifier -> (string)\n\nA unique DMS identifier for an account in a particular Amazon Web Services Region. The value of this identifier has the following format: c99999999999 . DMS uses this identifier to name artifacts. For example, DMS uses this identifier to name the default Amazon S3 bucket for storing task assessment reports in a given Amazon Web Services Region. The format of this S3 bucket name is the following: dms-*AccountNumber* -*UniqueAccountIdentifier* . Here is an example name for this default S3 bucket: dms-111122223333-c44445555666 .\n\nNote\n\nDMS supports the UniqueAccountIdentifier parameter in versions 3.1.4 and later.",
      "command_examples": "Examples\n\nTo describe account attributes\n\nThe following describe-account-attributes example lists the attributes for your AWS account.\n\naws dms describe-account-attributes\n\n\nOutput:\n\n{\n    \"AccountQuotas\": [\n        {\n            \"AccountQuotaName\": \"ReplicationInstances\",\n            \"Used\": 1,\n            \"Max\": 20\n        },\n        {\n            \"AccountQuotaName\": \"AllocatedStorage\",\n            \"Used\": 5,\n            \"Max\": 10000\n        },\n\n        ...remaining output omitted...\n\n    ],\n    \"UniqueAccountIdentifier\": \"cqahfbfy5xee\"\n}\n"
    },
    {
      "command_name": "describe-applicable-individual-assessments",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-applicable-individual-assessments.html",
      "command_description": "Description\n\nProvides a list of individual assessments that you can specify for a new premigration assessment run, given one or more parameters.\n\nIf you specify an existing migration task, this operation provides the default individual assessments you can specify for that task. Otherwise, the specified parameters model elements of a possible migration task on which to base a premigration assessment run.\n\nTo use these migration task modeling parameters, you must specify an existing replication instance, a source database engine, a target database engine, and a migration type. This combination of parameters potentially limits the default individual assessments available for an assessment run created for a corresponding migration task.\n\nIf you specify no parameters, this operation provides a list of all possible individual assessments that you can specify for an assessment run. If you specify any one of the task modeling parameters, you must specify all of them or the operation cannot provide a list of individual assessments. The only parameter that you can specify alone is for an existing migration task. The specified task definition then determines the default list of individual assessments that you can specify in an assessment run for the task.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-applicable-individual-assessments\n[--replication-task-arn <value>]\n[--replication-instance-arn <value>]\n[--source-engine-name <value>]\n[--target-engine-name <value>]\n[--migration-type <value>]\n[--max-records <value>]\n[--marker <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--replication-task-arn <value>]",
        "[--replication-instance-arn <value>]",
        "[--source-engine-name <value>]",
        "[--target-engine-name <value>]",
        "[--migration-type <value>]",
        "[--max-records <value>]",
        "[--marker <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-arn (string)\n\nAmazon Resource Name (ARN) of a migration task on which you want to base the default list of individual assessments.\n\n--replication-instance-arn (string)\n\nARN of a replication instance on which you want to base the default list of individual assessments.\n\n--source-engine-name (string)\n\nName of a database engine that the specified replication instance supports as a source.\n\n--target-engine-name (string)\n\nName of a database engine that the specified replication instance supports as a target.\n\n--migration-type (string)\n\nName of the migration type that each provided individual assessment must support.\n\nPossible values:\n\nfull-load\n\ncdc\n\nfull-load-and-cdc\n\n--max-records (integer)\n\nMaximum number of records to include in the response. If more records exist than the specified MaxRecords value, a pagination token called a marker is included in the response so that the remaining results can be retrieved.\n\n--marker (string)\n\nOptional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nIndividualAssessmentNames -> (list)\n\nList of names for the individual assessments supported by the premigration assessment run that you start based on the specified request parameters. For more information on the available individual assessments, including compatibility with different migration task configurations, see Working with premigration assessment runs in the Database Migration Service User Guide.\n\n(string)\n\nMarker -> (string)\n\nPagination token returned for you to pass to a subsequent request. If you pass this token as the Marker value in a subsequent request, the response includes only records beyond the marker, up to the value specified in the request by MaxRecords ."
    },
    {
      "command_name": "describe-certificates",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-certificates.html",
      "command_description": "Description\n\nProvides a description of the certificate.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-certificates is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: Certificates",
      "command_synopsis": "Synopsis\n  describe-certificates\n[--filters <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--filters <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--filters (list)\n\nFilters applied to the certificates described in the form of key-value pairs.\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nThe pagination token.\n\nCertificates -> (list)\n\nThe Secure Sockets Layer (SSL) certificates associated with the replication instance.\n\n(structure)\n\nThe SSL certificate that can be used to encrypt connections between the endpoints and the replication instance.\n\nCertificateIdentifier -> (string)\n\nA customer-assigned name for the certificate. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen or contain two consecutive hyphens.\n\nCertificateCreationDate -> (timestamp)\n\nThe date that the certificate was created.\n\nCertificatePem -> (string)\n\nThe contents of a .pem file, which contains an X.509 certificate.\n\nCertificateWallet -> (blob)\n\nThe location of an imported Oracle Wallet certificate for use with SSL.\n\nCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the certificate.\n\nCertificateOwner -> (string)\n\nThe owner of the certificate.\n\nValidFromDate -> (timestamp)\n\nThe beginning date that the certificate is valid.\n\nValidToDate -> (timestamp)\n\nThe final date that the certificate is valid.\n\nSigningAlgorithm -> (string)\n\nThe signing algorithm for the certificate.\n\nKeyLength -> (integer)\n\nThe key length of the cryptographic algorithm being used.",
      "command_examples": "Examples\n\nTo list the available certificates\n\nThe following describe-certificates example lists the available certificates in your AWS account.\n\naws dms describe-certificates\n\n\nOutput:\n\n{\n    \"Certificates\": [\n        {\n            \"CertificateIdentifier\": \"my-cert\",\n            \"CertificateCreationDate\": 1543259542.506,\n            \"CertificatePem\": \"-----BEGIN CERTIFICATE-----\\nMIID9DCCAtygAwIBAgIBQjANBgkqhkiG9w0BAQ ...U\"\n\n            ... remaining output omittted ...\n\n        }\n    ]\n}\n\n\nFor more information, see Using SSL in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-connections",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-connections.html",
      "command_description": "Description\n\nDescribes the status of the connections that have been made between the replication instance and an endpoint. Connections are created when you test an endpoint.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-connections is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: Connections",
      "command_synopsis": "Synopsis\n  describe-connections\n[--filters <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--filters <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--filters (list)\n\nThe filters applied to the connection.\n\nValid filter names: endpoint-arn | replication-instance-arn\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\nConnections -> (list)\n\nA description of the connections.\n\n(structure)\n\nStatus of the connection between an endpoint and a replication instance, including Amazon Resource Names (ARNs) and the last error message issued.\n\nReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance.\n\nEndpointArn -> (string)\n\nThe ARN string that uniquely identifies the endpoint.\n\nStatus -> (string)\n\nThe connection status. This parameter can return one of the following values:\n\n\"successful\"\n\n\"testing\"\n\n\"failed\"\n\n\"deleting\"\n\nLastFailureMessage -> (string)\n\nThe error message when the connection last failed.\n\nEndpointIdentifier -> (string)\n\nThe identifier of the endpoint. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen or contain two consecutive hyphens.\n\nReplicationInstanceIdentifier -> (string)\n\nThe replication instance identifier. This parameter is stored as a lowercase string.",
      "command_examples": "Examples\n\nTo describe connections\n\nThe following describe-connections example lists the connections that you have tested between a replication instance and an endpoint.\n\naws dms describe-connections\n\n\nOutput:\n\n{\n    \"Connections\": [\n        {\n            \"Status\": \"successful\",\n            \"ReplicationInstanceIdentifier\": \"test\",\n            \"EndpointArn\": \"arn:aws:dms:us-east-arn:aws:dms:us-east-1:123456789012:endpoint:ZW5UAN6P4E77EC7YWHK4RZZ3BE\",\n            \"EndpointIdentifier\": \"testsrc1\",\n            \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:6UTDJGBOUS3VI3SUWA66XFJCJQ\"\n        }\n    ]\n}\n\n\nFor more information, see Creating Source and Target Endpoints in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-endpoint-settings",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-endpoint-settings.html",
      "command_description": "Description\n\nReturns information about the possible endpoint settings available when you create an endpoint for a specific database engine.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-endpoint-settings\n--engine-name <value>\n[--max-records <value>]\n[--marker <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--engine-name <value>",
        "[--max-records <value>]",
        "[--marker <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--engine-name (string)\n\nThe databse engine used for your source or target endpoint.\n\n--max-records (integer)\n\nThe maximum number of records to include in the response. If more records exist than the specified MaxRecords value, a pagination token called a marker is included in the response so that the remaining results can be retrieved.\n\n--marker (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\nEndpointSettings -> (list)\n\nDescriptions of the endpoint settings available for your source or target database engine.\n\n(structure)\n\nEndpoint settings.\n\nName -> (string)\n\nThe name that you want to give the endpoint settings.\n\nType -> (string)\n\nThe type of endpoint. Valid values are source and target .\n\nEnumValues -> (list)\n\nEnumerated values to use for this endpoint.\n\n(string)\n\nSensitive -> (boolean)\n\nA value that marks this endpoint setting as sensitive.\n\nUnits -> (string)\n\nThe unit of measure for this endpoint setting.\n\nApplicability -> (string)\n\nThe relevance or validity of an endpoint setting for an engine name and its endpoint type.\n\nIntValueMin -> (integer)\n\nThe minimum value of an endpoint setting that is of type int .\n\nIntValueMax -> (integer)\n\nThe maximum value of an endpoint setting that is of type int .\n\nDefaultValue -> (string)\n\nThe default value of the endpoint setting if no value is specified using CreateEndpoint or ModifyEndpoint ."
    },
    {
      "command_name": "describe-endpoint-types",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-endpoint-types.html",
      "command_description": "Description\n\nReturns information about the type of endpoints available.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-endpoint-types is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: SupportedEndpointTypes",
      "command_synopsis": "Synopsis\n  describe-endpoint-types\n[--filters <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--filters <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--filters (list)\n\nFilters applied to the endpoint types.\n\nValid filter names: engine-name | endpoint-type\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\nSupportedEndpointTypes -> (list)\n\nThe types of endpoints that are supported.\n\n(structure)\n\nProvides information about types of supported endpoints in response to a request by the DescribeEndpointTypes operation. This information includes the type of endpoint, the database engine name, and whether change data capture (CDC) is supported.\n\nEngineName -> (string)\n\nThe database engine name. Valid values, depending on the EndpointType, include \"mysql\" , \"oracle\" , \"postgres\" , \"mariadb\" , \"aurora\" , \"aurora-postgresql\" , \"redshift\" , \"s3\" , \"db2\" , \"azuredb\" , \"sybase\" , \"dynamodb\" , \"mongodb\" , \"kinesis\" , \"kafka\" , \"elasticsearch\" , \"documentdb\" , \"sqlserver\" , and \"neptune\" .\n\nSupportsCDC -> (boolean)\n\nIndicates if change data capture (CDC) is supported.\n\nEndpointType -> (string)\n\nThe type of endpoint. Valid values are source and target .\n\nReplicationInstanceEngineMinimumVersion -> (string)\n\nThe earliest DMS engine version that supports this endpoint engine. Note that endpoint engines released with DMS versions earlier than 3.1.1 do not return a value for this parameter.\n\nEngineDisplayName -> (string)\n\nThe expanded name for the engine name. For example, if the EngineName parameter is “aurora,” this value would be “Amazon Aurora MySQL.”",
      "command_examples": "Examples\n\nTo list the available endpoint types\n\nThe following describe-endpoint-types example lists the MySQL endpoint types that are available.\n\naws dms describe-endpoint-types \\\n    --filters \"Name=engine-name,Values=mysql\"\n\n\nOutput:\n\n{\n    \"SupportedEndpointTypes\": [\n        {\n            \"EngineName\": \"mysql\",\n            \"SupportsCDC\": true,\n            \"EndpointType\": \"source\",\n            \"EngineDisplayName\": \"MySQL\"\n        },\n        {\n            \"EngineName\": \"mysql\",\n            \"SupportsCDC\": true,\n            \"EndpointType\": \"target\",\n            \"EngineDisplayName\": \"MySQL\"\n        }\n    ]\n}\n\n\nFor more information, see Working with AWS DMS Endpoints <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Endpoints.html>`__ in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-endpoints",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-endpoints.html",
      "command_description": "Description\n\nReturns information about the endpoints for your account in the current region.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-endpoints is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: Endpoints",
      "command_synopsis": "Synopsis\n  describe-endpoints\n[--filters <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--filters <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--filters (list)\n\nFilters applied to the endpoints.\n\nValid filter names: endpoint-arn | endpoint-type | endpoint-id | engine-name\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\nEndpoints -> (list)\n\nEndpoint description.\n\n(structure)\n\nDescribes an endpoint of a database instance in response to operations such as the following:\n\nCreateEndpoint\n\nDescribeEndpoint\n\nModifyEndpoint\n\nEndpointIdentifier -> (string)\n\nThe database endpoint identifier. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen or contain two consecutive hyphens.\n\nEndpointType -> (string)\n\nThe type of endpoint. Valid values are source and target .\n\nEngineName -> (string)\n\nThe database engine name. Valid values, depending on the EndpointType, include \"mysql\" , \"oracle\" , \"postgres\" , \"mariadb\" , \"aurora\" , \"aurora-postgresql\" , \"redshift\" , \"s3\" , \"db2\" , \"azuredb\" , \"sybase\" , \"dynamodb\" , \"mongodb\" , \"kinesis\" , \"kafka\" , \"elasticsearch\" , \"documentdb\" , \"sqlserver\" , and \"neptune\" .\n\nEngineDisplayName -> (string)\n\nThe expanded name for the engine name. For example, if the EngineName parameter is “aurora,” this value would be “Amazon Aurora MySQL.”\n\nUsername -> (string)\n\nThe user name used to connect to the endpoint.\n\nServerName -> (string)\n\nThe name of the server at the endpoint.\n\nPort -> (integer)\n\nThe port value used to access the endpoint.\n\nDatabaseName -> (string)\n\nThe name of the database at the endpoint.\n\nExtraConnectionAttributes -> (string)\n\nAdditional connection attributes used to connect to the endpoint.\n\nStatus -> (string)\n\nThe status of the endpoint.\n\nKmsKeyId -> (string)\n\nAn KMS key identifier that is used to encrypt the connection parameters for the endpoint.\n\nIf you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key.\n\nKMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\nCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) used for SSL connection to the endpoint.\n\nSslMode -> (string)\n\nThe SSL mode used to connect to the endpoint. The default value is none .\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nExternalTableDefinition -> (string)\n\nThe external table definition.\n\nExternalId -> (string)\n\nValue returned by a call to CreateEndpoint that can be used for cross-account validation. Use it on a subsequent call to CreateEndpoint to create the endpoint with a cross-account.\n\nDynamoDbSettings -> (structure)\n\nThe settings for the DynamoDB target endpoint. For more information, see the DynamoDBSettings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nS3Settings -> (structure)\n\nThe settings for the S3 target endpoint. For more information, see the S3Settings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action. It is a required parameter that enables DMS to write and read objects from an S3 bucket.\n\nExternalTableDefinition -> (string)\n\nSpecifies how tables are defined in the S3 source files only.\n\nCsvRowDelimiter -> (string)\n\nThe delimiter used to separate rows in the .csv file for both source and target. The default is a carriage return (\\n ).\n\nCsvDelimiter -> (string)\n\nThe delimiter used to separate columns in the .csv file for both source and target. The default is a comma.\n\nBucketFolder -> (string)\n\nAn optional parameter to set a folder name in the S3 bucket. If provided, tables are created in the path `` bucketFolder /schema_name /table_name /`` . If this parameter isn’t specified, then the path used is `` schema_name /table_name /`` .\n\nBucketName -> (string)\n\nThe name of the S3 bucket.\n\nCompressionType -> (string)\n\nAn optional parameter to use GZIP to compress the target files. Set to GZIP to compress the target files. Either set this parameter to NONE (the default) or don’t use it to leave the files uncompressed. This parameter applies to both .csv and .parquet file formats.\n\nEncryptionMode -> (string)\n\nThe type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either SSE_S3 (the default) or SSE_KMS .\n\nNote\n\nFor the ModifyEndpoint operation, you can change the existing value of the EncryptionMode parameter from SSE_KMS to SSE_S3 . But you can’t change the existing value from SSE_S3 to SSE_KMS .\n\nTo use SSE_S3 , you need an Identity and Access Management (IAM) role with permission to allow \"arn:aws:s3:::dms-*\" to use the following actions:\n\ns3:CreateBucket\n\ns3:ListBucket\n\ns3:DeleteBucket\n\ns3:GetBucketLocation\n\ns3:GetObject\n\ns3:PutObject\n\ns3:DeleteObject\n\ns3:GetObjectVersion\n\ns3:GetBucketPolicy\n\ns3:PutBucketPolicy\n\ns3:DeleteBucketPolicy\n\nServerSideEncryptionKmsKeyId -> (string)\n\nIf you are using SSE_KMS for the EncryptionMode , provide the KMS key ID. The key that you use needs an attached policy that enables Identity and Access Management (IAM) user permissions and allows use of the key.\n\nHere is a CLI example: ``aws dms create-endpoint –endpoint-identifier value –endpoint-type target –engine-name s3 –s3-settings ServiceAccessRoleArn=*value* ,BucketFolder=*value* ,BucketName=*value* ,EncryptionMode=SSE_KMS,ServerSideEncryptionKmsKeyId=*value* ``\n\nDataFormat -> (string)\n\nThe format of the data that you want to use for output. You can choose one of the following:\n\ncsv : This is a row-based file format with comma-separated values (.csv).\n\nparquet : Apache Parquet (.parquet) is a columnar storage file format that features efficient compression and provides faster query response.\n\nEncodingType -> (string)\n\nThe type of encoding you are using:\n\nRLE_DICTIONARY uses a combination of bit-packing and run-length encoding to store repeated values more efficiently. This is the default.\n\nPLAIN doesn’t use encoding at all. Values are stored as they are.\n\nPLAIN_DICTIONARY builds a dictionary of the values encountered in a given column. The dictionary is stored in a dictionary page for each column chunk.\n\nDictPageSizeLimit -> (integer)\n\nThe maximum size of an encoded dictionary page of a column. If the dictionary page exceeds this, this column is stored using an encoding type of PLAIN . This parameter defaults to 1024 * 1024 bytes (1 MiB), the maximum size of a dictionary page before it reverts to PLAIN encoding. This size is used for .parquet file format only.\n\nRowGroupLength -> (integer)\n\nThe number of rows in a row group. A smaller row group size provides faster reads. But as the number of row groups grows, the slower writes become. This parameter defaults to 10,000 rows. This number is used for .parquet file format only.\n\nIf you choose a value larger than the maximum, RowGroupLength is set to the max row group length in bytes (64 * 1024 * 1024).\n\nDataPageSize -> (integer)\n\nThe size of one data page in bytes. This parameter defaults to 1024 * 1024 bytes (1 MiB). This number is used for .parquet file format only.\n\nParquetVersion -> (string)\n\nThe version of the Apache Parquet format that you want to use: parquet_1_0 (the default) or parquet_2_0 .\n\nEnableStatistics -> (boolean)\n\nA value that enables statistics for Parquet pages and row groups. Choose true to enable statistics, false to disable. Statistics include NULL , DISTINCT , MAX , and MIN values. This parameter defaults to true . This value is used for .parquet file format only.\n\nIncludeOpForFullLoad -> (boolean)\n\nA value that enables a full load to write INSERT operations to the comma-separated value (.csv) output files only to indicate how the rows were added to the source database.\n\nNote\n\nDMS supports the IncludeOpForFullLoad parameter in versions 3.1.4 and later.\n\nFor full load, records can only be inserted. By default (the false setting), no information is recorded in these output files for a full load to indicate that the rows were inserted at the source database. If IncludeOpForFullLoad is set to true or y , the INSERT is recorded as an I annotation in the first field of the .csv file. This allows the format of your target records from a full load to be consistent with the target records from a CDC load.\n\nNote\n\nThis setting works together with the CdcInsertsOnly and the CdcInsertsAndUpdates parameters for output to .csv files only. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nCdcInsertsOnly -> (boolean)\n\nA value that enables a change data capture (CDC) load to write only INSERT operations to .csv or columnar storage (.parquet) output files. By default (the false setting), the first field in a .csv or .parquet record contains the letter I (INSERT), U (UPDATE), or D (DELETE). These values indicate whether the row was inserted, updated, or deleted at the source database for a CDC load to the target.\n\nIf CdcInsertsOnly is set to true or y , only INSERTs from the source database are migrated to the .csv or .parquet file. For .csv format only, how these INSERTs are recorded depends on the value of IncludeOpForFullLoad . If IncludeOpForFullLoad is set to true , the first field of every CDC record is set to I to indicate the INSERT operation at the source. If IncludeOpForFullLoad is set to false , every CDC record is written without a first field to indicate the INSERT operation at the source. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nNote\n\nDMS supports the interaction described preceding between the CdcInsertsOnly and IncludeOpForFullLoad parameters in versions 3.1.4 and later.\n\nCdcInsertsOnly and CdcInsertsAndUpdates can’t both be set to true for the same endpoint. Set either CdcInsertsOnly or CdcInsertsAndUpdates to true for the same endpoint, but not both.\n\nTimestampColumnName -> (string)\n\nA value that when nonblank causes DMS to add a column with timestamp information to the endpoint data for an Amazon S3 target.\n\nNote\n\nDMS supports the TimestampColumnName parameter in versions 3.1.4 and later.\n\nDMS includes an additional STRING column in the .csv or .parquet object files of your migrated data when you set TimestampColumnName to a nonblank value.\n\nFor a full load, each row of this timestamp column contains a timestamp for when the data was transferred from the source to the target by DMS.\n\nFor a change data capture (CDC) load, each row of the timestamp column contains the timestamp for the commit of that row in the source database.\n\nThe string format for this timestamp column value is yyyy-MM-dd HH:mm:ss.SSSSSS . By default, the precision of this value is in microseconds. For a CDC load, the rounding of the precision depends on the commit timestamp supported by DMS for the source database.\n\nWhen the AddColumnName parameter is set to true , DMS also includes a name for the timestamp column that you set with TimestampColumnName .\n\nParquetTimestampInMillisecond -> (boolean)\n\nA value that specifies the precision of any TIMESTAMP column values that are written to an Amazon S3 object file in .parquet format.\n\nNote\n\nDMS supports the ParquetTimestampInMillisecond parameter in versions 3.1.4 and later.\n\nWhen ParquetTimestampInMillisecond is set to true or y , DMS writes all TIMESTAMP columns in a .parquet formatted file with millisecond precision. Otherwise, DMS writes them with microsecond precision.\n\nCurrently, Amazon Athena and Glue can handle only millisecond precision for TIMESTAMP values. Set this parameter to true for S3 endpoint object files that are .parquet formatted only if you plan to query or process the data with Athena or Glue.\n\nNote\n\nDMS writes any TIMESTAMP column values written to an S3 file in .csv format with microsecond precision.\n\nSetting ParquetTimestampInMillisecond has no effect on the string format of the timestamp column value that is inserted by setting the TimestampColumnName parameter.\n\nCdcInsertsAndUpdates -> (boolean)\n\nA value that enables a change data capture (CDC) load to write INSERT and UPDATE operations to .csv or .parquet (columnar storage) output files. The default setting is false , but when CdcInsertsAndUpdates is set to true or y , only INSERTs and UPDATEs from the source database are migrated to the .csv or .parquet file.\n\nFor .csv file format only, how these INSERTs and UPDATEs are recorded depends on the value of the IncludeOpForFullLoad parameter. If IncludeOpForFullLoad is set to true , the first field of every CDC record is set to either I or U to indicate INSERT and UPDATE operations at the source. But if IncludeOpForFullLoad is set to false , CDC records are written without an indication of INSERT or UPDATE operations at the source. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nNote\n\nDMS supports the use of the CdcInsertsAndUpdates parameter in versions 3.3.1 and later.\n\nCdcInsertsOnly and CdcInsertsAndUpdates can’t both be set to true for the same endpoint. Set either CdcInsertsOnly or CdcInsertsAndUpdates to true for the same endpoint, but not both.\n\nDatePartitionEnabled -> (boolean)\n\nWhen set to true , this parameter partitions S3 bucket folders based on transaction commit dates. The default value is false . For more information about date-based folder partitioning, see Using date-based folder partitioning .\n\nDatePartitionSequence -> (string)\n\nIdentifies the sequence of the date format to use during folder partitioning. The default value is YYYYMMDD . Use this parameter when DatePartitionedEnabled is set to true .\n\nDatePartitionDelimiter -> (string)\n\nSpecifies a date separating delimiter to use during folder partitioning. The default value is SLASH . Use this parameter when DatePartitionedEnabled is set to true .\n\nUseCsvNoSupValue -> (boolean)\n\nThis setting applies if the S3 output files during a change data capture (CDC) load are written in .csv format. If set to true for columns not included in the supplemental log, DMS uses the value specified by ` CsvNoSupValue https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CsvNoSupValue`__ . If not set or set to false , DMS uses the null value for these columns.\n\nNote\n\nThis setting is supported in DMS versions 3.4.1 and later.\n\nCsvNoSupValue -> (string)\n\nThis setting only applies if your Amazon S3 output files during a change data capture (CDC) load are written in .csv format. If ` UseCsvNoSupValue https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-UseCsvNoSupValue`__ is set to true, specify a string value that you want DMS to use for all columns not included in the supplemental log. If you do not specify a string value, DMS uses the null value for these columns regardless of the UseCsvNoSupValue setting.\n\nNote\n\nThis setting is supported in DMS versions 3.4.1 and later.\n\nPreserveTransactions -> (boolean)\n\nIf set to true , DMS saves the transaction order for a change data capture (CDC) load on the Amazon S3 target specified by ` CdcPath https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CdcPath`__ . For more information, see Capturing data changes (CDC) including transaction order on the S3 target .\n\nNote\n\nThis setting is supported in DMS versions 3.4.2 and later.\n\nCdcPath -> (string)\n\nSpecifies the folder path of CDC files. For an S3 source, this setting is required if a task captures change data; otherwise, it’s optional. If CdcPath is set, DMS reads CDC files from this path and replicates the data changes to the target endpoint. For an S3 target if you set ` PreserveTransactions https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-PreserveTransactions`__ to true , DMS verifies that you have set this parameter to a folder path on your S3 target where DMS can save the transaction order for the CDC load. DMS creates this CDC folder path in either your S3 target working directory or the S3 target location specified by ` BucketFolder https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketFolder`__ and ` BucketName https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketName`__ .\n\nFor example, if you specify CdcPath as MyChangedData , and you specify BucketName as MyTargetBucket but do not specify BucketFolder , DMS creates the CDC folder path following: MyTargetBucket/MyChangedData .\n\nIf you specify the same CdcPath , and you specify BucketName as MyTargetBucket and BucketFolder as MyTargetData , DMS creates the CDC folder path following: MyTargetBucket/MyTargetData/MyChangedData .\n\nFor more information on CDC including transaction order on an S3 target, see Capturing data changes (CDC) including transaction order on the S3 target .\n\nNote\n\nThis setting is supported in DMS versions 3.4.2 and later.\n\nCannedAclForObjects -> (string)\n\nA value that enables DMS to specify a predefined (canned) access control list for objects created in an Amazon S3 bucket as .csv or .parquet files. For more information about Amazon S3 canned ACLs, see Canned ACL in the Amazon S3 Developer Guide.\n\nThe default value is NONE. Valid values include NONE, PRIVATE, PUBLIC_READ, PUBLIC_READ_WRITE, AUTHENTICATED_READ, AWS_EXEC_READ, BUCKET_OWNER_READ, and BUCKET_OWNER_FULL_CONTROL.\n\nAddColumnName -> (boolean)\n\nAn optional parameter that, when set to true or y , you can use to add column name information to the .csv output file.\n\nThe default value is false . Valid values are true , false , y , and n .\n\nCdcMaxBatchInterval -> (integer)\n\nMaximum length of the interval, defined in seconds, after which to output a file to Amazon S3.\n\nWhen CdcMaxBatchInterval and CdcMinFileSize are both specified, the file write is triggered by whichever parameter condition is met first within an DMS CloudFormation template.\n\nThe default value is 60 seconds.\n\nCdcMinFileSize -> (integer)\n\nMinimum file size, defined in megabytes, to reach for a file output to Amazon S3.\n\nWhen CdcMinFileSize and CdcMaxBatchInterval are both specified, the file write is triggered by whichever parameter condition is met first within an DMS CloudFormation template.\n\nThe default value is 32 MB.\n\nCsvNullValue -> (string)\n\nAn optional parameter that specifies how DMS treats null values. While handling the null value, you can use this parameter to pass a user-defined string as null when writing to the target. For example, when target columns are not nullable, you can use this option to differentiate between the empty string value and the null value. So, if you set this parameter value to the empty string (“” or ‘’), DMS treats the empty string as the null value instead of NULL .\n\nThe default value is NULL . Valid values include any valid string.\n\nIgnoreHeaderRows -> (integer)\n\nWhen this value is set to 1, DMS ignores the first row header in a .csv file. A value of 1 turns on the feature; a value of 0 turns off the feature.\n\nThe default is 0.\n\nMaxFileSize -> (integer)\n\nA value that specifies the maximum size (in KB) of any .csv file to be created while migrating to an S3 target during full load.\n\nThe default value is 1,048,576 KB (1 GB). Valid values include 1 to 1,048,576.\n\nRfc4180 -> (boolean)\n\nFor an S3 source, when this value is set to true or y , each leading double quotation mark has to be followed by an ending double quotation mark. This formatting complies with RFC 4180. When this value is set to false or n , string literals are copied to the target as is. In this case, a delimiter (row or column) signals the end of the field. Thus, you can’t use a delimiter as part of the string, because it signals the end of the value.\n\nFor an S3 target, an optional parameter used to set behavior to comply with RFC 4180 for data migrated to Amazon S3 using .csv file format only. When this value is set to true or y using Amazon S3 as a target, if the data has quotation marks or newline characters in it, DMS encloses the entire column with an additional pair of double quotation marks (“). Every quotation mark within the data is repeated twice.\n\nThe default value is true . Valid values include true , false , y , and n .\n\nDmsTransferSettings -> (structure)\n\nThe settings in JSON format for the DMS transfer type of source endpoint.\n\nPossible settings include the following:\n\nServiceAccessRoleArn - - The Amazon Resource Name (ARN) used by the service access IAM role. The role must allow the iam:PassRole action.\n\nBucketName - The name of the S3 bucket to use.\n\nShorthand syntax for these settings is as follows: ServiceAccessRoleArn=string,BucketName=string,\n\nJSON syntax for these settings is as follows: { \"ServiceAccessRoleArn\": \"string\", \"BucketName\": \"string\"}\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service access IAM role. The role must allow the iam:PassRole action.\n\nBucketName -> (string)\n\nThe name of the S3 bucket to use.\n\nMongoDbSettings -> (structure)\n\nThe settings for the MongoDB source endpoint. For more information, see the MongoDbSettings structure.\n\nUsername -> (string)\n\nThe user name you use to access the MongoDB source endpoint.\n\nPassword -> (string)\n\nThe password for the user account you use to access the MongoDB source endpoint.\n\nServerName -> (string)\n\nThe name of the server on the MongoDB source endpoint.\n\nPort -> (integer)\n\nThe port value for the MongoDB source endpoint.\n\nDatabaseName -> (string)\n\nThe database name on the MongoDB source endpoint.\n\nAuthType -> (string)\n\nThe authentication type you use to access the MongoDB source endpoint.\n\nWhen when set to \"no\" , user name and password parameters are not used and can be empty.\n\nAuthMechanism -> (string)\n\nThe authentication mechanism you use to access the MongoDB source endpoint.\n\nFor the default value, in MongoDB version 2.x, \"default\" is \"mongodb_cr\" . For MongoDB version 3.x or later, \"default\" is \"scram_sha_1\" . This setting isn’t used when AuthType is set to \"no\" .\n\nNestingLevel -> (string)\n\nSpecifies either document or table mode.\n\nDefault value is \"none\" . Specify \"none\" to use document mode. Specify \"one\" to use table mode.\n\nExtractDocId -> (string)\n\nSpecifies the document ID. Use this setting when NestingLevel is set to \"none\" .\n\nDefault value is \"false\" .\n\nDocsToInvestigate -> (string)\n\nIndicates the number of documents to preview to determine the document organization. Use this setting when NestingLevel is set to \"one\" .\n\nMust be a positive value greater than 0 . Default value is 1000 .\n\nAuthSource -> (string)\n\nThe MongoDB database name. This setting isn’t used when AuthType is set to \"no\" .\n\nThe default is \"admin\" .\n\nKmsKeyId -> (string)\n\nThe KMS key identifier that is used to encrypt the content on the replication instance. If you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key. KMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the MongoDB endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the MongoDB endpoint connection details.\n\nKinesisSettings -> (structure)\n\nThe settings for the Amazon Kinesis target endpoint. For more information, see the KinesisSettings structure.\n\nStreamArn -> (string)\n\nThe Amazon Resource Name (ARN) for the Amazon Kinesis Data Streams endpoint.\n\nMessageFormat -> (string)\n\nThe output format for the records created on the endpoint. The message format is JSON (default) or JSON_UNFORMATTED (a single line with no tab).\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) for the IAM role that DMS uses to write to the Kinesis data stream. The role must allow the iam:PassRole action.\n\nIncludeTransactionDetails -> (boolean)\n\nProvides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for transaction_id , previous transaction_id , and transaction_record_id (the record offset within a transaction). The default is false .\n\nIncludePartitionValue -> (boolean)\n\nShows the partition value within the Kinesis message output, unless the partition type is schema-table-type . The default is false .\n\nPartitionIncludeSchemaTable -> (boolean)\n\nPrefixes schema and table names to partition values, when the partition type is primary-key-type . Doing this increases data distribution among Kinesis shards. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same shard, which causes throttling. The default is false .\n\nIncludeTableAlterOperations -> (boolean)\n\nIncludes any data definition language (DDL) operations that change the table in the control data, such as rename-table , drop-table , add-column , drop-column , and rename-column . The default is false .\n\nIncludeControlDetails -> (boolean)\n\nShows detailed control information for table definition, column definition, and table and column changes in the Kinesis message output. The default is false .\n\nIncludeNullAndEmpty -> (boolean)\n\nInclude NULL and empty columns for records migrated to the endpoint. The default is false .\n\nNoHexPrefix -> (boolean)\n\nSet this optional parameter to true to avoid adding a ‘0x’ prefix to raw data in hexadecimal format. For example, by default, DMS adds a ‘0x’ prefix to the LOB column type in hexadecimal format moving from an Oracle source to an Amazon Kinesis target. Use the NoHexPrefix endpoint setting to enable migration of RAW data type columns without adding the ‘0x’ prefix.\n\nKafkaSettings -> (structure)\n\nThe settings for the Apache Kafka target endpoint. For more information, see the KafkaSettings structure.\n\nBroker -> (string)\n\nA comma-separated list of one or more broker locations in your Kafka cluster that host your Kafka instance. Specify each broker location in the form `` broker-hostname-or-ip :port `` . For example, \"ec2-12-345-678-901.compute-1.amazonaws.com:2345\" . For more information and examples of specifying a list of broker locations, see Using Apache Kafka as a target for Database Migration Service in the Database Migration Service User Guide .\n\nTopic -> (string)\n\nThe topic to which you migrate the data. If you don’t specify a topic, DMS specifies \"kafka-default-topic\" as the migration topic.\n\nMessageFormat -> (string)\n\nThe output format for the records created on the endpoint. The message format is JSON (default) or JSON_UNFORMATTED (a single line with no tab).\n\nIncludeTransactionDetails -> (boolean)\n\nProvides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for transaction_id , previous transaction_id , and transaction_record_id (the record offset within a transaction). The default is false .\n\nIncludePartitionValue -> (boolean)\n\nShows the partition value within the Kafka message output unless the partition type is schema-table-type . The default is false .\n\nPartitionIncludeSchemaTable -> (boolean)\n\nPrefixes schema and table names to partition values, when the partition type is primary-key-type . Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. The default is false .\n\nIncludeTableAlterOperations -> (boolean)\n\nIncludes any data definition language (DDL) operations that change the table in the control data, such as rename-table , drop-table , add-column , drop-column , and rename-column . The default is false .\n\nIncludeControlDetails -> (boolean)\n\nShows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. The default is false .\n\nMessageMaxBytes -> (integer)\n\nThe maximum size in bytes for records created on the endpoint The default is 1,000,000.\n\nIncludeNullAndEmpty -> (boolean)\n\nInclude NULL and empty columns for records migrated to the endpoint. The default is false .\n\nSecurityProtocol -> (string)\n\nSet secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include ssl-encryption , ssl-authentication , and sasl-ssl . sasl-ssl requires SaslUsername and SaslPassword .\n\nSslClientCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) of the client certificate used to securely connect to a Kafka target endpoint.\n\nSslClientKeyArn -> (string)\n\nThe Amazon Resource Name (ARN) for the client private key used to securely connect to a Kafka target endpoint.\n\nSslClientKeyPassword -> (string)\n\nThe password for the client private key used to securely connect to a Kafka target endpoint.\n\nSslCaCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the private certificate authority (CA) cert that DMS uses to securely connect to your Kafka target endpoint.\n\nSaslUsername -> (string)\n\nThe secure user name you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n\nSaslPassword -> (string)\n\nThe secure password you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n\nNoHexPrefix -> (boolean)\n\nSet this optional parameter to true to avoid adding a ‘0x’ prefix to raw data in hexadecimal format. For example, by default, DMS adds a ‘0x’ prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the NoHexPrefix endpoint setting to enable migration of RAW data type columns without adding the ‘0x’ prefix.\n\nElasticsearchSettings -> (structure)\n\nThe settings for the Elasticsearch source endpoint. For more information, see the ElasticsearchSettings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nEndpointUri -> (string)\n\nThe endpoint for the Elasticsearch cluster. DMS uses HTTPS if a transport protocol (http/https) is not specified.\n\nFullLoadErrorPercentage -> (integer)\n\nThe maximum percentage of records that can fail to be written before a full load operation stops.\n\nTo avoid early failure, this counter is only effective after 1000 records are transferred. Elasticsearch also has the concept of error monitoring during the last 10 minutes of an Observation Window. If transfer of all records fail in the last 10 minutes, the full load operation stops.\n\nErrorRetryDuration -> (integer)\n\nThe maximum number of seconds for which DMS retries failed API requests to the Elasticsearch cluster.\n\nNeptuneSettings -> (structure)\n\nThe settings for the Amazon Neptune target endpoint. For more information, see the NeptuneSettings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the service role that you created for the Neptune target endpoint. The role must allow the iam:PassRole action. For more information, see Creating an IAM Service Role for Accessing Amazon Neptune as a Target in the Database Migration Service User Guide.\n\nS3BucketName -> (string)\n\nThe name of the Amazon S3 bucket where DMS can temporarily store migrated graph data in .csv files before bulk-loading it to the Neptune target database. DMS maps the SQL source data to graph data before storing it in these .csv files.\n\nS3BucketFolder -> (string)\n\nA folder path where you want DMS to store migrated graph data in the S3 bucket specified by S3BucketName\n\nErrorRetryDuration -> (integer)\n\nThe number of milliseconds for DMS to wait to retry a bulk-load of migrated graph data to the Neptune target database before raising an error. The default is 250.\n\nMaxFileSize -> (integer)\n\nThe maximum size in kilobytes of migrated graph data stored in a .csv file before DMS bulk-loads the data to the Neptune target database. The default is 1,048,576 KB. If the bulk load is successful, DMS clears the bucket, ready to store the next batch of migrated graph data.\n\nMaxRetryCount -> (integer)\n\nThe number of times for DMS to retry a bulk load of migrated graph data to the Neptune target database before raising an error. The default is 5.\n\nIamAuthEnabled -> (boolean)\n\nIf you want Identity and Access Management (IAM) authorization enabled for this endpoint, set this parameter to true . Then attach the appropriate IAM policy document to your service role specified by ServiceAccessRoleArn . The default is false .\n\nRedshiftSettings -> (structure)\n\nSettings for the Amazon Redshift endpoint.\n\nAcceptAnyDate -> (boolean)\n\nA value that indicates to allow any date format, including invalid formats such as 00/00/00 00:00:00, to be loaded without generating an error. You can choose true or false (the default).\n\nThis parameter applies only to TIMESTAMP and DATE columns. Always use ACCEPTANYDATE with the DATEFORMAT parameter. If the date format for the data doesn’t match the DATEFORMAT specification, Amazon Redshift inserts a NULL value into that field.\n\nAfterConnectScript -> (string)\n\nCode to run after connecting. This parameter should contain the code itself, not the name of a file containing the code.\n\nBucketFolder -> (string)\n\nAn S3 folder where the comma-separated-value (.csv) files are stored before being uploaded to the target Redshift cluster.\n\nFor full load mode, DMS converts source records into .csv files and loads them to the BucketFolder/TableID path. DMS uses the Redshift COPY command to upload the .csv files to the target table. The files are deleted once the COPY operation has finished. For more information, see COPY in the Amazon Redshift Database Developer Guide .\n\nFor change-data-capture (CDC) mode, DMS creates a NetChanges table, and loads the .csv files to this BucketFolder/NetChangesTableID path.\n\nBucketName -> (string)\n\nThe name of the intermediate S3 bucket used to store .csv files before uploading data to Redshift.\n\nCaseSensitiveNames -> (boolean)\n\nIf Amazon Redshift is configured to support case sensitive schema names, set CaseSensitiveNames to true . The default is false .\n\nCompUpdate -> (boolean)\n\nIf you set CompUpdate to true Amazon Redshift applies automatic compression if the table is empty. This applies even if the table columns already have encodings other than RAW . If you set CompUpdate to false , automatic compression is disabled and existing column encodings aren’t changed. The default is true .\n\nConnectionTimeout -> (integer)\n\nA value that sets the amount of time to wait (in milliseconds) before timing out, beginning from when you initially establish a connection.\n\nDatabaseName -> (string)\n\nThe name of the Amazon Redshift data warehouse (service) that you are working with.\n\nDateFormat -> (string)\n\nThe date format that you are using. Valid values are auto (case-sensitive), your date format string enclosed in quotes, or NULL. If this parameter is left unset (NULL), it defaults to a format of ‘YYYY-MM-DD’. Using auto recognizes most strings, even some that aren’t supported when you use a date format string.\n\nIf your date and time values use formats different from each other, set this to auto .\n\nEmptyAsNull -> (boolean)\n\nA value that specifies whether DMS should migrate empty CHAR and VARCHAR fields as NULL. A value of true sets empty CHAR and VARCHAR fields to null. The default is false .\n\nEncryptionMode -> (string)\n\nThe type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either SSE_S3 (the default) or SSE_KMS .\n\nNote\n\nFor the ModifyEndpoint operation, you can change the existing value of the EncryptionMode parameter from SSE_KMS to SSE_S3 . But you can’t change the existing value from SSE_S3 to SSE_KMS .\n\nTo use SSE_S3 , create an Identity and Access Management (IAM) role with a policy that allows \"arn:aws:s3:::*\" to use the following actions: \"s3:PutObject\", \"s3:ListBucket\"\n\nExplicitIds -> (boolean)\n\nThis setting is only valid for a full-load migration task. Set ExplicitIds to true to have tables with IDENTITY columns override their auto-generated values with explicit values loaded from the source data files used to populate the tables. The default is false .\n\nFileTransferUploadStreams -> (integer)\n\nThe number of threads used to upload a single file. This parameter accepts a value from 1 through 64. It defaults to 10.\n\nThe number of parallel streams used to upload a single .csv file to an S3 bucket using S3 Multipart Upload. For more information, see Multipart upload overview .\n\nFileTransferUploadStreams accepts a value from 1 through 64. It defaults to 10.\n\nLoadTimeout -> (integer)\n\nThe amount of time to wait (in milliseconds) before timing out of operations performed by DMS on a Redshift cluster, such as Redshift COPY, INSERT, DELETE, and UPDATE.\n\nMaxFileSize -> (integer)\n\nThe maximum size (in KB) of any .csv file used to load data on an S3 bucket and transfer data to Amazon Redshift. It defaults to 1048576KB (1 GB).\n\nPassword -> (string)\n\nThe password for the user named in the username property.\n\nPort -> (integer)\n\nThe port number for Amazon Redshift. The default value is 5439.\n\nRemoveQuotes -> (boolean)\n\nA value that specifies to remove surrounding quotation marks from strings in the incoming data. All characters within the quotation marks, including delimiters, are retained. Choose true to remove quotation marks. The default is false .\n\nReplaceInvalidChars -> (string)\n\nA list of characters that you want to replace. Use with ReplaceChars .\n\nReplaceChars -> (string)\n\nA value that specifies to replaces the invalid characters specified in ReplaceInvalidChars , substituting the specified characters instead. The default is \"?\" .\n\nServerName -> (string)\n\nThe name of the Amazon Redshift cluster you are using.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the IAM role that has access to the Amazon Redshift service. The role must allow the iam:PassRole action.\n\nServerSideEncryptionKmsKeyId -> (string)\n\nThe KMS key ID. If you are using SSE_KMS for the EncryptionMode , provide this key ID. The key that you use needs an attached policy that enables IAM user permissions and allows use of the key.\n\nTimeFormat -> (string)\n\nThe time format that you want to use. Valid values are auto (case-sensitive), 'timeformat_string' , 'epochsecs' , or 'epochmillisecs' . It defaults to 10. Using auto recognizes most strings, even some that aren’t supported when you use a time format string.\n\nIf your date and time values use formats different from each other, set this parameter to auto .\n\nTrimBlanks -> (boolean)\n\nA value that specifies to remove the trailing white space characters from a VARCHAR string. This parameter applies only to columns with a VARCHAR data type. Choose true to remove unneeded white space. The default is false .\n\nTruncateColumns -> (boolean)\n\nA value that specifies to truncate data in columns to the appropriate number of characters, so that the data fits in the column. This parameter applies only to columns with a VARCHAR or CHAR data type, and rows with a size of 4 MB or less. Choose true to truncate data. The default is false .\n\nUsername -> (string)\n\nAn Amazon Redshift user name for a registered user.\n\nWriteBufferSize -> (integer)\n\nThe size (in KB) of the in-memory file write buffer used when generating .csv files on the local disk at the DMS replication instance. The default value is 1000 (buffer size is 1000KB).\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Amazon Redshift endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Amazon Redshift endpoint connection details.\n\nPostgreSQLSettings -> (structure)\n\nThe settings for the PostgreSQL source and target endpoint. For more information, see the PostgreSQLSettings structure.\n\nAfterConnectScript -> (string)\n\nFor use with change data capture (CDC) only, this attribute has DMS bypass foreign keys and user triggers to reduce the time it takes to bulk load data.\n\nExample: afterConnectScript=SET session_replication_role='replica'\n\nCaptureDdls -> (boolean)\n\nTo capture DDL events, DMS creates various artifacts in the PostgreSQL database when the task starts. You can later remove these artifacts.\n\nIf this value is set to N , you don’t have to create tables or triggers on the source database.\n\nMaxFileSize -> (integer)\n\nSpecifies the maximum size (in KB) of any .csv file used to transfer data to PostgreSQL.\n\nExample: maxFileSize=512\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nDdlArtifactsSchema -> (string)\n\nThe schema in which the operational DDL database artifacts are created.\n\nExample: ddlArtifactsSchema=xyzddlschema;\n\nExecuteTimeout -> (integer)\n\nSets the client statement timeout for the PostgreSQL instance, in seconds. The default value is 60 seconds.\n\nExample: executeTimeout=100;\n\nFailTasksOnLobTruncation -> (boolean)\n\nWhen set to true , this value causes a task to fail if the actual size of a LOB column is greater than the specified LobMaxSize .\n\nIf task is set to Limited LOB mode and this option is set to true, the task fails instead of truncating the LOB data.\n\nHeartbeatEnable -> (boolean)\n\nThe write-ahead log (WAL) heartbeat feature mimics a dummy transaction. By doing this, it prevents idle logical replication slots from holding onto old WAL logs, which can result in storage full situations on the source. This heartbeat keeps restart_lsn moving and prevents storage full scenarios.\n\nHeartbeatSchema -> (string)\n\nSets the schema in which the heartbeat artifacts are created.\n\nHeartbeatFrequency -> (integer)\n\nSets the WAL heartbeat frequency (in minutes).\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSlotName -> (string)\n\nSets the name of a previously created logical replication slot for a change data capture (CDC) load of the PostgreSQL source instance.\n\nWhen used with the CdcStartPosition request parameter for the DMS API , this attribute also makes it possible to use native CDC start points. DMS verifies that the specified logical replication slot exists before starting the CDC load task. It also verifies that the task was created with a valid setting of CdcStartPosition . If the specified slot doesn’t exist or the task doesn’t have a valid CdcStartPosition setting, DMS raises an error.\n\nFor more information about setting the CdcStartPosition request parameter, see Determining a CDC native start point in the Database Migration Service User Guide . For more information about using CdcStartPosition , see CreateReplicationTask , StartReplicationTask , and ModifyReplicationTask .\n\nPluginName -> (string)\n\nSpecifies the plugin to use to create a replication slot.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the PostgreSQL endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the PostgreSQL endpoint connection details.\n\nMySQLSettings -> (structure)\n\nThe settings for the MySQL source and target endpoint. For more information, see the MySQLSettings structure.\n\nAfterConnectScript -> (string)\n\nSpecifies a script to run immediately after DMS connects to the endpoint. The migration task continues running regardless if the SQL statement succeeds or fails.\n\nFor this parameter, provide the code of the script itself, not the name of a file containing the script.\n\nCleanSourceMetadataOnMismatch -> (boolean)\n\nAdjusts the behavior of DMS when migrating from an SQL Server source database that is hosted as part of an Always On availability group cluster. If you need DMS to poll all the nodes in the Always On cluster for transaction backups, set this attribute to false .\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint. For a MySQL source or target endpoint, don’t explicitly specify the database using the DatabaseName request parameter on either the CreateEndpoint or ModifyEndpoint API call. Specifying DatabaseName when you create or modify a MySQL endpoint replicates all the task tables to this single database. For MySQL endpoints, you specify the database only when you specify the schema in the table-mapping rules of the DMS task.\n\nEventsPollInterval -> (integer)\n\nSpecifies how often to check the binary log for new changes/events when the database is idle.\n\nExample: eventsPollInterval=5;\n\nIn the example, DMS checks for changes in the binary logs every five seconds.\n\nTargetDbType -> (string)\n\nSpecifies where to migrate source tables on the target, either to a single database or multiple databases.\n\nExample: targetDbType=MULTIPLE_DATABASES\n\nMaxFileSize -> (integer)\n\nSpecifies the maximum size (in KB) of any .csv file used to transfer data to a MySQL-compatible database.\n\nExample: maxFileSize=512\n\nParallelLoadThreads -> (integer)\n\nImproves performance when loading data into the MySQL-compatible target database. Specifies how many threads to use to load the data into the MySQL-compatible target database. Setting a large number of threads can have an adverse effect on database performance, because a separate connection is required for each thread.\n\nExample: parallelLoadThreads=1\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nServerTimezone -> (string)\n\nSpecifies the time zone for the source MySQL database.\n\nExample: serverTimezone=US/Pacific;\n\nNote: Do not enclose time zones in single quotes.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the MySQL endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the MySQL endpoint connection details.\n\nOracleSettings -> (structure)\n\nThe settings for the Oracle source and target endpoint. For more information, see the OracleSettings structure.\n\nAddSupplementalLogging -> (boolean)\n\nSet this attribute to set up table-level supplemental logging for the Oracle database. This attribute enables PRIMARY KEY supplemental logging on all tables selected for a migration task.\n\nIf you use this option, you still need to enable database-level supplemental logging.\n\nArchivedLogDestId -> (integer)\n\nSpecifies the ID of the destination for the archived redo logs. This value should be the same as a number in the dest_id column of the v$archived_log view. If you work with an additional redo log destination, use the AdditionalArchivedLogDestId option to specify the additional destination ID. Doing this improves performance by ensuring that the correct logs are accessed from the outset.\n\nAdditionalArchivedLogDestId -> (integer)\n\nSet this attribute with ArchivedLogDestId in a primary/ standby setup. This attribute is useful in the case of a switchover. In this case, DMS needs to know which destination to get archive redo logs from to read changes. This need arises because the previous primary instance is now a standby instance after switchover.\n\nAlthough DMS supports the use of the Oracle RESETLOGS option to open the database, never use RESETLOGS unless necessary. For additional information about RESETLOGS , see RMAN Data Repair Concepts in the Oracle Database Backup and Recovery User’s Guide .\n\nExtraArchivedLogDestIds -> (list)\n\nSpecifies the IDs of one more destinations for one or more archived redo logs. These IDs are the values of the dest_id column in the v$archived_log view. Use this setting with the archivedLogDestId extra connection attribute in a primary-to-single setup or a primary-to-multiple-standby setup.\n\nThis setting is useful in a switchover when you use an Oracle Data Guard database as a source. In this case, DMS needs information about what destination to get archive redo logs from to read changes. DMS needs this because after the switchover the previous primary is a standby instance. For example, in a primary-to-single standby setup you might apply the following settings.\n\narchivedLogDestId=1; ExtraArchivedLogDestIds=[2]\n\nIn a primary-to-multiple-standby setup, you might apply the following settings.\n\narchivedLogDestId=1; ExtraArchivedLogDestIds=[2,3,4]\n\nAlthough DMS supports the use of the Oracle RESETLOGS option to open the database, never use RESETLOGS unless it’s necessary. For more information about RESETLOGS , see RMAN Data Repair Concepts in the Oracle Database Backup and Recovery User’s Guide .\n\n(integer)\n\nAllowSelectNestedTables -> (boolean)\n\nSet this attribute to true to enable replication of Oracle tables containing columns that are nested tables or defined types.\n\nParallelAsmReadThreads -> (integer)\n\nSet this attribute to change the number of threads that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 2 (the default) and 8 (the maximum). Use this attribute together with the readAheadBlocks attribute.\n\nReadAheadBlocks -> (integer)\n\nSet this attribute to change the number of read-ahead blocks that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 1000 (the default) and 200,000 (the maximum).\n\nAccessAlternateDirectly -> (boolean)\n\nSet this attribute to false in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to not access redo logs through any specified path prefix replacement using direct file access.\n\nUseAlternateFolderForOnline -> (boolean)\n\nSet this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to use any specified prefix replacement to access all online redo logs.\n\nOraclePathPrefix -> (string)\n\nSet this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the default Oracle root used to access the redo logs.\n\nUsePathPrefix -> (string)\n\nSet this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the path prefix used to replace the default Oracle root to access the redo logs.\n\nReplacePathPrefix -> (boolean)\n\nSet this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This setting tells DMS instance to replace the default Oracle root with the specified usePathPrefix setting to access the redo logs.\n\nEnableHomogenousTablespace -> (boolean)\n\nSet this attribute to enable homogenous tablespace replication and create existing tables or indexes under the same tablespace on the target.\n\nDirectPathNoLog -> (boolean)\n\nWhen set to true , this attribute helps to increase the commit rate on the Oracle target database by writing directly to tables and not writing a trail to database logs.\n\nArchivedLogsOnly -> (boolean)\n\nWhen this field is set to Y , DMS only accesses the archived redo logs. If the archived redo logs are stored on Oracle ASM only, the DMS user account needs to be granted ASM privileges.\n\nAsmPassword -> (string)\n\nFor an Oracle source endpoint, your Oracle Automatic Storage Management (ASM) password. You can set this value from the `` asm_user_password `` value. You set this value as part of the comma-separated value that you set to the Password request parameter when you create the endpoint to access transaction logs using Binary Reader. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nAsmServer -> (string)\n\nFor an Oracle source endpoint, your ASM server address. You can set this value from the asm_server value. You set asm_server as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nAsmUser -> (string)\n\nFor an Oracle source endpoint, your ASM user name. You can set this value from the asm_user value. You set asm_user as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nCharLengthSemantics -> (string)\n\nSpecifies whether the length of a character column is in bytes or in characters. To indicate that the character column length is in characters, set this attribute to CHAR . Otherwise, the character column length is in bytes.\n\nExample: charLengthSemantics=CHAR;\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nDirectPathParallelLoad -> (boolean)\n\nWhen set to true , this attribute specifies a parallel load when useDirectPathFullLoad is set to Y . This attribute also only applies when you use the DMS parallel load feature. Note that the target table cannot have any constraints or indexes.\n\nFailTasksOnLobTruncation -> (boolean)\n\nWhen set to true , this attribute causes a task to fail if the actual size of an LOB column is greater than the specified LobMaxSize .\n\nIf a task is set to limited LOB mode and this option is set to true , the task fails instead of truncating the LOB data.\n\nNumberDatatypeScale -> (integer)\n\nSpecifies the number scale. You can select a scale up to 38, or you can select FLOAT. By default, the NUMBER data type is converted to precision 38, scale 10.\n\nExample: numberDataTypeScale=12\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nReadTableSpaceName -> (boolean)\n\nWhen set to true , this attribute supports tablespace replication.\n\nRetryInterval -> (integer)\n\nSpecifies the number of seconds that the system waits before resending a query.\n\nExample: retryInterval=6;\n\nSecurityDbEncryption -> (string)\n\nFor an Oracle source endpoint, the transparent data encryption (TDE) password required by AWM DMS to access Oracle redo logs encrypted by TDE using Binary Reader. It is also the `` TDE_Password `` part of the comma-separated value you set to the Password request parameter when you create the endpoint. The SecurityDbEncryptian setting is related to this SecurityDbEncryptionName setting. For more information, see Supported encryption methods for using Oracle as a source for DMS in the Database Migration Service User Guide .\n\nSecurityDbEncryptionName -> (string)\n\nFor an Oracle source endpoint, the name of a key used for the transparent data encryption (TDE) of the columns and tablespaces in an Oracle source database that is encrypted using TDE. The key value is the value of the SecurityDbEncryption setting. For more information on setting the key name value of SecurityDbEncryptionName , see the information and example for setting the securityDbEncryptionName extra connection attribute in Supported encryption methods for using Oracle as a source for DMS in the Database Migration Service User Guide .\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nSpatialDataOptionToGeoJsonFunctionName -> (string)\n\nUse this attribute to convert SDO_GEOMETRY to GEOJSON format. By default, DMS calls the SDO2GEOJSON custom function if present and accessible. Or you can create your own custom function that mimics the operation of SDOGEOJSON and set SpatialDataOptionToGeoJsonFunctionName to call it instead.\n\nStandbyDelayTime -> (integer)\n\nUse this attribute to specify a time in minutes for the delay in standby sync. If the source is an Oracle Active Data Guard standby database, use this attribute to specify the time lag between primary and standby databases.\n\nIn DMS, you can create an Oracle CDC task that uses an Active Data Guard standby instance as a source for replicating ongoing changes. Doing this eliminates the need to connect to an active database that might be in production.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nUseBFile -> (boolean)\n\nSet this attribute to Y to capture change data using the Binary Reader utility. Set UseLogminerReader to N to set this attribute to Y. To use Binary Reader with Amazon RDS for Oracle as the source, you set additional attributes. For more information about using this setting with Oracle Automatic Storage Management (ASM), see Using Oracle LogMiner or DMS Binary Reader for CDC .\n\nUseDirectPathFullLoad -> (boolean)\n\nSet this attribute to Y to have DMS use a direct path full load. Specify this value to use the direct path protocol in the Oracle Call Interface (OCI). By using this OCI protocol, you can bulk-load Oracle target tables during a full load.\n\nUseLogminerReader -> (boolean)\n\nSet this attribute to Y to capture change data using the Oracle LogMiner utility (the default). Set this attribute to N if you want to access the redo logs as a binary file. When you set UseLogminerReader to N, also set UseBfile to Y. For more information on this setting and using Oracle ASM, see Using Oracle LogMiner or DMS Binary Reader for CDC in the DMS User Guide .\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Oracle endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Oracle endpoint connection details.\n\nSecretsManagerOracleAsmAccessRoleArn -> (string)\n\nRequired only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the SecretsManagerOracleAsmSecret . This SecretsManagerOracleAsmSecret has the secret value that allows access to the Oracle ASM of the endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerOracleAsmSecretId . Or you can specify clear-text values for AsmUserName , AsmPassword , and AsmServerName . You can’t specify both. For more information on creating this SecretsManagerOracleAsmSecret and the SecretsManagerOracleAsmAccessRoleArn and SecretsManagerOracleAsmSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerOracleAsmSecretId -> (string)\n\nRequired only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN, partial ARN, or friendly name of the SecretsManagerOracleAsmSecret that contains the Oracle ASM connection details for the Oracle endpoint.\n\nSybaseSettings -> (structure)\n\nThe settings for the SAP ASE source and target endpoint. For more information, see the SybaseSettings structure.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the SAP ASE endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the SAP SAE endpoint connection details.\n\nMicrosoftSQLServerSettings -> (structure)\n\nThe settings for the Microsoft SQL Server source and target endpoint. For more information, see the MicrosoftSQLServerSettings structure.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nBcpPacketSize -> (integer)\n\nThe maximum size of the packets (in bytes) used to transfer data using BCP.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nControlTablesFileGroup -> (string)\n\nSpecifies a file group for the DMS internal tables. When the replication task starts, all the internal DMS control tables (awsdms_ apply_exception, awsdms_apply, awsdms_changes) are created for the specified file group.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nQuerySingleAlwaysOnNode -> (boolean)\n\nCleans and recreates table metadata information on the replication instance when a mismatch occurs. An example is a situation where running an alter DDL statement on a table might result in different information about the table cached in the replication instance.\n\nReadBackupOnly -> (boolean)\n\nWhen this attribute is set to Y , DMS only reads changes from transaction log backups and doesn’t read from the active transaction log file during ongoing replication. Setting this parameter to Y enables you to control active transaction log file growth during full load and ongoing replication tasks. However, it can add some source latency to ongoing replication.\n\nSafeguardPolicy -> (string)\n\nUse this attribute to minimize the need to access the backup log and enable DMS to prevent truncation using one of the following two methods.\n\nStart transactions in the database: This is the default method. When this method is used, DMS prevents TLOG truncation by mimicking a transaction in the database. As long as such a transaction is open, changes that appear after the transaction started aren’t truncated. If you need Microsoft Replication to be enabled in your database, then you must choose this method.\n\nExclusively use sp_repldone within a single task : When this method is used, DMS reads the changes and then uses sp_repldone to mark the TLOG transactions as ready for truncation. Although this method doesn’t involve any transactional activities, it can only be used when Microsoft Replication isn’t running. Also, when using this method, only one DMS task can access the database at any given time. Therefore, if you need to run parallel DMS tasks against the same database, use the default method.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nUseBcpFullLoad -> (boolean)\n\nUse this to attribute to transfer data for full-load operations using BCP. When the target table contains an identity column that does not exist in the source table, you must disable the use BCP for loading table option.\n\nUseThirdPartyBackupDevice -> (boolean)\n\nWhen this attribute is set to Y , DMS processes third-party transaction log backups if they are created in native format.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the SQL Server endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the SQL Server endpoint connection details.\n\nIBMDb2Settings -> (structure)\n\nThe settings for the IBM Db2 LUW source endpoint. For more information, see the IBMDb2Settings structure.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port. The default value is 50000.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nSetDataCaptureChanges -> (boolean)\n\nEnables ongoing replication (CDC) as a BOOLEAN value. The default is true.\n\nCurrentLsn -> (string)\n\nFor ongoing replication (CDC), use CurrentLSN to specify a log sequence number (LSN) where you want the replication to start.\n\nMaxKBytesPerRead -> (integer)\n\nMaximum number of bytes per read, as a NUMBER value. The default is 64 KB.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Db2 LUW endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Db2 LUW endpoint connection details.\n\nDocDbSettings -> (structure)\n\nProvides information that defines a DocumentDB endpoint.\n\nUsername -> (string)\n\nThe user name you use to access the DocumentDB source endpoint.\n\nPassword -> (string)\n\nThe password for the user account you use to access the DocumentDB source endpoint.\n\nServerName -> (string)\n\nThe name of the server on the DocumentDB source endpoint.\n\nPort -> (integer)\n\nThe port value for the DocumentDB source endpoint.\n\nDatabaseName -> (string)\n\nThe database name on the DocumentDB source endpoint.\n\nNestingLevel -> (string)\n\nSpecifies either document or table mode.\n\nDefault value is \"none\" . Specify \"none\" to use document mode. Specify \"one\" to use table mode.\n\nExtractDocId -> (boolean)\n\nSpecifies the document ID. Use this setting when NestingLevel is set to \"none\" .\n\nDefault value is \"false\" .\n\nDocsToInvestigate -> (integer)\n\nIndicates the number of documents to preview to determine the document organization. Use this setting when NestingLevel is set to \"one\" .\n\nMust be a positive value greater than 0 . Default value is 1000 .\n\nKmsKeyId -> (string)\n\nThe KMS key identifier that is used to encrypt the content on the replication instance. If you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key. KMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the DocumentDB endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the DocumentDB endpoint connection details.\n\nRedisSettings -> (structure)\n\nThe settings for the Redis target endpoint. For more information, see the RedisSettings structure.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nPort -> (integer)\n\nTransmission Control Protocol (TCP) port for the endpoint.\n\nSslSecurityProtocol -> (string)\n\nThe connection to a Redis target endpoint using Transport Layer Security (TLS). Valid values include plaintext and ssl-encryption . The default is ssl-encryption . The ssl-encryption option makes an encrypted connection. Optionally, you can identify an Amazon Resource Name (ARN) for an SSL certificate authority (CA) using the SslCaCertificateArn setting. If an ARN isn’t given for a CA, DMS uses the Amazon root CA.\n\nThe plaintext option doesn’t provide Transport Layer Security (TLS) encryption for traffic between endpoint and database.\n\nAuthType -> (string)\n\nThe type of authentication to perform when connecting to a Redis target. Options include none , auth-token , and auth-role . The auth-token option requires an AuthPassword value to be provided. The auth-role option requires AuthUserName and AuthPassword values to be provided.\n\nAuthUserName -> (string)\n\nThe user name provided with the auth-role option of the AuthType setting for a Redis target endpoint.\n\nAuthPassword -> (string)\n\nThe password provided with the auth-role and auth-token options of the AuthType setting for a Redis target endpoint.\n\nSslCaCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the certificate authority (CA) that DMS uses to connect to your Redis target endpoint.",
      "command_examples": "Examples\n\nTo describe endpoints\n\nThe following describe-endpoints example lists the endpoints in your AWS account.\n\naws dms describe-endpoints\n\n\nOutput:\n\n{\n    \"Endpoints\": [\n        {\n            \"Username\": \"dms\",\n            \"Status\": \"active\",\n            \"EndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:SF2WOFLWYWKVEOHID2EKLP3SJI\",\n            \"ServerName\": \"ec2-52-32-48-61.us-west-2.compute.amazonaws.com\",\n            \"EndpointType\": \"SOURCE\",\n            \"KmsKeyId\": \"arn:aws:kms:us-east-1:123456789012:key/94d5c4e7-4e4c-44be-b58a-c8da7adf57cd\",\n            \"DatabaseName\": \"test\",\n            \"EngineName\": \"mysql\",\n            \"EndpointIdentifier\": \"pri100\",\n            \"Port\": 8193\n        },\n        {\n            \"Username\": \"admin\",\n            \"Status\": \"active\",\n            \"EndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:TJJZCIH3CJ24TJRU4VC32WEWFR\",\n            \"ServerName\": \"test.example.com\",\n            \"EndpointType\": \"SOURCE\",\n            \"KmsKeyId\": \"arn:aws:kms:us-east-1:123456789012:key/2431021b-1cf2-a2d4-77b2-59a9e4bce323\",\n            \"DatabaseName\": \"EMPL\",\n            \"EngineName\": \"oracle\",\n            \"EndpointIdentifier\": \"test\",\n            \"Port\": 1521\n        }\n    ]\n}\n\n\nFor more information, see Working with AWS DMS Endpoints in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-event-categories",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-event-categories.html",
      "command_description": "Description\n\nLists categories for all event source types, or, if specified, for a specified source type. You can see a list of the event categories and source types in Working with Events and Notifications in the Database Migration Service User Guide.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-event-categories\n[--source-type <value>]\n[--filters <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--source-type <value>]",
        "[--filters <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--source-type (string)\n\nThe type of DMS resource that generates events.\n\nValid values: replication-instance | replication-task\n\n--filters (list)\n\nFilters applied to the event categories.\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nEventCategoryGroupList -> (list)\n\nA list of event categories.\n\n(structure)\n\nLists categories of events subscribed to, and generated by, the applicable DMS resource type. This data type appears in response to the ` DescribeEventCategories https://docs.aws.amazon.com/dms/latest/APIReference/API_EventCategoryGroup.html`__ action.\n\nSourceType -> (string)\n\nThe type of DMS resource that generates events.\n\nValid values: replication-instance | replication-server | security-group | replication-task\n\nEventCategories -> (list)\n\nA list of event categories from a source type that you’ve chosen.\n\n(string)",
      "command_examples": "Examples\n\nTo describe event categories\n\nThe following describe-event-categories example lists the available event categories.\n\naws dms describe-event-categories\n\n\nOutput:\n\n{\n    \"EventCategoryGroupList\": [\n        {\n            \"SourceType\": \"replication-instance\",\n            \"EventCategories\": [\n                \"low storage\",\n                \"configuration change\",\n                \"maintenance\",\n                \"deletion\",\n                \"creation\",\n                \"failover\",\n                \"failure\"\n            ]\n        },\n        {\n            \"SourceType\": \"replication-task\",\n            \"EventCategories\": [\n                \"configuration change\",\n                \"state change\",\n                \"deletion\",\n                \"creation\",\n                \"failure\"\n            ]\n        }\n    ]\n}\n\n\nFor more information, see Working with Events and Notifications in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-event-subscriptions",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-event-subscriptions.html",
      "command_description": "Description\n\nLists all the event subscriptions for a customer account. The description of a subscription includes SubscriptionName , SNSTopicARN , CustomerID , SourceType , SourceID , CreationTime , and Status .\n\nIf you specify SubscriptionName , this action lists the description for that subscription.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-event-subscriptions is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: EventSubscriptionsList",
      "command_synopsis": "Synopsis\n  describe-event-subscriptions\n[--subscription-name <value>]\n[--filters <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--subscription-name <value>]",
        "[--filters <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--subscription-name (string)\n\nThe name of the DMS event subscription to be described.\n\n--filters (list)\n\nFilters applied to event subscriptions.\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\nEventSubscriptionsList -> (list)\n\nA list of event subscriptions.\n\n(structure)\n\nDescribes an event notification subscription created by the CreateEventSubscription operation.\n\nCustomerAwsId -> (string)\n\nThe Amazon Web Services customer account associated with the DMS event notification subscription.\n\nCustSubscriptionId -> (string)\n\nThe DMS event notification subscription Id.\n\nSnsTopicArn -> (string)\n\nThe topic ARN of the DMS event notification subscription.\n\nStatus -> (string)\n\nThe status of the DMS event notification subscription.\n\nConstraints:\n\nCan be one of the following: creating | modifying | deleting | active | no-permission | topic-not-exist\n\nThe status “no-permission” indicates that DMS no longer has permission to post to the SNS topic. The status “topic-not-exist” indicates that the topic was deleted after the subscription was created.\n\nSubscriptionCreationTime -> (string)\n\nThe time the DMS event notification subscription was created.\n\nSourceType -> (string)\n\nThe type of DMS resource that generates events.\n\nValid values: replication-instance | replication-server | security-group | replication-task\n\nSourceIdsList -> (list)\n\nA list of source Ids for the event subscription.\n\n(string)\n\nEventCategoriesList -> (list)\n\nA lists of event categories.\n\n(string)\n\nEnabled -> (boolean)\n\nBoolean value that indicates if the event subscription is enabled.",
      "command_examples": "Examples\n\nTo describe event subscriptions\n\nThe following describe-event-subscriptions example lists the event subscriptions to an Amazon SNS topic.\n\naws dms describe-event-subscriptions\n\n\nOutput:\n\n{\n    \"EventSubscriptionsList\": [\n        {\n            \"CustomerAwsId\": \"123456789012\",\n            \"CustSubscriptionId\": \"my-dms-events\",\n            \"SnsTopicArn\": \"arn:aws:sns:us-east-1:123456789012:my-sns-topic\",\n            \"Status\": \"deleting\",\n            \"SubscriptionCreationTime\": \"2020-05-21 22:28:51.924\",\n            \"Enabled\": true\n        }\n    ]\n}\n\n\nFor more information, see Working with Events and Notifications in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-events",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-events.html",
      "command_description": "Description\n\nLists events for a given source identifier and source type. You can also specify a start and end time. For more information on DMS events, see Working with Events and Notifications in the Database Migration Service User Guide.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-events is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: Events",
      "command_synopsis": "Synopsis\n  describe-events\n[--source-identifier <value>]\n[--source-type <value>]\n[--start-time <value>]\n[--end-time <value>]\n[--duration <value>]\n[--event-categories <value>]\n[--filters <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--source-identifier <value>]",
        "[--source-type <value>]",
        "[--start-time <value>]",
        "[--end-time <value>]",
        "[--duration <value>]",
        "[--event-categories <value>]",
        "[--filters <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--source-identifier (string)\n\nThe identifier of an event source.\n\n--source-type (string)\n\nThe type of DMS resource that generates events.\n\nValid values: replication-instance | replication-task\n\nPossible values:\n\nreplication-instance\n\n--start-time (timestamp)\n\nThe start time for the events to be listed.\n\n--end-time (timestamp)\n\nThe end time for the events to be listed.\n\n--duration (integer)\n\nThe duration of the events to be listed.\n\n--event-categories (list)\n\nA list of event categories for the source type that you’ve chosen.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--filters (list)\n\nFilters applied to events.\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\nEvents -> (list)\n\nThe events described.\n\n(structure)\n\nDescribes an identifiable significant activity that affects a replication instance or task. This object can provide the message, the available event categories, the date and source of the event, and the DMS resource type.\n\nSourceIdentifier -> (string)\n\nThe identifier of an event source.\n\nSourceType -> (string)\n\nThe type of DMS resource that generates events.\n\nValid values: replication-instance | endpoint | replication-task\n\nMessage -> (string)\n\nThe event message.\n\nEventCategories -> (list)\n\nThe event categories available for the specified source type.\n\n(string)\n\nDate -> (timestamp)\n\nThe date of the event.",
      "command_examples": "Examples\n\nTo list DMS events\n\nThe following describe-events example lists the events that originated from a replication instance.\n\naws dms describe-events \\\n    --source-type \"replication-instance\"\n\n\nOutput:\n\n{\n    \"Events\": [\n        {\n            \"SourceIdentifier\": \"my-repl-instance\",\n            \"SourceType\": \"replication-instance\",\n            \"Message\": \"Replication application shutdown\",\n            \"EventCategories\": [],\n            \"Date\": 1590771645.776\n        }\n    ]\n}\n\n\nFor more information, see Working with Events and Notifications in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-orderable-replication-instances",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-orderable-replication-instances.html",
      "command_description": "Description\n\nReturns information about the replication instance types that can be created in the specified region.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-orderable-replication-instances is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: OrderableReplicationInstances",
      "command_synopsis": "Synopsis\n  describe-orderable-replication-instances\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nOrderableReplicationInstances -> (list)\n\nThe order-able replication instances available.\n\n(structure)\n\nIn response to the DescribeOrderableReplicationInstances operation, this object describes an available replication instance. This description includes the replication instance’s type, engine version, and allocated storage.\n\nEngineVersion -> (string)\n\nThe version of the replication engine.\n\nReplicationInstanceClass -> (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class. For example to specify the instance class dms.c4.large, set this parameter to \"dms.c4.large\" .\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\nStorageType -> (string)\n\nThe type of storage used by the replication instance.\n\nMinAllocatedStorage -> (integer)\n\nThe minimum amount of storage (in gigabytes) that can be allocated for the replication instance.\n\nMaxAllocatedStorage -> (integer)\n\nThe minimum amount of storage (in gigabytes) that can be allocated for the replication instance.\n\nDefaultAllocatedStorage -> (integer)\n\nThe default amount of storage (in gigabytes) that is allocated for the replication instance.\n\nIncludedAllocatedStorage -> (integer)\n\nThe amount of storage (in gigabytes) that is allocated for the replication instance.\n\nAvailabilityZones -> (list)\n\nList of Availability Zones for this replication instance.\n\n(string)\n\nReleaseStatus -> (string)\n\nThe value returned when the specified EngineVersion of the replication instance is in Beta or test mode. This indicates some features might not work as expected.\n\nNote\n\nDMS supports the ReleaseStatus parameter in versions 3.1.4 and later.\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .",
      "command_examples": "Examples\n\nTo describe orderable replication instances\n\nThe following describe-orderable-replication-instances example lists replication instance types that you can order.\n\naws dms describe-orderable-replication-instances\n\n\nOutput:\n\n{\n    \"OrderableReplicationInstances\": [\n        {\n            \"EngineVersion\": \"3.3.2\",\n            \"ReplicationInstanceClass\": \"dms.c4.2xlarge\",\n            \"StorageType\": \"gp2\",\n            \"MinAllocatedStorage\": 5,\n            \"MaxAllocatedStorage\": 6144,\n            \"DefaultAllocatedStorage\": 100,\n            \"IncludedAllocatedStorage\": 100,\n            \"AvailabilityZones\": [\n                \"us-east-1a\",\n                \"us-east-1b\",\n                \"us-east-1c\",\n                \"us-east-1d\",\n                \"us-east-1e\",\n                \"us-east-1f\"\n            ]\n        },\n        {\n            \"EngineVersion\": \"3.3.2\",\n            \"ReplicationInstanceClass\": \"dms.c4.4xlarge\",\n            \"StorageType\": \"gp2\",\n            \"MinAllocatedStorage\": 5,\n            \"MaxAllocatedStorage\": 6144,\n            \"DefaultAllocatedStorage\": 100,\n            \"IncludedAllocatedStorage\": 100,\n            \"AvailabilityZones\": [\n                \"us-east-1a\",\n                \"us-east-1b\",\n                \"us-east-1c\",\n                \"us-east-1d\",\n                \"us-east-1e\",\n                \"us-east-1f\"\n            ]\n        },\n\n        ...remaining output omitted...\n\n    }\n\n\nFor more information, see Working with an AWS DMS Replication Instance in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-pending-maintenance-actions",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-pending-maintenance-actions.html",
      "command_description": "Description\n\nFor internal use only\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-pending-maintenance-actions\n[--replication-instance-arn <value>]\n[--filters <value>]\n[--marker <value>]\n[--max-records <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--replication-instance-arn <value>]",
        "[--filters <value>]",
        "[--marker <value>]",
        "[--max-records <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-instance-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\n--filters (list)\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--marker (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\n--max-records (integer)\n\nThe maximum number of records to include in the response. If more records exist than the specified MaxRecords value, a pagination token called a marker is included in the response so that the remaining results can be retrieved.\n\nDefault: 100\n\nConstraints: Minimum 20, maximum 100.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nPendingMaintenanceActions -> (list)\n\nThe pending maintenance action.\n\n(structure)\n\nIdentifies an DMS resource and any pending actions for it.\n\nResourceIdentifier -> (string)\n\nThe Amazon Resource Name (ARN) of the DMS resource that the pending maintenance action applies to. For information about creating an ARN, see Constructing an Amazon Resource Name (ARN) for DMS in the DMS documentation.\n\nPendingMaintenanceActionDetails -> (list)\n\nDetailed information about the pending maintenance action.\n\n(structure)\n\nDescribes a maintenance action pending for an DMS resource, including when and how it will be applied. This data type is a response element to the DescribePendingMaintenanceActions operation.\n\nAction -> (string)\n\nThe type of pending maintenance action that is available for the resource.\n\nAutoAppliedAfterDate -> (timestamp)\n\nThe date of the maintenance window when the action is to be applied. The maintenance action is applied to the resource during its first maintenance window after this date. If this date is specified, any next-maintenance opt-in requests are ignored.\n\nForcedApplyDate -> (timestamp)\n\nThe date when the maintenance action will be automatically applied. The maintenance action is applied to the resource on this date regardless of the maintenance window for the resource. If this date is specified, any immediate opt-in requests are ignored.\n\nOptInStatus -> (string)\n\nThe type of opt-in request that has been received for the resource.\n\nCurrentApplyDate -> (timestamp)\n\nThe effective date when the pending maintenance action will be applied to the resource. This date takes into account opt-in requests received from the ApplyPendingMaintenanceAction API operation, and also the AutoAppliedAfterDate and ForcedApplyDate parameter values. This value is blank if an opt-in request has not been received and nothing has been specified for AutoAppliedAfterDate or ForcedApplyDate .\n\nDescription -> (string)\n\nA description providing more detail about the maintenance action.\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords ."
    },
    {
      "command_name": "describe-refresh-schemas-status",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-refresh-schemas-status.html",
      "command_description": "Description\n\nReturns the status of the RefreshSchemas operation.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-refresh-schemas-status\n--endpoint-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--endpoint-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--endpoint-arn (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nRefreshSchemasStatus -> (structure)\n\nThe status of the schema.\n\nEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\nReplicationInstanceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\nStatus -> (string)\n\nThe status of the schema.\n\nLastRefreshDate -> (timestamp)\n\nThe date the schema was last refreshed.\n\nLastFailureMessage -> (string)\n\nThe last failure message for the schema.",
      "command_examples": "Examples\n\nTo list the refresh status for an endpoint\n\nThe following describe-refresh-schemas-status example returns the status of a previous refresh request.\n\naws dms describe-refresh-schemas-status \\\n    --endpoint-arn arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\n\n\nOutput:\n\n{\n    \"RefreshSchemasStatus\": {\n        \"EndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\",\n        \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n        \"Status\": \"successful\",\n        \"LastRefreshDate\": 1590786544.605\n    }\n}\n"
    },
    {
      "command_name": "describe-replication-instance-task-logs",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-replication-instance-task-logs.html",
      "command_description": "Description\n\nReturns information about the task logs for the specified task.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-replication-instance-task-logs\n--replication-instance-arn <value>\n[--max-records <value>]\n[--marker <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-instance-arn <value>",
        "[--max-records <value>]",
        "[--marker <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-instance-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\n--max-records (integer)\n\nThe maximum number of records to include in the response. If more records exist than the specified MaxRecords value, a pagination token called a marker is included in the response so that the remaining results can be retrieved.\n\nDefault: 100\n\nConstraints: Minimum 20, maximum 100.\n\n--marker (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationInstanceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\nReplicationInstanceTaskLogs -> (list)\n\nAn array of replication task log metadata. Each member of the array contains the replication task name, ARN, and task log size (in bytes).\n\n(structure)\n\nContains metadata for a replication instance task log.\n\nReplicationTaskName -> (string)\n\nThe name of the replication task.\n\nReplicationTaskArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\nReplicationInstanceTaskLogSize -> (long)\n\nThe size, in bytes, of the replication task log.\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords ."
    },
    {
      "command_name": "describe-replication-instances",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-replication-instances.html",
      "command_description": "Description\n\nReturns information about replication instances for your account in the current region.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-replication-instances is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: ReplicationInstances",
      "command_synopsis": "Synopsis\n  describe-replication-instances\n[--filters <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--filters <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--filters (list)\n\nFilters applied to replication instances.\n\nValid filter names: replication-instance-arn | replication-instance-id | replication-instance-class | engine-version\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\nReplicationInstances -> (list)\n\nThe replication instances described.\n\n(structure)\n\nProvides information that defines a replication instance.\n\nReplicationInstanceIdentifier -> (string)\n\nThe replication instance identifier is a required parameter. This parameter is stored as a lowercase string.\n\nConstraints:\n\nMust contain 1-63 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nExample: myrepinstance\n\nReplicationInstanceClass -> (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class. It is a required parameter, although a default value is pre-selected in the DMS console.\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\nReplicationInstanceStatus -> (string)\n\nThe status of the replication instance. The possible return values include:\n\n\"available\"\n\n\"creating\"\n\n\"deleted\"\n\n\"deleting\"\n\n\"failed\"\n\n\"modifying\"\n\n\"upgrading\"\n\n\"rebooting\"\n\n\"resetting-master-credentials\"\n\n\"storage-full\"\n\n\"incompatible-credentials\"\n\n\"incompatible-network\"\n\n\"maintenance\"\n\nAllocatedStorage -> (integer)\n\nThe amount of storage (in gigabytes) that is allocated for the replication instance.\n\nInstanceCreateTime -> (timestamp)\n\nThe time the replication instance was created.\n\nVpcSecurityGroups -> (list)\n\nThe VPC security group for the instance.\n\n(structure)\n\nDescribes the status of a security group associated with the virtual private cloud (VPC) hosting your replication and DB instances.\n\nVpcSecurityGroupId -> (string)\n\nThe VPC security group ID.\n\nStatus -> (string)\n\nThe status of the VPC security group.\n\nAvailabilityZone -> (string)\n\nThe Availability Zone for the instance.\n\nReplicationSubnetGroup -> (structure)\n\nThe subnet group for the replication instance.\n\nReplicationSubnetGroupIdentifier -> (string)\n\nThe identifier of the replication instance subnet group.\n\nReplicationSubnetGroupDescription -> (string)\n\nA description for the replication subnet group.\n\nVpcId -> (string)\n\nThe ID of the VPC.\n\nSubnetGroupStatus -> (string)\n\nThe status of the subnet group.\n\nSubnets -> (list)\n\nThe subnets that are in the subnet group.\n\n(structure)\n\nIn response to a request by the DescribeReplicationSubnetGroups operation, this object identifies a subnet by its given Availability Zone, subnet identifier, and status.\n\nSubnetIdentifier -> (string)\n\nThe subnet identifier.\n\nSubnetAvailabilityZone -> (structure)\n\nThe Availability Zone of the subnet.\n\nName -> (string)\n\nThe name of the Availability Zone.\n\nSubnetStatus -> (string)\n\nThe status of the subnet.\n\nPreferredMaintenanceWindow -> (string)\n\nThe maintenance window times for the replication instance. Any pending upgrades to the replication instance are performed during this time.\n\nPendingModifiedValues -> (structure)\n\nThe pending modification values.\n\nReplicationInstanceClass -> (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class.\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\nAllocatedStorage -> (integer)\n\nThe amount of storage (in gigabytes) that is allocated for the replication instance.\n\nMultiAZ -> (boolean)\n\nSpecifies whether the replication instance is a Multi-AZ deployment. You can’t set the AvailabilityZone parameter if the Multi-AZ parameter is set to true .\n\nEngineVersion -> (string)\n\nThe engine version number of the replication instance.\n\nMultiAZ -> (boolean)\n\nSpecifies whether the replication instance is a Multi-AZ deployment. You can’t set the AvailabilityZone parameter if the Multi-AZ parameter is set to true .\n\nEngineVersion -> (string)\n\nThe engine version number of the replication instance.\n\nIf an engine version number is not specified when a replication instance is created, the default is the latest engine version available.\n\nWhen modifying a major engine version of an instance, also set AllowMajorVersionUpgrade to true .\n\nAutoMinorVersionUpgrade -> (boolean)\n\nBoolean value indicating if minor version upgrades will be automatically applied to the instance.\n\nKmsKeyId -> (string)\n\nAn KMS key identifier that is used to encrypt the data on the replication instance.\n\nIf you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key.\n\nKMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nReplicationInstanceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\nReplicationInstancePublicIpAddress -> (string)\n\nThe public IP address of the replication instance.\n\nReplicationInstancePrivateIpAddress -> (string)\n\nThe private IP address of the replication instance.\n\nReplicationInstancePublicIpAddresses -> (list)\n\nOne or more public IP addresses for the replication instance.\n\n(string)\n\nReplicationInstancePrivateIpAddresses -> (list)\n\nOne or more private IP addresses for the replication instance.\n\n(string)\n\nPubliclyAccessible -> (boolean)\n\nSpecifies the accessibility options for the replication instance. A value of true represents an instance with a public IP address. A value of false represents an instance with a private IP address. The default value is true .\n\nSecondaryAvailabilityZone -> (string)\n\nThe Availability Zone of the standby replication instance in a Multi-AZ deployment.\n\nFreeUntil -> (timestamp)\n\nThe expiration date of the free replication instance that is part of the Free DMS program.\n\nDnsNameServers -> (string)\n\nThe DNS name servers supported for the replication instance to access your on-premise source or target database.",
      "command_examples": "Examples\n\nTo describe replication instances\n\nThe following describe-replication-instances example lists the replication instances in your AWS account.\n\naws dms describe-replication-instances\n\n\nOutput:\n\n{\n    \"ReplicationInstances\": [\n        {\n            \"ReplicationInstanceIdentifier\": \"my-repl-instance\",\n            \"ReplicationInstanceClass\": \"dms.t2.micro\",\n            \"ReplicationInstanceStatus\": \"available\",\n            \"AllocatedStorage\": 5,\n            \"InstanceCreateTime\": 1590011235.952,\n            \"VpcSecurityGroups\": [\n                {\n                    \"VpcSecurityGroupId\": \"sg-f839b688\",\n                    \"Status\": \"active\"\n                }\n            ],\n            \"AvailabilityZone\": \"us-east-1e\",\n            \"ReplicationSubnetGroup\": {\n                \"ReplicationSubnetGroupIdentifier\": \"default\",\n                \"ReplicationSubnetGroupDescription\": \"default\",\n                \"VpcId\": \"vpc-136a4c6a\",\n                \"SubnetGroupStatus\": \"Complete\",\n                \"Subnets\": [\n                    {\n                        \"SubnetIdentifier\": \"subnet-da327bf6\",\n                        \"SubnetAvailabilityZone\": {\n                            \"Name\": \"us-east-1a\"\n                        },\n                        \"SubnetStatus\": \"Active\"\n                    },\n                    {\n                        \"SubnetIdentifier\": \"subnet-42599426\",\n                        \"SubnetAvailabilityZone\": {\n                            \"Name\": \"us-east-1d\"\n                        },\n                        \"SubnetStatus\": \"Active\"\n                    },\n                    {\n                        \"SubnetIdentifier\": \"subnet-bac383e0\",\n                        \"SubnetAvailabilityZone\": {\n                            \"Name\": \"us-east-1c\"\n                        },\n                        \"SubnetStatus\": \"Active\"\n                    },\n                    {\n                        \"SubnetIdentifier\": \"subnet-6746046b\",\n                        \"SubnetAvailabilityZone\": {\n                            \"Name\": \"us-east-1f\"\n                        },\n                        \"SubnetStatus\": \"Active\"\n                    },\n                    {\n                        \"SubnetIdentifier\": \"subnet-d7c825e8\",\n                        \"SubnetAvailabilityZone\": {\n                            \"Name\": \"us-east-1e\"\n                        },\n                        \"SubnetStatus\": \"Active\"\n                    },\n                    {\n                        \"SubnetIdentifier\": \"subnet-cbfff283\",\n                        \"SubnetAvailabilityZone\": {\n                            \"Name\": \"us-east-1b\"\n                        },\n                        \"SubnetStatus\": \"Active\"\n                    }\n                ]\n            },\n            \"PreferredMaintenanceWindow\": \"wed:11:42-wed:12:12\",\n            \"PendingModifiedValues\": {\n                \"MultiAZ\": true\n            },\n            \"MultiAZ\": false,\n            \"EngineVersion\": \"3.3.2\",\n            \"AutoMinorVersionUpgrade\": true,\n            \"KmsKeyId\": \"arn:aws:kms:us-east-1:123456789012:key/f7bc0f8e-1a3a-4ace-9faa-e8494fa3921a\",\n            \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n            \"ReplicationInstancePublicIpAddress\": \"3.230.18.248\",\n            \"ReplicationInstancePrivateIpAddress\": \"172.31.75.90\",\n            \"ReplicationInstancePublicIpAddresses\": [\n                \"3.230.18.248\"\n            ],\n            \"ReplicationInstancePrivateIpAddresses\": [\n                \"172.31.75.90\"\n            ],\n            \"PubliclyAccessible\": true,\n            \"FreeUntil\": 1590194829.267\n        }\n    ]\n}\n\n\nFor more information, see Working with an AWS DMS Replication Instance in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-replication-subnet-groups",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-replication-subnet-groups.html",
      "command_description": "Description\n\nReturns information about the replication subnet groups.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-replication-subnet-groups is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: ReplicationSubnetGroups",
      "command_synopsis": "Synopsis\n  describe-replication-subnet-groups\n[--filters <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--filters <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--filters (list)\n\nFilters applied to replication subnet groups.\n\nValid filter names: replication-subnet-group-id\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\nReplicationSubnetGroups -> (list)\n\nA description of the replication subnet groups.\n\n(structure)\n\nDescribes a subnet group in response to a request by the DescribeReplicationSubnetGroups operation.\n\nReplicationSubnetGroupIdentifier -> (string)\n\nThe identifier of the replication instance subnet group.\n\nReplicationSubnetGroupDescription -> (string)\n\nA description for the replication subnet group.\n\nVpcId -> (string)\n\nThe ID of the VPC.\n\nSubnetGroupStatus -> (string)\n\nThe status of the subnet group.\n\nSubnets -> (list)\n\nThe subnets that are in the subnet group.\n\n(structure)\n\nIn response to a request by the DescribeReplicationSubnetGroups operation, this object identifies a subnet by its given Availability Zone, subnet identifier, and status.\n\nSubnetIdentifier -> (string)\n\nThe subnet identifier.\n\nSubnetAvailabilityZone -> (structure)\n\nThe Availability Zone of the subnet.\n\nName -> (string)\n\nThe name of the Availability Zone.\n\nSubnetStatus -> (string)\n\nThe status of the subnet.",
      "command_examples": "Examples\n\nTo display the available subnet groups\n\nThe following describe-replication-subnet-groups example lists the available subnet groups.\n\naws dms describe-replication-subnet-groups \\\n    --filter \"Name=replication-subnet-group-id,Values=my-subnet-group\"\n\n\nOutput:\n\n{\n    \"ReplicationSubnetGroups\": [\n        {\n            \"ReplicationSubnetGroupIdentifier\": \"my-subnet-group\",\n            \"ReplicationSubnetGroupDescription\": \"my subnet group\",\n            \"VpcId\": \"vpc-136a4c6a\",\n            \"SubnetGroupStatus\": \"Complete\",\n            \"Subnets\": [\n                {\n                    \"SubnetIdentifier\": \"subnet-da327bf6\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1a\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                },\n                {\n                    \"SubnetIdentifier\": \"subnet-bac383e0\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1c\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                },\n                {\n                    \"SubnetIdentifier\": \"subnet-d7c825e8\",\n                    \"SubnetAvailabilityZone\": {\n                        \"Name\": \"us-east-1e\"\n                    },\n                    \"SubnetStatus\": \"Active\"\n                }\n            ]\n        }\n    ]\n}\n\n\nFor more information, see Setting Up a Network for a Replication Instance in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-replication-task-assessment-results",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-replication-task-assessment-results.html",
      "command_description": "Description\n\nReturns the task assessment results from the Amazon S3 bucket that DMS creates in your Amazon Web Services account. This action always returns the latest results.\n\nFor more information about DMS task assessments, see Creating a task assessment report in the Database Migration Service User Guide .\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-replication-task-assessment-results is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: ReplicationTaskAssessmentResults",
      "command_synopsis": "Synopsis\n  describe-replication-task-assessment-results\n[--replication-task-arn <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--replication-task-arn <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-arn (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the task. When this input parameter is specified, the API returns only one result and ignore the values of the MaxRecords and Marker parameters.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\nBucketName -> (string)\n\nThe Amazon S3 bucket where the task assessment report is located.\n\nReplicationTaskAssessmentResults -> (list)\n\nThe task assessment report.\n\n(structure)\n\nThe task assessment report in JSON format.\n\nReplicationTaskIdentifier -> (string)\n\nThe replication task identifier of the task on which the task assessment was run.\n\nReplicationTaskArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\nReplicationTaskLastAssessmentDate -> (timestamp)\n\nThe date the task assessment was completed.\n\nAssessmentStatus -> (string)\n\nThe status of the task assessment.\n\nAssessmentResultsFile -> (string)\n\nThe file containing the results of the task assessment.\n\nAssessmentResults -> (string)\n\nThe task assessment results in JSON format.\n\nThe response object only contains this field if you provide DescribeReplicationTaskAssessmentResultsMessage$ReplicationTaskArn in the request.\n\nS3ObjectUrl -> (string)\n\nThe URL of the S3 object containing the task assessment results.\n\nThe response object only contains this field if you provide DescribeReplicationTaskAssessmentResultsMessage$ReplicationTaskArn in the request.",
      "command_examples": "Examples\n\nTo list the results of replication task assessmentss\n\nThe following describe-replication-task-assessment-results example lists the results of a prior task assesssment.\n\naws dms describe-replication-task-assessment-results\n\n\nOutput:\n\n{\n    \"ReplicationTaskAssessmentResults\": [\n        {\n            \"ReplicationTaskIdentifier\": \"moveit2\",\n            \"ReplicationTaskArn\": \"arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\",\n            \"ReplicationTaskLastAssessmentDate\": 1590790230.0,\n            \"AssessmentStatus\": \"No issues found\",\n            \"AssessmentResultsFile\": \"moveit2/2020-05-29-22-10\"\n        }\n    ]\n}\n\n\nFor more information, see Creating a Task Assessment Report in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-replication-task-assessment-runs",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-replication-task-assessment-runs.html",
      "command_description": "Description\n\nReturns a paginated list of premigration assessment runs based on filter settings.\n\nThese filter settings can specify a combination of premigration assessment runs, migration tasks, replication instances, and assessment run status values.\n\nNote\n\nThis operation doesn’t return information about individual assessments. For this information, see the DescribeReplicationTaskIndividualAssessments operation.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-replication-task-assessment-runs\n[--filters <value>]\n[--max-records <value>]\n[--marker <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--filters <value>]",
        "[--max-records <value>]",
        "[--marker <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--filters (list)\n\nFilters applied to the premigration assessment runs described in the form of key-value pairs.\n\nValid filter names: replication-task-assessment-run-arn , replication-task-arn , replication-instance-arn , status\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--max-records (integer)\n\nThe maximum number of records to include in the response. If more records exist than the specified MaxRecords value, a pagination token called a marker is included in the response so that the remaining results can be retrieved.\n\n--marker (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nA pagination token returned for you to pass to a subsequent request. If you pass this token as the Marker value in a subsequent request, the response includes only records beyond the marker, up to the value specified in the request by MaxRecords .\n\nReplicationTaskAssessmentRuns -> (list)\n\nOne or more premigration assessment runs as specified by Filters .\n\n(structure)\n\nProvides information that describes a premigration assessment run that you have started using the StartReplicationTaskAssessmentRun operation.\n\nSome of the information appears based on other operations that can return the ReplicationTaskAssessmentRun object.\n\nReplicationTaskAssessmentRunArn -> (string)\n\nAmazon Resource Name (ARN) of this assessment run.\n\nReplicationTaskArn -> (string)\n\nARN of the migration task associated with this premigration assessment run.\n\nStatus -> (string)\n\nAssessment run status.\n\nThis status can have one of the following values:\n\n\"cancelling\" – The assessment run was canceled by the CancelReplicationTaskAssessmentRun operation.\n\n\"deleting\" – The assessment run was deleted by the DeleteReplicationTaskAssessmentRun operation.\n\n\"failed\" – At least one individual assessment completed with a failed status.\n\n\"error-provisioning\" – An internal error occurred while resources were provisioned (during provisioning status).\n\n\"error-executing\" – An internal error occurred while individual assessments ran (during running status).\n\n\"invalid state\" – The assessment run is in an unknown state.\n\n\"passed\" – All individual assessments have completed, and none has a failed status.\n\n\"provisioning\" – Resources required to run individual assessments are being provisioned.\n\n\"running\" – Individual assessments are being run.\n\n\"starting\" – The assessment run is starting, but resources are not yet being provisioned for individual assessments.\n\nReplicationTaskAssessmentRunCreationDate -> (timestamp)\n\nDate on which the assessment run was created using the StartReplicationTaskAssessmentRun operation.\n\nAssessmentProgress -> (structure)\n\nIndication of the completion progress for the individual assessments specified to run.\n\nIndividualAssessmentCount -> (integer)\n\nThe number of individual assessments that are specified to run.\n\nIndividualAssessmentCompletedCount -> (integer)\n\nThe number of individual assessments that have completed, successfully or not.\n\nLastFailureMessage -> (string)\n\nLast message generated by an individual assessment failure.\n\nServiceAccessRoleArn -> (string)\n\nARN of the service role used to start the assessment run using the StartReplicationTaskAssessmentRun operation. The role must allow the iam:PassRole action.\n\nResultLocationBucket -> (string)\n\nAmazon S3 bucket where DMS stores the results of this assessment run.\n\nResultLocationFolder -> (string)\n\nFolder in an Amazon S3 bucket where DMS stores the results of this assessment run.\n\nResultEncryptionMode -> (string)\n\nEncryption mode used to encrypt the assessment run results.\n\nResultKmsKeyArn -> (string)\n\nARN of the KMS encryption key used to encrypt the assessment run results.\n\nAssessmentRunName -> (string)\n\nUnique name of the assessment run."
    },
    {
      "command_name": "describe-replication-task-individual-assessments",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-replication-task-individual-assessments.html",
      "command_description": "Description\n\nReturns a paginated list of individual assessments based on filter settings.\n\nThese filter settings can specify a combination of premigration assessment runs, migration tasks, and assessment status values.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  describe-replication-task-individual-assessments\n[--filters <value>]\n[--max-records <value>]\n[--marker <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--filters <value>]",
        "[--max-records <value>]",
        "[--marker <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--filters (list)\n\nFilters applied to the individual assessments described in the form of key-value pairs.\n\nValid filter names: replication-task-assessment-run-arn , replication-task-arn , status\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--max-records (integer)\n\nThe maximum number of records to include in the response. If more records exist than the specified MaxRecords value, a pagination token called a marker is included in the response so that the remaining results can be retrieved.\n\n--marker (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nA pagination token returned for you to pass to a subsequent request. If you pass this token as the Marker value in a subsequent request, the response includes only records beyond the marker, up to the value specified in the request by MaxRecords .\n\nReplicationTaskIndividualAssessments -> (list)\n\nOne or more individual assessments as specified by Filters .\n\n(structure)\n\nProvides information that describes an individual assessment from a premigration assessment run.\n\nReplicationTaskIndividualAssessmentArn -> (string)\n\nAmazon Resource Name (ARN) of this individual assessment.\n\nReplicationTaskAssessmentRunArn -> (string)\n\nARN of the premigration assessment run that is created to run this individual assessment.\n\nIndividualAssessmentName -> (string)\n\nName of this individual assessment.\n\nStatus -> (string)\n\nIndividual assessment status.\n\nThis status can have one of the following values:\n\n\"cancelled\"\n\n\"error\"\n\n\"failed\"\n\n\"passed\"\n\n\"pending\"\n\n\"running\"\n\nReplicationTaskIndividualAssessmentStartDate -> (timestamp)\n\nDate when this individual assessment was started as part of running the StartReplicationTaskAssessmentRun operation."
    },
    {
      "command_name": "describe-replication-tasks",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-replication-tasks.html",
      "command_description": "Description\n\nReturns information about replication tasks for your account in the current region.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-replication-tasks is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: ReplicationTasks",
      "command_synopsis": "Synopsis\n  describe-replication-tasks\n[--filters <value>]\n[--without-settings | --no-without-settings]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--filters <value>]",
        "[--without-settings | --no-without-settings]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--filters (list)\n\nFilters applied to replication tasks.\n\nValid filter names: replication-task-arn | replication-task-id | migration-type | endpoint-arn | replication-instance-arn\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--without-settings | --no-without-settings (boolean)\n\nAn option to set to avoid returning information about settings. Use this to reduce overhead when setting information is too large. To use this option, choose true ; otherwise, choose false (the default).\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\nReplicationTasks -> (list)\n\nA description of the replication tasks.\n\n(structure)\n\nProvides information that describes a replication task created by the CreateReplicationTask operation.\n\nReplicationTaskIdentifier -> (string)\n\nThe user-assigned replication task identifier or name.\n\nConstraints:\n\nMust contain 1-255 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nSourceEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) that uniquely identifies the endpoint.\n\nTargetEndpointArn -> (string)\n\nThe ARN that uniquely identifies the endpoint.\n\nReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance.\n\nMigrationType -> (string)\n\nThe type of migration.\n\nTableMappings -> (string)\n\nTable mappings specified in the task.\n\nReplicationTaskSettings -> (string)\n\nThe settings for the replication task.\n\nStatus -> (string)\n\nThe status of the replication task. This response parameter can return one of the following values:\n\n\"moving\" – The task is being moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"creating\" – The task is being created in response to running the ` CreateReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_CreateReplicationTask.html`__ operation.\n\n\"deleting\" – The task is being deleted in response to running the ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ operation.\n\n\"failed\" – The task failed to successfully complete the database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"failed-move\" – The task failed to move in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"modifying\" – The task definition is being modified in response to running the ` ModifyReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_ModifyReplicationTask.html`__ operation.\n\n\"ready\" – The task is in a ready state where it can respond to other task operations, such as ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ or ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ .\n\n\"running\" – The task is performing a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"starting\" – The task is preparing to perform a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"stopped\" – The task has stopped in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"stopping\" – The task is preparing to stop in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"testing\" – The database migration specified for this task is being tested in response to running either the ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ or the ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation.\n\nNote\n\n` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ is an improved premigration task assessment operation. The ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation assesses data type compatibility only between the source and target database of a given migration task. In contrast, ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ enables you to specify a variety of premigration task assessments in addition to data type compatibility. These assessments include ones for the validity of primary key definitions and likely issues with database migration performance, among others.\n\nLastFailureMessage -> (string)\n\nThe last error (failure) message generated for the replication task.\n\nStopReason -> (string)\n\nThe reason the replication task was stopped. This response parameter can return one of the following values:\n\n\"STOP_REASON_FULL_LOAD_COMPLETED\" – Full-load migration completed.\n\n\"STOP_REASON_CACHED_CHANGES_APPLIED\" – Change data capture (CDC) load completed.\n\n\"STOP_REASON_CACHED_CHANGES_NOT_APPLIED\" – In a full-load and CDC migration, the full load stopped as specified before starting the CDC migration.\n\n\"STOP_REASON_SERVER_TIME\" – The migration stopped at the specified server time.\n\nReplicationTaskCreationDate -> (timestamp)\n\nThe date the replication task was created.\n\nReplicationTaskStartDate -> (timestamp)\n\nThe date the replication task is scheduled to start.\n\nCdcStartPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to start. Use either CdcStartPosition or CdcStartTime to specify when you want the CDC operation to start. Specifying both values results in an error.\n\nThe value can be in date, checkpoint, or LSN/SCN format.\n\nDate Example: –cdc-start-position “2018-03-08T12:12:12”\n\nCheckpoint Example: –cdc-start-position “checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93”\n\nLSN Example: –cdc-start-position “mysql-bin-changelog.000024:373”\n\nCdcStopPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time.\n\nServer time example: –cdc-stop-position “server_time:2018-02-09T12:12:12”\n\nCommit time example: –cdc-stop-position “commit_time: 2018-02-09T12:12:12 “\n\nRecoveryCheckpoint -> (string)\n\nIndicates the last checkpoint that occurred during a change data capture (CDC) operation. You can provide this value to the CdcStartPosition parameter to start a CDC operation that begins at that checkpoint.\n\nReplicationTaskArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\nReplicationTaskStats -> (structure)\n\nThe statistics for the task, including elapsed time, tables loaded, and table errors.\n\nFullLoadProgressPercent -> (integer)\n\nThe percent complete for the full load migration task.\n\nElapsedTimeMillis -> (long)\n\nThe elapsed time of the task, in milliseconds.\n\nTablesLoaded -> (integer)\n\nThe number of tables loaded for this task.\n\nTablesLoading -> (integer)\n\nThe number of tables currently loading for this task.\n\nTablesQueued -> (integer)\n\nThe number of tables queued for this task.\n\nTablesErrored -> (integer)\n\nThe number of errors that have occurred during this task.\n\nFreshStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a target reload.\n\nStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a resume. For more information, see StartReplicationTaskType .\n\nStopDate -> (timestamp)\n\nThe date the replication task was stopped.\n\nFullLoadStartDate -> (timestamp)\n\nThe date the replication task full load was started.\n\nFullLoadFinishDate -> (timestamp)\n\nThe date the replication task full load was completed.\n\nTaskData -> (string)\n\nSupplemental information that the task requires to migrate the data for certain source and target endpoints. For more information, see Specifying Supplemental Data for Task Settings in the Database Migration Service User Guide.\n\nTargetReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance to which this task is moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation. Otherwise, this response parameter isn’t a member of the ReplicationTask object.",
      "command_examples": "Examples\n\nTo create a replication task\n\nThe following describe-replication-tasks example describes current replication tasks.\n\naws dms describe-replication-tasks\n\n\nOutput:\n\n{\n    \"ReplicationTasks\": [\n        {\n            \"ReplicationTaskIdentifier\": \"moveit2\",\n            \"SourceEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\",\n            \"TargetEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:EOM4SFKCZEYHZBFGAGZT3QEC5U\",\n            \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n            \"MigrationType\": \"full-load\",\n            \"TableMappings\": ...output omitted... ,\n            \"ReplicationTaskSettings\": ...output omitted... ,\n            \"Status\": \"stopped\",\n            \"StopReason\": \"Stop Reason FULL_LOAD_ONLY_FINISHED\",\n            \"ReplicationTaskCreationDate\": 1590524772.505,\n            \"ReplicationTaskStartDate\": 1590619805.212,\n            \"ReplicationTaskArn\": \"arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\",\n            \"ReplicationTaskStats\": {\n                \"FullLoadProgressPercent\": 100,\n                \"ElapsedTimeMillis\": 0,\n                \"TablesLoaded\": 0,\n                \"TablesLoading\": 0,\n                \"TablesQueued\": 0,\n                \"TablesErrored\": 0,\n                \"FreshStartDate\": 1590619811.528,\n                \"StartDate\": 1590619811.528,\n                \"StopDate\": 1590619842.068\n            }\n        }\n    ]\n}\n\n\nFor more information, see Working with AWS DMS Tasks in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-schemas",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-schemas.html",
      "command_description": "Description\n\nReturns information about the schema for the specified endpoint.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-schemas is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: Schemas",
      "command_synopsis": "Synopsis\n  describe-schemas\n--endpoint-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--endpoint-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--endpoint-arn (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords .\n\nSchemas -> (list)\n\nThe described schema.\n\n(string)",
      "command_examples": "Examples\n\nTo desscribe database schemas\n\nThe following describe-schemas example lists the available tables at an endpoint.\n\naws dms describe-schemas \\\n    --endpoint-arn \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\"\n\n\nOutput:\n\n{\n    \"Schemas\": [\n        \"prodrep\"\n    ]\n}\n\n\nFor more information, see This is the topic title in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "describe-table-statistics",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/describe-table-statistics.html",
      "command_description": "Description\n\nReturns table statistics on the database migration task, including table name, rows inserted, rows updated, and rows deleted.\n\nNote that the “last updated” column the DMS console only indicates the time that DMS last updated the table statistics record for a table. It does not indicate the time of the last update to the table.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.\n\ndescribe-table-statistics is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: TableStatistics",
      "command_synopsis": "Synopsis\n  describe-table-statistics\n--replication-task-arn <value>\n[--filters <value>]\n[--cli-input-json | --cli-input-yaml]\n[--starting-token <value>]\n[--page-size <value>]\n[--max-items <value>]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-task-arn <value>",
        "[--filters <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--starting-token <value>]",
        "[--page-size <value>]",
        "[--max-items <value>]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\n--filters (list)\n\nFilters applied to table statistics.\n\nValid filter names: schema-name | table-name | table-state\n\nA combination of filters creates an AND condition where each record matches all specified filters.\n\n(structure)\n\nIdentifies the name and value of a filter object. This filter is used to limit the number and type of DMS objects that are returned for a particular Describe* call or similar operation. Filters are used as an optional parameter for certain API operations.\n\nName -> (string)\n\nThe name of the filter as specified for a Describe* or similar operation.\n\nValues -> (list)\n\nThe filter value, which can specify one or more values used to narrow the returned results.\n\n(string)\n\nShorthand Syntax:\n\nName=string,Values=string,string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Name\": \"string\",\n    \"Values\": [\"string\", ...]\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the command’s output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the command’s output. If the total number of items available is more than the value specified, a NextToken is provided in the command’s output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationTaskArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\nTableStatistics -> (list)\n\nThe table statistics.\n\n(structure)\n\nProvides a collection of table statistics in response to a request by the DescribeTableStatistics operation.\n\nSchemaName -> (string)\n\nThe schema name.\n\nTableName -> (string)\n\nThe name of the table.\n\nInserts -> (long)\n\nThe number of insert actions performed on a table.\n\nDeletes -> (long)\n\nThe number of delete actions performed on a table.\n\nUpdates -> (long)\n\nThe number of update actions performed on a table.\n\nDdls -> (long)\n\nThe data definition language (DDL) used to build and modify the structure of your tables.\n\nFullLoadRows -> (long)\n\nThe number of rows added during the full load operation.\n\nFullLoadCondtnlChkFailedRows -> (long)\n\nThe number of rows that failed conditional checks during the full load operation (valid only for migrations where DynamoDB is the target).\n\nFullLoadErrorRows -> (long)\n\nThe number of rows that failed to load during the full load operation (valid only for migrations where DynamoDB is the target).\n\nFullLoadStartTime -> (timestamp)\n\nThe time when the full load operation started.\n\nFullLoadEndTime -> (timestamp)\n\nThe time when the full load operation completed.\n\nFullLoadReloaded -> (boolean)\n\nA value that indicates if the table was reloaded (true ) or loaded as part of a new full load operation (false ).\n\nLastUpdateTime -> (timestamp)\n\nThe last time a table was updated.\n\nTableState -> (string)\n\nThe state of the tables described.\n\nValid states: Table does not exist | Before load | Full load | Table completed | Table cancelled | Table error | Table all | Table updates | Table is being reloaded\n\nValidationPendingRecords -> (long)\n\nThe number of records that have yet to be validated.\n\nValidationFailedRecords -> (long)\n\nThe number of records that failed validation.\n\nValidationSuspendedRecords -> (long)\n\nThe number of records that couldn’t be validated.\n\nValidationState -> (string)\n\nThe validation state of the table.\n\nThis parameter can have the following values:\n\nNot enabled – Validation isn’t enabled for the table in the migration task.\n\nPending records – Some records in the table are waiting for validation.\n\nMismatched records – Some records in the table don’t match between the source and target.\n\nSuspended records – Some records in the table couldn’t be validated.\n\nNo primary key –The table couldn’t be validated because it has no primary key.\n\nTable error – The table wasn’t validated because it’s in an error state and some data wasn’t migrated.\n\nValidated – All rows in the table are validated. If the table is updated, the status can change from Validated.\n\nError – The table couldn’t be validated because of an unexpected error.\n\nPending validation – The table is waiting validation.\n\nPreparing table – Preparing the table enabled in the migration task for validation.\n\nPending revalidation – All rows in the table are pending validation after the table was updated.\n\nValidationStateDetails -> (string)\n\nAdditional details about the state of validation.\n\nMarker -> (string)\n\nAn optional pagination token provided by a previous request. If this parameter is specified, the response includes only records beyond the marker, up to the value specified by MaxRecords ."
    },
    {
      "command_name": "import-certificate",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/import-certificate.html",
      "command_description": "Description\n\nUploads the specified certificate.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  import-certificate\n--certificate-identifier <value>\n[--certificate-pem <value>]\n[--certificate-wallet <value>]\n[--tags <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--certificate-identifier <value>",
        "[--certificate-pem <value>]",
        "[--certificate-wallet <value>]",
        "[--tags <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--certificate-identifier (string)\n\nA customer-assigned name for the certificate. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen or contain two consecutive hyphens.\n\n--certificate-pem (string)\n\nThe contents of a .pem file, which contains an X.509 certificate.\n\n--certificate-wallet (blob)\n\nThe location of an imported Oracle Wallet certificate for use with SSL. Provide the name of a .sso file using the fileb:// prefix. You can’t provide the certificate inline.\n\n--tags (list)\n\nThe tags associated with the certificate.\n\n(structure)\n\nA user-defined key-value pair that describes metadata added to an DMS resource and that is used by operations such as the following:\n\nAddTagsToResource\n\nListTagsForResource\n\nRemoveTagsFromResource\n\nKey -> (string)\n\nA key is the required name of the tag. The string value can be 1-128 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nValue -> (string)\n\nA value is the optional value of the tag. The string value can be 1-256 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the resource for which the tag is created.\n\nShorthand Syntax:\n\nKey=string,Value=string,ResourceArn=string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"Key\": \"string\",\n    \"Value\": \"string\",\n    \"ResourceArn\": \"string\"\n  }\n  ...\n]\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nCertificate -> (structure)\n\nThe certificate to be uploaded.\n\nCertificateIdentifier -> (string)\n\nA customer-assigned name for the certificate. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen or contain two consecutive hyphens.\n\nCertificateCreationDate -> (timestamp)\n\nThe date that the certificate was created.\n\nCertificatePem -> (string)\n\nThe contents of a .pem file, which contains an X.509 certificate.\n\nCertificateWallet -> (blob)\n\nThe location of an imported Oracle Wallet certificate for use with SSL.\n\nCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the certificate.\n\nCertificateOwner -> (string)\n\nThe owner of the certificate.\n\nValidFromDate -> (timestamp)\n\nThe beginning date that the certificate is valid.\n\nValidToDate -> (timestamp)\n\nThe final date that the certificate is valid.\n\nSigningAlgorithm -> (string)\n\nThe signing algorithm for the certificate.\n\nKeyLength -> (integer)\n\nThe key length of the cryptographic algorithm being used."
    },
    {
      "command_name": "list-tags-for-resource",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/list-tags-for-resource.html",
      "command_description": "Description\n\nLists all metadata tags attached to an DMS resource, including replication instance, endpoint, security group, and migration task. For more information, see ` Tag https://docs.aws.amazon.com/dms/latest/APIReference/API_Tag.html`__ data type description.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  list-tags-for-resource\n[--resource-arn <value>]\n[--resource-arn-list <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "[--resource-arn <value>]",
        "[--resource-arn-list <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--resource-arn (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the DMS resource to list tags for. This returns a list of keys (names of tags) created for the resource and their associated tag values.\n\n--resource-arn-list (list)\n\nList of ARNs that identify multiple DMS resources that you want to list tags for. This returns a list of keys (tag names) and their associated tag values. It also returns each tag’s associated ResourceArn value, which is the ARN of the resource for which each listed tag is created.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nTagList -> (list)\n\nA list of tags for the resource.\n\n(structure)\n\nA user-defined key-value pair that describes metadata added to an DMS resource and that is used by operations such as the following:\n\nAddTagsToResource\n\nListTagsForResource\n\nRemoveTagsFromResource\n\nKey -> (string)\n\nA key is the required name of the tag. The string value can be 1-128 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nValue -> (string)\n\nA value is the optional value of the tag. The string value can be 1-256 Unicode characters in length and can’t be prefixed with “aws:” or “dms:”. The string can only contain only the set of Unicode letters, digits, white-space, ‘_’, ‘.’, ‘/’, ‘=’, ‘+’, ‘-‘ (Java regular expressions: “^([\\p{L}\\p{Z}\\p{N}_.:/=+-]*)$”).\n\nResourceArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the resource for which the tag is created.",
      "command_examples": "Examples\n\nTo list the tags for a resource\n\nThe following list-tags-for-resource example lists the tags for a replication instance.\n\naws dms list-tags-for-resource \\\n    --resource-arn arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\n\n\nOutput:\n\n{\n    \"TagList\": [\n        {\n            \"Key\": \"Project\",\n            \"Value\": \"dbMigration\"\n        },\n        {\n            \"Key\": \"Environment\",\n            \"Value\": \"PROD\"\n        }\n    ]\n}\n\n\nFor more information, see Tagging Resources in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "modify-endpoint",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/modify-endpoint.html",
      "command_description": "Description\n\nModifies the specified endpoint.\n\nNote\n\nFor a MySQL source or target endpoint, don’t explicitly specify the database using the DatabaseName request parameter on the ModifyEndpoint API call. Specifying DatabaseName when you modify a MySQL endpoint replicates all the task tables to this single database. For MySQL endpoints, you specify the database only when you specify the schema in the table-mapping rules of the DMS task.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  modify-endpoint\n--endpoint-arn <value>\n[--endpoint-identifier <value>]\n[--endpoint-type <value>]\n[--engine-name <value>]\n[--username <value>]\n[--password <value>]\n[--server-name <value>]\n[--port <value>]\n[--database-name <value>]\n[--extra-connection-attributes <value>]\n[--certificate-arn <value>]\n[--ssl-mode <value>]\n[--service-access-role-arn <value>]\n[--external-table-definition <value>]\n[--dynamo-db-settings <value>]\n[--s3-settings <value>]\n[--dms-transfer-settings <value>]\n[--mongo-db-settings <value>]\n[--kinesis-settings <value>]\n[--kafka-settings <value>]\n[--elasticsearch-settings <value>]\n[--neptune-settings <value>]\n[--redshift-settings <value>]\n[--postgre-sql-settings <value>]\n[--my-sql-settings <value>]\n[--oracle-settings <value>]\n[--sybase-settings <value>]\n[--microsoft-sql-server-settings <value>]\n[--ibm-db2-settings <value>]\n[--doc-db-settings <value>]\n[--redis-settings <value>]\n[--exact-settings | --no-exact-settings]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--endpoint-arn <value>",
        "[--endpoint-identifier <value>]",
        "[--endpoint-type <value>]",
        "[--engine-name <value>]",
        "[--username <value>]",
        "[--password <value>]",
        "[--server-name <value>]",
        "[--port <value>]",
        "[--database-name <value>]",
        "[--extra-connection-attributes <value>]",
        "[--certificate-arn <value>]",
        "[--ssl-mode <value>]",
        "[--service-access-role-arn <value>]",
        "[--external-table-definition <value>]",
        "[--dynamo-db-settings <value>]",
        "[--s3-settings <value>]",
        "[--dms-transfer-settings <value>]",
        "[--mongo-db-settings <value>]",
        "[--kinesis-settings <value>]",
        "[--kafka-settings <value>]",
        "[--elasticsearch-settings <value>]",
        "[--neptune-settings <value>]",
        "[--redshift-settings <value>]",
        "[--postgre-sql-settings <value>]",
        "[--my-sql-settings <value>]",
        "[--oracle-settings <value>]",
        "[--sybase-settings <value>]",
        "[--microsoft-sql-server-settings <value>]",
        "[--ibm-db2-settings <value>]",
        "[--doc-db-settings <value>]",
        "[--redis-settings <value>]",
        "[--exact-settings | --no-exact-settings]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--endpoint-arn (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\n--endpoint-identifier (string)\n\nThe database endpoint identifier. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen or contain two consecutive hyphens.\n\n--endpoint-type (string)\n\nThe type of endpoint. Valid values are source and target .\n\nPossible values:\n\nsource\n\ntarget\n\n--engine-name (string)\n\nThe type of engine for the endpoint. Valid values, depending on the EndpointType, include \"mysql\" , \"oracle\" , \"postgres\" , \"mariadb\" , \"aurora\" , \"aurora-postgresql\" , \"redshift\" , \"s3\" , \"db2\" , \"azuredb\" , \"sybase\" , \"dynamodb\" , \"mongodb\" , \"kinesis\" , \"kafka\" , \"elasticsearch\" , \"documentdb\" , \"sqlserver\" , and \"neptune\" .\n\n--username (string)\n\nThe user name to be used to login to the endpoint database.\n\n--password (string)\n\nThe password to be used to login to the endpoint database.\n\n--server-name (string)\n\nThe name of the server where the endpoint database resides.\n\n--port (integer)\n\nThe port used by the endpoint database.\n\n--database-name (string)\n\nThe name of the endpoint database. For a MySQL source or target endpoint, do not specify DatabaseName.\n\n--extra-connection-attributes (string)\n\nAdditional attributes associated with the connection. To reset this parameter, pass the empty string (“”) as an argument.\n\n--certificate-arn (string)\n\nThe Amazon Resource Name (ARN) of the certificate used for SSL connection.\n\n--ssl-mode (string)\n\nThe SSL mode used to connect to the endpoint. The default value is none .\n\nPossible values:\n\nnone\n\nrequire\n\nverify-ca\n\nverify-full\n\n--service-access-role-arn (string)\n\nThe Amazon Resource Name (ARN) for the IAM role you want to use to modify the endpoint. The role must allow the iam:PassRole action.\n\n--external-table-definition (string)\n\nThe external table definition.\n\n--dynamo-db-settings (structure)\n\nSettings in JSON format for the target Amazon DynamoDB endpoint. For information about other available settings, see Using Object Mapping to Migrate Data to DynamoDB in the Database Migration Service User Guide.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nShorthand Syntax:\n\nServiceAccessRoleArn=string\n\n\nJSON Syntax:\n\n{\n  \"ServiceAccessRoleArn\": \"string\"\n}\n\n\n--s3-settings (structure)\n\nSettings in JSON format for the target Amazon S3 endpoint. For more information about the available settings, see Extra Connection Attributes When Using Amazon S3 as a Target for DMS in the Database Migration Service User Guide.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action. It is a required parameter that enables DMS to write and read objects from an S3 bucket.\n\nExternalTableDefinition -> (string)\n\nSpecifies how tables are defined in the S3 source files only.\n\nCsvRowDelimiter -> (string)\n\nThe delimiter used to separate rows in the .csv file for both source and target. The default is a carriage return (\\n ).\n\nCsvDelimiter -> (string)\n\nThe delimiter used to separate columns in the .csv file for both source and target. The default is a comma.\n\nBucketFolder -> (string)\n\nAn optional parameter to set a folder name in the S3 bucket. If provided, tables are created in the path `` bucketFolder /schema_name /table_name /`` . If this parameter isn’t specified, then the path used is `` schema_name /table_name /`` .\n\nBucketName -> (string)\n\nThe name of the S3 bucket.\n\nCompressionType -> (string)\n\nAn optional parameter to use GZIP to compress the target files. Set to GZIP to compress the target files. Either set this parameter to NONE (the default) or don’t use it to leave the files uncompressed. This parameter applies to both .csv and .parquet file formats.\n\nEncryptionMode -> (string)\n\nThe type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either SSE_S3 (the default) or SSE_KMS .\n\nNote\n\nFor the ModifyEndpoint operation, you can change the existing value of the EncryptionMode parameter from SSE_KMS to SSE_S3 . But you can’t change the existing value from SSE_S3 to SSE_KMS .\n\nTo use SSE_S3 , you need an Identity and Access Management (IAM) role with permission to allow \"arn:aws:s3:::dms-*\" to use the following actions:\n\ns3:CreateBucket\n\ns3:ListBucket\n\ns3:DeleteBucket\n\ns3:GetBucketLocation\n\ns3:GetObject\n\ns3:PutObject\n\ns3:DeleteObject\n\ns3:GetObjectVersion\n\ns3:GetBucketPolicy\n\ns3:PutBucketPolicy\n\ns3:DeleteBucketPolicy\n\nServerSideEncryptionKmsKeyId -> (string)\n\nIf you are using SSE_KMS for the EncryptionMode , provide the KMS key ID. The key that you use needs an attached policy that enables Identity and Access Management (IAM) user permissions and allows use of the key.\n\nHere is a CLI example: ``aws dms create-endpoint –endpoint-identifier value –endpoint-type target –engine-name s3 –s3-settings ServiceAccessRoleArn=*value* ,BucketFolder=*value* ,BucketName=*value* ,EncryptionMode=SSE_KMS,ServerSideEncryptionKmsKeyId=*value* ``\n\nDataFormat -> (string)\n\nThe format of the data that you want to use for output. You can choose one of the following:\n\ncsv : This is a row-based file format with comma-separated values (.csv).\n\nparquet : Apache Parquet (.parquet) is a columnar storage file format that features efficient compression and provides faster query response.\n\nEncodingType -> (string)\n\nThe type of encoding you are using:\n\nRLE_DICTIONARY uses a combination of bit-packing and run-length encoding to store repeated values more efficiently. This is the default.\n\nPLAIN doesn’t use encoding at all. Values are stored as they are.\n\nPLAIN_DICTIONARY builds a dictionary of the values encountered in a given column. The dictionary is stored in a dictionary page for each column chunk.\n\nDictPageSizeLimit -> (integer)\n\nThe maximum size of an encoded dictionary page of a column. If the dictionary page exceeds this, this column is stored using an encoding type of PLAIN . This parameter defaults to 1024 * 1024 bytes (1 MiB), the maximum size of a dictionary page before it reverts to PLAIN encoding. This size is used for .parquet file format only.\n\nRowGroupLength -> (integer)\n\nThe number of rows in a row group. A smaller row group size provides faster reads. But as the number of row groups grows, the slower writes become. This parameter defaults to 10,000 rows. This number is used for .parquet file format only.\n\nIf you choose a value larger than the maximum, RowGroupLength is set to the max row group length in bytes (64 * 1024 * 1024).\n\nDataPageSize -> (integer)\n\nThe size of one data page in bytes. This parameter defaults to 1024 * 1024 bytes (1 MiB). This number is used for .parquet file format only.\n\nParquetVersion -> (string)\n\nThe version of the Apache Parquet format that you want to use: parquet_1_0 (the default) or parquet_2_0 .\n\nEnableStatistics -> (boolean)\n\nA value that enables statistics for Parquet pages and row groups. Choose true to enable statistics, false to disable. Statistics include NULL , DISTINCT , MAX , and MIN values. This parameter defaults to true . This value is used for .parquet file format only.\n\nIncludeOpForFullLoad -> (boolean)\n\nA value that enables a full load to write INSERT operations to the comma-separated value (.csv) output files only to indicate how the rows were added to the source database.\n\nNote\n\nDMS supports the IncludeOpForFullLoad parameter in versions 3.1.4 and later.\n\nFor full load, records can only be inserted. By default (the false setting), no information is recorded in these output files for a full load to indicate that the rows were inserted at the source database. If IncludeOpForFullLoad is set to true or y , the INSERT is recorded as an I annotation in the first field of the .csv file. This allows the format of your target records from a full load to be consistent with the target records from a CDC load.\n\nNote\n\nThis setting works together with the CdcInsertsOnly and the CdcInsertsAndUpdates parameters for output to .csv files only. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nCdcInsertsOnly -> (boolean)\n\nA value that enables a change data capture (CDC) load to write only INSERT operations to .csv or columnar storage (.parquet) output files. By default (the false setting), the first field in a .csv or .parquet record contains the letter I (INSERT), U (UPDATE), or D (DELETE). These values indicate whether the row was inserted, updated, or deleted at the source database for a CDC load to the target.\n\nIf CdcInsertsOnly is set to true or y , only INSERTs from the source database are migrated to the .csv or .parquet file. For .csv format only, how these INSERTs are recorded depends on the value of IncludeOpForFullLoad . If IncludeOpForFullLoad is set to true , the first field of every CDC record is set to I to indicate the INSERT operation at the source. If IncludeOpForFullLoad is set to false , every CDC record is written without a first field to indicate the INSERT operation at the source. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nNote\n\nDMS supports the interaction described preceding between the CdcInsertsOnly and IncludeOpForFullLoad parameters in versions 3.1.4 and later.\n\nCdcInsertsOnly and CdcInsertsAndUpdates can’t both be set to true for the same endpoint. Set either CdcInsertsOnly or CdcInsertsAndUpdates to true for the same endpoint, but not both.\n\nTimestampColumnName -> (string)\n\nA value that when nonblank causes DMS to add a column with timestamp information to the endpoint data for an Amazon S3 target.\n\nNote\n\nDMS supports the TimestampColumnName parameter in versions 3.1.4 and later.\n\nDMS includes an additional STRING column in the .csv or .parquet object files of your migrated data when you set TimestampColumnName to a nonblank value.\n\nFor a full load, each row of this timestamp column contains a timestamp for when the data was transferred from the source to the target by DMS.\n\nFor a change data capture (CDC) load, each row of the timestamp column contains the timestamp for the commit of that row in the source database.\n\nThe string format for this timestamp column value is yyyy-MM-dd HH:mm:ss.SSSSSS . By default, the precision of this value is in microseconds. For a CDC load, the rounding of the precision depends on the commit timestamp supported by DMS for the source database.\n\nWhen the AddColumnName parameter is set to true , DMS also includes a name for the timestamp column that you set with TimestampColumnName .\n\nParquetTimestampInMillisecond -> (boolean)\n\nA value that specifies the precision of any TIMESTAMP column values that are written to an Amazon S3 object file in .parquet format.\n\nNote\n\nDMS supports the ParquetTimestampInMillisecond parameter in versions 3.1.4 and later.\n\nWhen ParquetTimestampInMillisecond is set to true or y , DMS writes all TIMESTAMP columns in a .parquet formatted file with millisecond precision. Otherwise, DMS writes them with microsecond precision.\n\nCurrently, Amazon Athena and Glue can handle only millisecond precision for TIMESTAMP values. Set this parameter to true for S3 endpoint object files that are .parquet formatted only if you plan to query or process the data with Athena or Glue.\n\nNote\n\nDMS writes any TIMESTAMP column values written to an S3 file in .csv format with microsecond precision.\n\nSetting ParquetTimestampInMillisecond has no effect on the string format of the timestamp column value that is inserted by setting the TimestampColumnName parameter.\n\nCdcInsertsAndUpdates -> (boolean)\n\nA value that enables a change data capture (CDC) load to write INSERT and UPDATE operations to .csv or .parquet (columnar storage) output files. The default setting is false , but when CdcInsertsAndUpdates is set to true or y , only INSERTs and UPDATEs from the source database are migrated to the .csv or .parquet file.\n\nFor .csv file format only, how these INSERTs and UPDATEs are recorded depends on the value of the IncludeOpForFullLoad parameter. If IncludeOpForFullLoad is set to true , the first field of every CDC record is set to either I or U to indicate INSERT and UPDATE operations at the source. But if IncludeOpForFullLoad is set to false , CDC records are written without an indication of INSERT or UPDATE operations at the source. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nNote\n\nDMS supports the use of the CdcInsertsAndUpdates parameter in versions 3.3.1 and later.\n\nCdcInsertsOnly and CdcInsertsAndUpdates can’t both be set to true for the same endpoint. Set either CdcInsertsOnly or CdcInsertsAndUpdates to true for the same endpoint, but not both.\n\nDatePartitionEnabled -> (boolean)\n\nWhen set to true , this parameter partitions S3 bucket folders based on transaction commit dates. The default value is false . For more information about date-based folder partitioning, see Using date-based folder partitioning .\n\nDatePartitionSequence -> (string)\n\nIdentifies the sequence of the date format to use during folder partitioning. The default value is YYYYMMDD . Use this parameter when DatePartitionedEnabled is set to true .\n\nDatePartitionDelimiter -> (string)\n\nSpecifies a date separating delimiter to use during folder partitioning. The default value is SLASH . Use this parameter when DatePartitionedEnabled is set to true .\n\nUseCsvNoSupValue -> (boolean)\n\nThis setting applies if the S3 output files during a change data capture (CDC) load are written in .csv format. If set to true for columns not included in the supplemental log, DMS uses the value specified by ` CsvNoSupValue https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CsvNoSupValue`__ . If not set or set to false , DMS uses the null value for these columns.\n\nNote\n\nThis setting is supported in DMS versions 3.4.1 and later.\n\nCsvNoSupValue -> (string)\n\nThis setting only applies if your Amazon S3 output files during a change data capture (CDC) load are written in .csv format. If ` UseCsvNoSupValue https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-UseCsvNoSupValue`__ is set to true, specify a string value that you want DMS to use for all columns not included in the supplemental log. If you do not specify a string value, DMS uses the null value for these columns regardless of the UseCsvNoSupValue setting.\n\nNote\n\nThis setting is supported in DMS versions 3.4.1 and later.\n\nPreserveTransactions -> (boolean)\n\nIf set to true , DMS saves the transaction order for a change data capture (CDC) load on the Amazon S3 target specified by ` CdcPath https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CdcPath`__ . For more information, see Capturing data changes (CDC) including transaction order on the S3 target .\n\nNote\n\nThis setting is supported in DMS versions 3.4.2 and later.\n\nCdcPath -> (string)\n\nSpecifies the folder path of CDC files. For an S3 source, this setting is required if a task captures change data; otherwise, it’s optional. If CdcPath is set, DMS reads CDC files from this path and replicates the data changes to the target endpoint. For an S3 target if you set ` PreserveTransactions https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-PreserveTransactions`__ to true , DMS verifies that you have set this parameter to a folder path on your S3 target where DMS can save the transaction order for the CDC load. DMS creates this CDC folder path in either your S3 target working directory or the S3 target location specified by ` BucketFolder https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketFolder`__ and ` BucketName https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketName`__ .\n\nFor example, if you specify CdcPath as MyChangedData , and you specify BucketName as MyTargetBucket but do not specify BucketFolder , DMS creates the CDC folder path following: MyTargetBucket/MyChangedData .\n\nIf you specify the same CdcPath , and you specify BucketName as MyTargetBucket and BucketFolder as MyTargetData , DMS creates the CDC folder path following: MyTargetBucket/MyTargetData/MyChangedData .\n\nFor more information on CDC including transaction order on an S3 target, see Capturing data changes (CDC) including transaction order on the S3 target .\n\nNote\n\nThis setting is supported in DMS versions 3.4.2 and later.\n\nCannedAclForObjects -> (string)\n\nA value that enables DMS to specify a predefined (canned) access control list for objects created in an Amazon S3 bucket as .csv or .parquet files. For more information about Amazon S3 canned ACLs, see Canned ACL in the Amazon S3 Developer Guide.\n\nThe default value is NONE. Valid values include NONE, PRIVATE, PUBLIC_READ, PUBLIC_READ_WRITE, AUTHENTICATED_READ, AWS_EXEC_READ, BUCKET_OWNER_READ, and BUCKET_OWNER_FULL_CONTROL.\n\nAddColumnName -> (boolean)\n\nAn optional parameter that, when set to true or y , you can use to add column name information to the .csv output file.\n\nThe default value is false . Valid values are true , false , y , and n .\n\nCdcMaxBatchInterval -> (integer)\n\nMaximum length of the interval, defined in seconds, after which to output a file to Amazon S3.\n\nWhen CdcMaxBatchInterval and CdcMinFileSize are both specified, the file write is triggered by whichever parameter condition is met first within an DMS CloudFormation template.\n\nThe default value is 60 seconds.\n\nCdcMinFileSize -> (integer)\n\nMinimum file size, defined in megabytes, to reach for a file output to Amazon S3.\n\nWhen CdcMinFileSize and CdcMaxBatchInterval are both specified, the file write is triggered by whichever parameter condition is met first within an DMS CloudFormation template.\n\nThe default value is 32 MB.\n\nCsvNullValue -> (string)\n\nAn optional parameter that specifies how DMS treats null values. While handling the null value, you can use this parameter to pass a user-defined string as null when writing to the target. For example, when target columns are not nullable, you can use this option to differentiate between the empty string value and the null value. So, if you set this parameter value to the empty string (“” or ‘’), DMS treats the empty string as the null value instead of NULL .\n\nThe default value is NULL . Valid values include any valid string.\n\nIgnoreHeaderRows -> (integer)\n\nWhen this value is set to 1, DMS ignores the first row header in a .csv file. A value of 1 turns on the feature; a value of 0 turns off the feature.\n\nThe default is 0.\n\nMaxFileSize -> (integer)\n\nA value that specifies the maximum size (in KB) of any .csv file to be created while migrating to an S3 target during full load.\n\nThe default value is 1,048,576 KB (1 GB). Valid values include 1 to 1,048,576.\n\nRfc4180 -> (boolean)\n\nFor an S3 source, when this value is set to true or y , each leading double quotation mark has to be followed by an ending double quotation mark. This formatting complies with RFC 4180. When this value is set to false or n , string literals are copied to the target as is. In this case, a delimiter (row or column) signals the end of the field. Thus, you can’t use a delimiter as part of the string, because it signals the end of the value.\n\nFor an S3 target, an optional parameter used to set behavior to comply with RFC 4180 for data migrated to Amazon S3 using .csv file format only. When this value is set to true or y using Amazon S3 as a target, if the data has quotation marks or newline characters in it, DMS encloses the entire column with an additional pair of double quotation marks (“). Every quotation mark within the data is repeated twice.\n\nThe default value is true . Valid values include true , false , y , and n .\n\nShorthand Syntax:\n\nServiceAccessRoleArn=string,ExternalTableDefinition=string,CsvRowDelimiter=string,CsvDelimiter=string,BucketFolder=string,BucketName=string,CompressionType=string,EncryptionMode=string,ServerSideEncryptionKmsKeyId=string,DataFormat=string,EncodingType=string,DictPageSizeLimit=integer,RowGroupLength=integer,DataPageSize=integer,ParquetVersion=string,EnableStatistics=boolean,IncludeOpForFullLoad=boolean,CdcInsertsOnly=boolean,TimestampColumnName=string,ParquetTimestampInMillisecond=boolean,CdcInsertsAndUpdates=boolean,DatePartitionEnabled=boolean,DatePartitionSequence=string,DatePartitionDelimiter=string,UseCsvNoSupValue=boolean,CsvNoSupValue=string,PreserveTransactions=boolean,CdcPath=string,CannedAclForObjects=string,AddColumnName=boolean,CdcMaxBatchInterval=integer,CdcMinFileSize=integer,CsvNullValue=string,IgnoreHeaderRows=integer,MaxFileSize=integer,Rfc4180=boolean\n\n\nJSON Syntax:\n\n{\n  \"ServiceAccessRoleArn\": \"string\",\n  \"ExternalTableDefinition\": \"string\",\n  \"CsvRowDelimiter\": \"string\",\n  \"CsvDelimiter\": \"string\",\n  \"BucketFolder\": \"string\",\n  \"BucketName\": \"string\",\n  \"CompressionType\": \"none\"|\"gzip\",\n  \"EncryptionMode\": \"sse-s3\"|\"sse-kms\",\n  \"ServerSideEncryptionKmsKeyId\": \"string\",\n  \"DataFormat\": \"csv\"|\"parquet\",\n  \"EncodingType\": \"plain\"|\"plain-dictionary\"|\"rle-dictionary\",\n  \"DictPageSizeLimit\": integer,\n  \"RowGroupLength\": integer,\n  \"DataPageSize\": integer,\n  \"ParquetVersion\": \"parquet-1-0\"|\"parquet-2-0\",\n  \"EnableStatistics\": true|false,\n  \"IncludeOpForFullLoad\": true|false,\n  \"CdcInsertsOnly\": true|false,\n  \"TimestampColumnName\": \"string\",\n  \"ParquetTimestampInMillisecond\": true|false,\n  \"CdcInsertsAndUpdates\": true|false,\n  \"DatePartitionEnabled\": true|false,\n  \"DatePartitionSequence\": \"YYYYMMDD\"|\"YYYYMMDDHH\"|\"YYYYMM\"|\"MMYYYYDD\"|\"DDMMYYYY\",\n  \"DatePartitionDelimiter\": \"SLASH\"|\"UNDERSCORE\"|\"DASH\"|\"NONE\",\n  \"UseCsvNoSupValue\": true|false,\n  \"CsvNoSupValue\": \"string\",\n  \"PreserveTransactions\": true|false,\n  \"CdcPath\": \"string\",\n  \"CannedAclForObjects\": \"none\"|\"private\"|\"public-read\"|\"public-read-write\"|\"authenticated-read\"|\"aws-exec-read\"|\"bucket-owner-read\"|\"bucket-owner-full-control\",\n  \"AddColumnName\": true|false,\n  \"CdcMaxBatchInterval\": integer,\n  \"CdcMinFileSize\": integer,\n  \"CsvNullValue\": \"string\",\n  \"IgnoreHeaderRows\": integer,\n  \"MaxFileSize\": integer,\n  \"Rfc4180\": true|false\n}\n\n\n--dms-transfer-settings (structure)\n\nThe settings in JSON format for the DMS transfer type of source endpoint.\n\nAttributes include the following:\n\nserviceAccessRoleArn - The Amazon Resource Name (ARN) used by the service access IAM role. The role must allow the iam:PassRole action.\n\nBucketName - The name of the S3 bucket to use.\n\nShorthand syntax for these settings is as follows: ServiceAccessRoleArn=string ,BucketName=string\n\nJSON syntax for these settings is as follows: { \"ServiceAccessRoleArn\": \"string\", \"BucketName\": \"string\"}\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service access IAM role. The role must allow the iam:PassRole action.\n\nBucketName -> (string)\n\nThe name of the S3 bucket to use.\n\nShorthand Syntax:\n\nServiceAccessRoleArn=string,BucketName=string\n\n\nJSON Syntax:\n\n{\n  \"ServiceAccessRoleArn\": \"string\",\n  \"BucketName\": \"string\"\n}\n\n\n--mongo-db-settings (structure)\n\nSettings in JSON format for the source MongoDB endpoint. For more information about the available settings, see the configuration properties section in Endpoint configuration settings when using MongoDB as a source for Database Migration Service in the Database Migration Service User Guide.\n\nUsername -> (string)\n\nThe user name you use to access the MongoDB source endpoint.\n\nPassword -> (string)\n\nThe password for the user account you use to access the MongoDB source endpoint.\n\nServerName -> (string)\n\nThe name of the server on the MongoDB source endpoint.\n\nPort -> (integer)\n\nThe port value for the MongoDB source endpoint.\n\nDatabaseName -> (string)\n\nThe database name on the MongoDB source endpoint.\n\nAuthType -> (string)\n\nThe authentication type you use to access the MongoDB source endpoint.\n\nWhen when set to \"no\" , user name and password parameters are not used and can be empty.\n\nAuthMechanism -> (string)\n\nThe authentication mechanism you use to access the MongoDB source endpoint.\n\nFor the default value, in MongoDB version 2.x, \"default\" is \"mongodb_cr\" . For MongoDB version 3.x or later, \"default\" is \"scram_sha_1\" . This setting isn’t used when AuthType is set to \"no\" .\n\nNestingLevel -> (string)\n\nSpecifies either document or table mode.\n\nDefault value is \"none\" . Specify \"none\" to use document mode. Specify \"one\" to use table mode.\n\nExtractDocId -> (string)\n\nSpecifies the document ID. Use this setting when NestingLevel is set to \"none\" .\n\nDefault value is \"false\" .\n\nDocsToInvestigate -> (string)\n\nIndicates the number of documents to preview to determine the document organization. Use this setting when NestingLevel is set to \"one\" .\n\nMust be a positive value greater than 0 . Default value is 1000 .\n\nAuthSource -> (string)\n\nThe MongoDB database name. This setting isn’t used when AuthType is set to \"no\" .\n\nThe default is \"admin\" .\n\nKmsKeyId -> (string)\n\nThe KMS key identifier that is used to encrypt the content on the replication instance. If you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key. KMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the MongoDB endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the MongoDB endpoint connection details.\n\nShorthand Syntax:\n\nUsername=string,Password=string,ServerName=string,Port=integer,DatabaseName=string,AuthType=string,AuthMechanism=string,NestingLevel=string,ExtractDocId=string,DocsToInvestigate=string,AuthSource=string,KmsKeyId=string,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"Username\": \"string\",\n  \"Password\": \"string\",\n  \"ServerName\": \"string\",\n  \"Port\": integer,\n  \"DatabaseName\": \"string\",\n  \"AuthType\": \"no\"|\"password\",\n  \"AuthMechanism\": \"default\"|\"mongodb_cr\"|\"scram_sha_1\",\n  \"NestingLevel\": \"none\"|\"one\",\n  \"ExtractDocId\": \"string\",\n  \"DocsToInvestigate\": \"string\",\n  \"AuthSource\": \"string\",\n  \"KmsKeyId\": \"string\",\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--kinesis-settings (structure)\n\nSettings in JSON format for the target endpoint for Amazon Kinesis Data Streams. For more information about the available settings, see Using object mapping to migrate data to a Kinesis data stream in the Database Migration Service User Guide.\n\nStreamArn -> (string)\n\nThe Amazon Resource Name (ARN) for the Amazon Kinesis Data Streams endpoint.\n\nMessageFormat -> (string)\n\nThe output format for the records created on the endpoint. The message format is JSON (default) or JSON_UNFORMATTED (a single line with no tab).\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) for the IAM role that DMS uses to write to the Kinesis data stream. The role must allow the iam:PassRole action.\n\nIncludeTransactionDetails -> (boolean)\n\nProvides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for transaction_id , previous transaction_id , and transaction_record_id (the record offset within a transaction). The default is false .\n\nIncludePartitionValue -> (boolean)\n\nShows the partition value within the Kinesis message output, unless the partition type is schema-table-type . The default is false .\n\nPartitionIncludeSchemaTable -> (boolean)\n\nPrefixes schema and table names to partition values, when the partition type is primary-key-type . Doing this increases data distribution among Kinesis shards. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same shard, which causes throttling. The default is false .\n\nIncludeTableAlterOperations -> (boolean)\n\nIncludes any data definition language (DDL) operations that change the table in the control data, such as rename-table , drop-table , add-column , drop-column , and rename-column . The default is false .\n\nIncludeControlDetails -> (boolean)\n\nShows detailed control information for table definition, column definition, and table and column changes in the Kinesis message output. The default is false .\n\nIncludeNullAndEmpty -> (boolean)\n\nInclude NULL and empty columns for records migrated to the endpoint. The default is false .\n\nNoHexPrefix -> (boolean)\n\nSet this optional parameter to true to avoid adding a ‘0x’ prefix to raw data in hexadecimal format. For example, by default, DMS adds a ‘0x’ prefix to the LOB column type in hexadecimal format moving from an Oracle source to an Amazon Kinesis target. Use the NoHexPrefix endpoint setting to enable migration of RAW data type columns without adding the ‘0x’ prefix.\n\nShorthand Syntax:\n\nStreamArn=string,MessageFormat=string,ServiceAccessRoleArn=string,IncludeTransactionDetails=boolean,IncludePartitionValue=boolean,PartitionIncludeSchemaTable=boolean,IncludeTableAlterOperations=boolean,IncludeControlDetails=boolean,IncludeNullAndEmpty=boolean,NoHexPrefix=boolean\n\n\nJSON Syntax:\n\n{\n  \"StreamArn\": \"string\",\n  \"MessageFormat\": \"json\"|\"json-unformatted\",\n  \"ServiceAccessRoleArn\": \"string\",\n  \"IncludeTransactionDetails\": true|false,\n  \"IncludePartitionValue\": true|false,\n  \"PartitionIncludeSchemaTable\": true|false,\n  \"IncludeTableAlterOperations\": true|false,\n  \"IncludeControlDetails\": true|false,\n  \"IncludeNullAndEmpty\": true|false,\n  \"NoHexPrefix\": true|false\n}\n\n\n--kafka-settings (structure)\n\nSettings in JSON format for the target Apache Kafka endpoint. For more information about the available settings, see Using object mapping to migrate data to a Kafka topic in the Database Migration Service User Guide.\n\nBroker -> (string)\n\nA comma-separated list of one or more broker locations in your Kafka cluster that host your Kafka instance. Specify each broker location in the form `` broker-hostname-or-ip :port `` . For example, \"ec2-12-345-678-901.compute-1.amazonaws.com:2345\" . For more information and examples of specifying a list of broker locations, see Using Apache Kafka as a target for Database Migration Service in the Database Migration Service User Guide .\n\nTopic -> (string)\n\nThe topic to which you migrate the data. If you don’t specify a topic, DMS specifies \"kafka-default-topic\" as the migration topic.\n\nMessageFormat -> (string)\n\nThe output format for the records created on the endpoint. The message format is JSON (default) or JSON_UNFORMATTED (a single line with no tab).\n\nIncludeTransactionDetails -> (boolean)\n\nProvides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for transaction_id , previous transaction_id , and transaction_record_id (the record offset within a transaction). The default is false .\n\nIncludePartitionValue -> (boolean)\n\nShows the partition value within the Kafka message output unless the partition type is schema-table-type . The default is false .\n\nPartitionIncludeSchemaTable -> (boolean)\n\nPrefixes schema and table names to partition values, when the partition type is primary-key-type . Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. The default is false .\n\nIncludeTableAlterOperations -> (boolean)\n\nIncludes any data definition language (DDL) operations that change the table in the control data, such as rename-table , drop-table , add-column , drop-column , and rename-column . The default is false .\n\nIncludeControlDetails -> (boolean)\n\nShows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. The default is false .\n\nMessageMaxBytes -> (integer)\n\nThe maximum size in bytes for records created on the endpoint The default is 1,000,000.\n\nIncludeNullAndEmpty -> (boolean)\n\nInclude NULL and empty columns for records migrated to the endpoint. The default is false .\n\nSecurityProtocol -> (string)\n\nSet secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include ssl-encryption , ssl-authentication , and sasl-ssl . sasl-ssl requires SaslUsername and SaslPassword .\n\nSslClientCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) of the client certificate used to securely connect to a Kafka target endpoint.\n\nSslClientKeyArn -> (string)\n\nThe Amazon Resource Name (ARN) for the client private key used to securely connect to a Kafka target endpoint.\n\nSslClientKeyPassword -> (string)\n\nThe password for the client private key used to securely connect to a Kafka target endpoint.\n\nSslCaCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the private certificate authority (CA) cert that DMS uses to securely connect to your Kafka target endpoint.\n\nSaslUsername -> (string)\n\nThe secure user name you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n\nSaslPassword -> (string)\n\nThe secure password you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n\nNoHexPrefix -> (boolean)\n\nSet this optional parameter to true to avoid adding a ‘0x’ prefix to raw data in hexadecimal format. For example, by default, DMS adds a ‘0x’ prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the NoHexPrefix endpoint setting to enable migration of RAW data type columns without adding the ‘0x’ prefix.\n\nShorthand Syntax:\n\nBroker=string,Topic=string,MessageFormat=string,IncludeTransactionDetails=boolean,IncludePartitionValue=boolean,PartitionIncludeSchemaTable=boolean,IncludeTableAlterOperations=boolean,IncludeControlDetails=boolean,MessageMaxBytes=integer,IncludeNullAndEmpty=boolean,SecurityProtocol=string,SslClientCertificateArn=string,SslClientKeyArn=string,SslClientKeyPassword=string,SslCaCertificateArn=string,SaslUsername=string,SaslPassword=string,NoHexPrefix=boolean\n\n\nJSON Syntax:\n\n{\n  \"Broker\": \"string\",\n  \"Topic\": \"string\",\n  \"MessageFormat\": \"json\"|\"json-unformatted\",\n  \"IncludeTransactionDetails\": true|false,\n  \"IncludePartitionValue\": true|false,\n  \"PartitionIncludeSchemaTable\": true|false,\n  \"IncludeTableAlterOperations\": true|false,\n  \"IncludeControlDetails\": true|false,\n  \"MessageMaxBytes\": integer,\n  \"IncludeNullAndEmpty\": true|false,\n  \"SecurityProtocol\": \"plaintext\"|\"ssl-authentication\"|\"ssl-encryption\"|\"sasl-ssl\",\n  \"SslClientCertificateArn\": \"string\",\n  \"SslClientKeyArn\": \"string\",\n  \"SslClientKeyPassword\": \"string\",\n  \"SslCaCertificateArn\": \"string\",\n  \"SaslUsername\": \"string\",\n  \"SaslPassword\": \"string\",\n  \"NoHexPrefix\": true|false\n}\n\n\n--elasticsearch-settings (structure)\n\nSettings in JSON format for the target Elasticsearch endpoint. For more information about the available settings, see Extra Connection Attributes When Using Elasticsearch as a Target for DMS in the Database Migration Service User Guide.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nEndpointUri -> (string)\n\nThe endpoint for the Elasticsearch cluster. DMS uses HTTPS if a transport protocol (http/https) is not specified.\n\nFullLoadErrorPercentage -> (integer)\n\nThe maximum percentage of records that can fail to be written before a full load operation stops.\n\nTo avoid early failure, this counter is only effective after 1000 records are transferred. Elasticsearch also has the concept of error monitoring during the last 10 minutes of an Observation Window. If transfer of all records fail in the last 10 minutes, the full load operation stops.\n\nErrorRetryDuration -> (integer)\n\nThe maximum number of seconds for which DMS retries failed API requests to the Elasticsearch cluster.\n\nShorthand Syntax:\n\nServiceAccessRoleArn=string,EndpointUri=string,FullLoadErrorPercentage=integer,ErrorRetryDuration=integer\n\n\nJSON Syntax:\n\n{\n  \"ServiceAccessRoleArn\": \"string\",\n  \"EndpointUri\": \"string\",\n  \"FullLoadErrorPercentage\": integer,\n  \"ErrorRetryDuration\": integer\n}\n\n\n--neptune-settings (structure)\n\nSettings in JSON format for the target Amazon Neptune endpoint. For more information about the available settings, see Specifying graph-mapping rules using Gremlin and R2RML for Amazon Neptune as a target in the Database Migration Service User Guide.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the service role that you created for the Neptune target endpoint. The role must allow the iam:PassRole action. For more information, see Creating an IAM Service Role for Accessing Amazon Neptune as a Target in the Database Migration Service User Guide.\n\nS3BucketName -> (string)\n\nThe name of the Amazon S3 bucket where DMS can temporarily store migrated graph data in .csv files before bulk-loading it to the Neptune target database. DMS maps the SQL source data to graph data before storing it in these .csv files.\n\nS3BucketFolder -> (string)\n\nA folder path where you want DMS to store migrated graph data in the S3 bucket specified by S3BucketName\n\nErrorRetryDuration -> (integer)\n\nThe number of milliseconds for DMS to wait to retry a bulk-load of migrated graph data to the Neptune target database before raising an error. The default is 250.\n\nMaxFileSize -> (integer)\n\nThe maximum size in kilobytes of migrated graph data stored in a .csv file before DMS bulk-loads the data to the Neptune target database. The default is 1,048,576 KB. If the bulk load is successful, DMS clears the bucket, ready to store the next batch of migrated graph data.\n\nMaxRetryCount -> (integer)\n\nThe number of times for DMS to retry a bulk load of migrated graph data to the Neptune target database before raising an error. The default is 5.\n\nIamAuthEnabled -> (boolean)\n\nIf you want Identity and Access Management (IAM) authorization enabled for this endpoint, set this parameter to true . Then attach the appropriate IAM policy document to your service role specified by ServiceAccessRoleArn . The default is false .\n\nShorthand Syntax:\n\nServiceAccessRoleArn=string,S3BucketName=string,S3BucketFolder=string,ErrorRetryDuration=integer,MaxFileSize=integer,MaxRetryCount=integer,IamAuthEnabled=boolean\n\n\nJSON Syntax:\n\n{\n  \"ServiceAccessRoleArn\": \"string\",\n  \"S3BucketName\": \"string\",\n  \"S3BucketFolder\": \"string\",\n  \"ErrorRetryDuration\": integer,\n  \"MaxFileSize\": integer,\n  \"MaxRetryCount\": integer,\n  \"IamAuthEnabled\": true|false\n}\n\n\n--redshift-settings (structure)\n\nProvides information that defines an Amazon Redshift endpoint.\n\nAcceptAnyDate -> (boolean)\n\nA value that indicates to allow any date format, including invalid formats such as 00/00/00 00:00:00, to be loaded without generating an error. You can choose true or false (the default).\n\nThis parameter applies only to TIMESTAMP and DATE columns. Always use ACCEPTANYDATE with the DATEFORMAT parameter. If the date format for the data doesn’t match the DATEFORMAT specification, Amazon Redshift inserts a NULL value into that field.\n\nAfterConnectScript -> (string)\n\nCode to run after connecting. This parameter should contain the code itself, not the name of a file containing the code.\n\nBucketFolder -> (string)\n\nAn S3 folder where the comma-separated-value (.csv) files are stored before being uploaded to the target Redshift cluster.\n\nFor full load mode, DMS converts source records into .csv files and loads them to the BucketFolder/TableID path. DMS uses the Redshift COPY command to upload the .csv files to the target table. The files are deleted once the COPY operation has finished. For more information, see COPY in the Amazon Redshift Database Developer Guide .\n\nFor change-data-capture (CDC) mode, DMS creates a NetChanges table, and loads the .csv files to this BucketFolder/NetChangesTableID path.\n\nBucketName -> (string)\n\nThe name of the intermediate S3 bucket used to store .csv files before uploading data to Redshift.\n\nCaseSensitiveNames -> (boolean)\n\nIf Amazon Redshift is configured to support case sensitive schema names, set CaseSensitiveNames to true . The default is false .\n\nCompUpdate -> (boolean)\n\nIf you set CompUpdate to true Amazon Redshift applies automatic compression if the table is empty. This applies even if the table columns already have encodings other than RAW . If you set CompUpdate to false , automatic compression is disabled and existing column encodings aren’t changed. The default is true .\n\nConnectionTimeout -> (integer)\n\nA value that sets the amount of time to wait (in milliseconds) before timing out, beginning from when you initially establish a connection.\n\nDatabaseName -> (string)\n\nThe name of the Amazon Redshift data warehouse (service) that you are working with.\n\nDateFormat -> (string)\n\nThe date format that you are using. Valid values are auto (case-sensitive), your date format string enclosed in quotes, or NULL. If this parameter is left unset (NULL), it defaults to a format of ‘YYYY-MM-DD’. Using auto recognizes most strings, even some that aren’t supported when you use a date format string.\n\nIf your date and time values use formats different from each other, set this to auto .\n\nEmptyAsNull -> (boolean)\n\nA value that specifies whether DMS should migrate empty CHAR and VARCHAR fields as NULL. A value of true sets empty CHAR and VARCHAR fields to null. The default is false .\n\nEncryptionMode -> (string)\n\nThe type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either SSE_S3 (the default) or SSE_KMS .\n\nNote\n\nFor the ModifyEndpoint operation, you can change the existing value of the EncryptionMode parameter from SSE_KMS to SSE_S3 . But you can’t change the existing value from SSE_S3 to SSE_KMS .\n\nTo use SSE_S3 , create an Identity and Access Management (IAM) role with a policy that allows \"arn:aws:s3:::*\" to use the following actions: \"s3:PutObject\", \"s3:ListBucket\"\n\nExplicitIds -> (boolean)\n\nThis setting is only valid for a full-load migration task. Set ExplicitIds to true to have tables with IDENTITY columns override their auto-generated values with explicit values loaded from the source data files used to populate the tables. The default is false .\n\nFileTransferUploadStreams -> (integer)\n\nThe number of threads used to upload a single file. This parameter accepts a value from 1 through 64. It defaults to 10.\n\nThe number of parallel streams used to upload a single .csv file to an S3 bucket using S3 Multipart Upload. For more information, see Multipart upload overview .\n\nFileTransferUploadStreams accepts a value from 1 through 64. It defaults to 10.\n\nLoadTimeout -> (integer)\n\nThe amount of time to wait (in milliseconds) before timing out of operations performed by DMS on a Redshift cluster, such as Redshift COPY, INSERT, DELETE, and UPDATE.\n\nMaxFileSize -> (integer)\n\nThe maximum size (in KB) of any .csv file used to load data on an S3 bucket and transfer data to Amazon Redshift. It defaults to 1048576KB (1 GB).\n\nPassword -> (string)\n\nThe password for the user named in the username property.\n\nPort -> (integer)\n\nThe port number for Amazon Redshift. The default value is 5439.\n\nRemoveQuotes -> (boolean)\n\nA value that specifies to remove surrounding quotation marks from strings in the incoming data. All characters within the quotation marks, including delimiters, are retained. Choose true to remove quotation marks. The default is false .\n\nReplaceInvalidChars -> (string)\n\nA list of characters that you want to replace. Use with ReplaceChars .\n\nReplaceChars -> (string)\n\nA value that specifies to replaces the invalid characters specified in ReplaceInvalidChars , substituting the specified characters instead. The default is \"?\" .\n\nServerName -> (string)\n\nThe name of the Amazon Redshift cluster you are using.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the IAM role that has access to the Amazon Redshift service. The role must allow the iam:PassRole action.\n\nServerSideEncryptionKmsKeyId -> (string)\n\nThe KMS key ID. If you are using SSE_KMS for the EncryptionMode , provide this key ID. The key that you use needs an attached policy that enables IAM user permissions and allows use of the key.\n\nTimeFormat -> (string)\n\nThe time format that you want to use. Valid values are auto (case-sensitive), 'timeformat_string' , 'epochsecs' , or 'epochmillisecs' . It defaults to 10. Using auto recognizes most strings, even some that aren’t supported when you use a time format string.\n\nIf your date and time values use formats different from each other, set this parameter to auto .\n\nTrimBlanks -> (boolean)\n\nA value that specifies to remove the trailing white space characters from a VARCHAR string. This parameter applies only to columns with a VARCHAR data type. Choose true to remove unneeded white space. The default is false .\n\nTruncateColumns -> (boolean)\n\nA value that specifies to truncate data in columns to the appropriate number of characters, so that the data fits in the column. This parameter applies only to columns with a VARCHAR or CHAR data type, and rows with a size of 4 MB or less. Choose true to truncate data. The default is false .\n\nUsername -> (string)\n\nAn Amazon Redshift user name for a registered user.\n\nWriteBufferSize -> (integer)\n\nThe size (in KB) of the in-memory file write buffer used when generating .csv files on the local disk at the DMS replication instance. The default value is 1000 (buffer size is 1000KB).\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Amazon Redshift endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Amazon Redshift endpoint connection details.\n\nShorthand Syntax:\n\nAcceptAnyDate=boolean,AfterConnectScript=string,BucketFolder=string,BucketName=string,CaseSensitiveNames=boolean,CompUpdate=boolean,ConnectionTimeout=integer,DatabaseName=string,DateFormat=string,EmptyAsNull=boolean,EncryptionMode=string,ExplicitIds=boolean,FileTransferUploadStreams=integer,LoadTimeout=integer,MaxFileSize=integer,Password=string,Port=integer,RemoveQuotes=boolean,ReplaceInvalidChars=string,ReplaceChars=string,ServerName=string,ServiceAccessRoleArn=string,ServerSideEncryptionKmsKeyId=string,TimeFormat=string,TrimBlanks=boolean,TruncateColumns=boolean,Username=string,WriteBufferSize=integer,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"AcceptAnyDate\": true|false,\n  \"AfterConnectScript\": \"string\",\n  \"BucketFolder\": \"string\",\n  \"BucketName\": \"string\",\n  \"CaseSensitiveNames\": true|false,\n  \"CompUpdate\": true|false,\n  \"ConnectionTimeout\": integer,\n  \"DatabaseName\": \"string\",\n  \"DateFormat\": \"string\",\n  \"EmptyAsNull\": true|false,\n  \"EncryptionMode\": \"sse-s3\"|\"sse-kms\",\n  \"ExplicitIds\": true|false,\n  \"FileTransferUploadStreams\": integer,\n  \"LoadTimeout\": integer,\n  \"MaxFileSize\": integer,\n  \"Password\": \"string\",\n  \"Port\": integer,\n  \"RemoveQuotes\": true|false,\n  \"ReplaceInvalidChars\": \"string\",\n  \"ReplaceChars\": \"string\",\n  \"ServerName\": \"string\",\n  \"ServiceAccessRoleArn\": \"string\",\n  \"ServerSideEncryptionKmsKeyId\": \"string\",\n  \"TimeFormat\": \"string\",\n  \"TrimBlanks\": true|false,\n  \"TruncateColumns\": true|false,\n  \"Username\": \"string\",\n  \"WriteBufferSize\": integer,\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--postgre-sql-settings (structure)\n\nSettings in JSON format for the source and target PostgreSQL endpoint. For information about other available settings, see Extra connection attributes when using PostgreSQL as a source for DMS and Extra connection attributes when using PostgreSQL as a target for DMS in the Database Migration Service User Guide.\n\nAfterConnectScript -> (string)\n\nFor use with change data capture (CDC) only, this attribute has DMS bypass foreign keys and user triggers to reduce the time it takes to bulk load data.\n\nExample: afterConnectScript=SET session_replication_role='replica'\n\nCaptureDdls -> (boolean)\n\nTo capture DDL events, DMS creates various artifacts in the PostgreSQL database when the task starts. You can later remove these artifacts.\n\nIf this value is set to N , you don’t have to create tables or triggers on the source database.\n\nMaxFileSize -> (integer)\n\nSpecifies the maximum size (in KB) of any .csv file used to transfer data to PostgreSQL.\n\nExample: maxFileSize=512\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nDdlArtifactsSchema -> (string)\n\nThe schema in which the operational DDL database artifacts are created.\n\nExample: ddlArtifactsSchema=xyzddlschema;\n\nExecuteTimeout -> (integer)\n\nSets the client statement timeout for the PostgreSQL instance, in seconds. The default value is 60 seconds.\n\nExample: executeTimeout=100;\n\nFailTasksOnLobTruncation -> (boolean)\n\nWhen set to true , this value causes a task to fail if the actual size of a LOB column is greater than the specified LobMaxSize .\n\nIf task is set to Limited LOB mode and this option is set to true, the task fails instead of truncating the LOB data.\n\nHeartbeatEnable -> (boolean)\n\nThe write-ahead log (WAL) heartbeat feature mimics a dummy transaction. By doing this, it prevents idle logical replication slots from holding onto old WAL logs, which can result in storage full situations on the source. This heartbeat keeps restart_lsn moving and prevents storage full scenarios.\n\nHeartbeatSchema -> (string)\n\nSets the schema in which the heartbeat artifacts are created.\n\nHeartbeatFrequency -> (integer)\n\nSets the WAL heartbeat frequency (in minutes).\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSlotName -> (string)\n\nSets the name of a previously created logical replication slot for a change data capture (CDC) load of the PostgreSQL source instance.\n\nWhen used with the CdcStartPosition request parameter for the DMS API , this attribute also makes it possible to use native CDC start points. DMS verifies that the specified logical replication slot exists before starting the CDC load task. It also verifies that the task was created with a valid setting of CdcStartPosition . If the specified slot doesn’t exist or the task doesn’t have a valid CdcStartPosition setting, DMS raises an error.\n\nFor more information about setting the CdcStartPosition request parameter, see Determining a CDC native start point in the Database Migration Service User Guide . For more information about using CdcStartPosition , see CreateReplicationTask , StartReplicationTask , and ModifyReplicationTask .\n\nPluginName -> (string)\n\nSpecifies the plugin to use to create a replication slot.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the PostgreSQL endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the PostgreSQL endpoint connection details.\n\nShorthand Syntax:\n\nAfterConnectScript=string,CaptureDdls=boolean,MaxFileSize=integer,DatabaseName=string,DdlArtifactsSchema=string,ExecuteTimeout=integer,FailTasksOnLobTruncation=boolean,HeartbeatEnable=boolean,HeartbeatSchema=string,HeartbeatFrequency=integer,Password=string,Port=integer,ServerName=string,Username=string,SlotName=string,PluginName=string,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"AfterConnectScript\": \"string\",\n  \"CaptureDdls\": true|false,\n  \"MaxFileSize\": integer,\n  \"DatabaseName\": \"string\",\n  \"DdlArtifactsSchema\": \"string\",\n  \"ExecuteTimeout\": integer,\n  \"FailTasksOnLobTruncation\": true|false,\n  \"HeartbeatEnable\": true|false,\n  \"HeartbeatSchema\": \"string\",\n  \"HeartbeatFrequency\": integer,\n  \"Password\": \"string\",\n  \"Port\": integer,\n  \"ServerName\": \"string\",\n  \"Username\": \"string\",\n  \"SlotName\": \"string\",\n  \"PluginName\": \"no-preference\"|\"test-decoding\"|\"pglogical\",\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--my-sql-settings (structure)\n\nSettings in JSON format for the source and target MySQL endpoint. For information about other available settings, see Extra connection attributes when using MySQL as a source for DMS and Extra connection attributes when using a MySQL-compatible database as a target for DMS in the Database Migration Service User Guide.\n\nAfterConnectScript -> (string)\n\nSpecifies a script to run immediately after DMS connects to the endpoint. The migration task continues running regardless if the SQL statement succeeds or fails.\n\nFor this parameter, provide the code of the script itself, not the name of a file containing the script.\n\nCleanSourceMetadataOnMismatch -> (boolean)\n\nAdjusts the behavior of DMS when migrating from an SQL Server source database that is hosted as part of an Always On availability group cluster. If you need DMS to poll all the nodes in the Always On cluster for transaction backups, set this attribute to false .\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint. For a MySQL source or target endpoint, don’t explicitly specify the database using the DatabaseName request parameter on either the CreateEndpoint or ModifyEndpoint API call. Specifying DatabaseName when you create or modify a MySQL endpoint replicates all the task tables to this single database. For MySQL endpoints, you specify the database only when you specify the schema in the table-mapping rules of the DMS task.\n\nEventsPollInterval -> (integer)\n\nSpecifies how often to check the binary log for new changes/events when the database is idle.\n\nExample: eventsPollInterval=5;\n\nIn the example, DMS checks for changes in the binary logs every five seconds.\n\nTargetDbType -> (string)\n\nSpecifies where to migrate source tables on the target, either to a single database or multiple databases.\n\nExample: targetDbType=MULTIPLE_DATABASES\n\nMaxFileSize -> (integer)\n\nSpecifies the maximum size (in KB) of any .csv file used to transfer data to a MySQL-compatible database.\n\nExample: maxFileSize=512\n\nParallelLoadThreads -> (integer)\n\nImproves performance when loading data into the MySQL-compatible target database. Specifies how many threads to use to load the data into the MySQL-compatible target database. Setting a large number of threads can have an adverse effect on database performance, because a separate connection is required for each thread.\n\nExample: parallelLoadThreads=1\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nServerTimezone -> (string)\n\nSpecifies the time zone for the source MySQL database.\n\nExample: serverTimezone=US/Pacific;\n\nNote: Do not enclose time zones in single quotes.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the MySQL endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the MySQL endpoint connection details.\n\nShorthand Syntax:\n\nAfterConnectScript=string,CleanSourceMetadataOnMismatch=boolean,DatabaseName=string,EventsPollInterval=integer,TargetDbType=string,MaxFileSize=integer,ParallelLoadThreads=integer,Password=string,Port=integer,ServerName=string,ServerTimezone=string,Username=string,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"AfterConnectScript\": \"string\",\n  \"CleanSourceMetadataOnMismatch\": true|false,\n  \"DatabaseName\": \"string\",\n  \"EventsPollInterval\": integer,\n  \"TargetDbType\": \"specific-database\"|\"multiple-databases\",\n  \"MaxFileSize\": integer,\n  \"ParallelLoadThreads\": integer,\n  \"Password\": \"string\",\n  \"Port\": integer,\n  \"ServerName\": \"string\",\n  \"ServerTimezone\": \"string\",\n  \"Username\": \"string\",\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--oracle-settings (structure)\n\nSettings in JSON format for the source and target Oracle endpoint. For information about other available settings, see Extra connection attributes when using Oracle as a source for DMS and Extra connection attributes when using Oracle as a target for DMS in the Database Migration Service User Guide.\n\nAddSupplementalLogging -> (boolean)\n\nSet this attribute to set up table-level supplemental logging for the Oracle database. This attribute enables PRIMARY KEY supplemental logging on all tables selected for a migration task.\n\nIf you use this option, you still need to enable database-level supplemental logging.\n\nArchivedLogDestId -> (integer)\n\nSpecifies the ID of the destination for the archived redo logs. This value should be the same as a number in the dest_id column of the v$archived_log view. If you work with an additional redo log destination, use the AdditionalArchivedLogDestId option to specify the additional destination ID. Doing this improves performance by ensuring that the correct logs are accessed from the outset.\n\nAdditionalArchivedLogDestId -> (integer)\n\nSet this attribute with ArchivedLogDestId in a primary/ standby setup. This attribute is useful in the case of a switchover. In this case, DMS needs to know which destination to get archive redo logs from to read changes. This need arises because the previous primary instance is now a standby instance after switchover.\n\nAlthough DMS supports the use of the Oracle RESETLOGS option to open the database, never use RESETLOGS unless necessary. For additional information about RESETLOGS , see RMAN Data Repair Concepts in the Oracle Database Backup and Recovery User’s Guide .\n\nExtraArchivedLogDestIds -> (list)\n\nSpecifies the IDs of one more destinations for one or more archived redo logs. These IDs are the values of the dest_id column in the v$archived_log view. Use this setting with the archivedLogDestId extra connection attribute in a primary-to-single setup or a primary-to-multiple-standby setup.\n\nThis setting is useful in a switchover when you use an Oracle Data Guard database as a source. In this case, DMS needs information about what destination to get archive redo logs from to read changes. DMS needs this because after the switchover the previous primary is a standby instance. For example, in a primary-to-single standby setup you might apply the following settings.\n\narchivedLogDestId=1; ExtraArchivedLogDestIds=[2]\n\nIn a primary-to-multiple-standby setup, you might apply the following settings.\n\narchivedLogDestId=1; ExtraArchivedLogDestIds=[2,3,4]\n\nAlthough DMS supports the use of the Oracle RESETLOGS option to open the database, never use RESETLOGS unless it’s necessary. For more information about RESETLOGS , see RMAN Data Repair Concepts in the Oracle Database Backup and Recovery User’s Guide .\n\n(integer)\n\nAllowSelectNestedTables -> (boolean)\n\nSet this attribute to true to enable replication of Oracle tables containing columns that are nested tables or defined types.\n\nParallelAsmReadThreads -> (integer)\n\nSet this attribute to change the number of threads that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 2 (the default) and 8 (the maximum). Use this attribute together with the readAheadBlocks attribute.\n\nReadAheadBlocks -> (integer)\n\nSet this attribute to change the number of read-ahead blocks that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 1000 (the default) and 200,000 (the maximum).\n\nAccessAlternateDirectly -> (boolean)\n\nSet this attribute to false in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to not access redo logs through any specified path prefix replacement using direct file access.\n\nUseAlternateFolderForOnline -> (boolean)\n\nSet this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to use any specified prefix replacement to access all online redo logs.\n\nOraclePathPrefix -> (string)\n\nSet this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the default Oracle root used to access the redo logs.\n\nUsePathPrefix -> (string)\n\nSet this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the path prefix used to replace the default Oracle root to access the redo logs.\n\nReplacePathPrefix -> (boolean)\n\nSet this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This setting tells DMS instance to replace the default Oracle root with the specified usePathPrefix setting to access the redo logs.\n\nEnableHomogenousTablespace -> (boolean)\n\nSet this attribute to enable homogenous tablespace replication and create existing tables or indexes under the same tablespace on the target.\n\nDirectPathNoLog -> (boolean)\n\nWhen set to true , this attribute helps to increase the commit rate on the Oracle target database by writing directly to tables and not writing a trail to database logs.\n\nArchivedLogsOnly -> (boolean)\n\nWhen this field is set to Y , DMS only accesses the archived redo logs. If the archived redo logs are stored on Oracle ASM only, the DMS user account needs to be granted ASM privileges.\n\nAsmPassword -> (string)\n\nFor an Oracle source endpoint, your Oracle Automatic Storage Management (ASM) password. You can set this value from the `` asm_user_password `` value. You set this value as part of the comma-separated value that you set to the Password request parameter when you create the endpoint to access transaction logs using Binary Reader. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nAsmServer -> (string)\n\nFor an Oracle source endpoint, your ASM server address. You can set this value from the asm_server value. You set asm_server as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nAsmUser -> (string)\n\nFor an Oracle source endpoint, your ASM user name. You can set this value from the asm_user value. You set asm_user as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nCharLengthSemantics -> (string)\n\nSpecifies whether the length of a character column is in bytes or in characters. To indicate that the character column length is in characters, set this attribute to CHAR . Otherwise, the character column length is in bytes.\n\nExample: charLengthSemantics=CHAR;\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nDirectPathParallelLoad -> (boolean)\n\nWhen set to true , this attribute specifies a parallel load when useDirectPathFullLoad is set to Y . This attribute also only applies when you use the DMS parallel load feature. Note that the target table cannot have any constraints or indexes.\n\nFailTasksOnLobTruncation -> (boolean)\n\nWhen set to true , this attribute causes a task to fail if the actual size of an LOB column is greater than the specified LobMaxSize .\n\nIf a task is set to limited LOB mode and this option is set to true , the task fails instead of truncating the LOB data.\n\nNumberDatatypeScale -> (integer)\n\nSpecifies the number scale. You can select a scale up to 38, or you can select FLOAT. By default, the NUMBER data type is converted to precision 38, scale 10.\n\nExample: numberDataTypeScale=12\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nReadTableSpaceName -> (boolean)\n\nWhen set to true , this attribute supports tablespace replication.\n\nRetryInterval -> (integer)\n\nSpecifies the number of seconds that the system waits before resending a query.\n\nExample: retryInterval=6;\n\nSecurityDbEncryption -> (string)\n\nFor an Oracle source endpoint, the transparent data encryption (TDE) password required by AWM DMS to access Oracle redo logs encrypted by TDE using Binary Reader. It is also the `` TDE_Password `` part of the comma-separated value you set to the Password request parameter when you create the endpoint. The SecurityDbEncryptian setting is related to this SecurityDbEncryptionName setting. For more information, see Supported encryption methods for using Oracle as a source for DMS in the Database Migration Service User Guide .\n\nSecurityDbEncryptionName -> (string)\n\nFor an Oracle source endpoint, the name of a key used for the transparent data encryption (TDE) of the columns and tablespaces in an Oracle source database that is encrypted using TDE. The key value is the value of the SecurityDbEncryption setting. For more information on setting the key name value of SecurityDbEncryptionName , see the information and example for setting the securityDbEncryptionName extra connection attribute in Supported encryption methods for using Oracle as a source for DMS in the Database Migration Service User Guide .\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nSpatialDataOptionToGeoJsonFunctionName -> (string)\n\nUse this attribute to convert SDO_GEOMETRY to GEOJSON format. By default, DMS calls the SDO2GEOJSON custom function if present and accessible. Or you can create your own custom function that mimics the operation of SDOGEOJSON and set SpatialDataOptionToGeoJsonFunctionName to call it instead.\n\nStandbyDelayTime -> (integer)\n\nUse this attribute to specify a time in minutes for the delay in standby sync. If the source is an Oracle Active Data Guard standby database, use this attribute to specify the time lag between primary and standby databases.\n\nIn DMS, you can create an Oracle CDC task that uses an Active Data Guard standby instance as a source for replicating ongoing changes. Doing this eliminates the need to connect to an active database that might be in production.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nUseBFile -> (boolean)\n\nSet this attribute to Y to capture change data using the Binary Reader utility. Set UseLogminerReader to N to set this attribute to Y. To use Binary Reader with Amazon RDS for Oracle as the source, you set additional attributes. For more information about using this setting with Oracle Automatic Storage Management (ASM), see Using Oracle LogMiner or DMS Binary Reader for CDC .\n\nUseDirectPathFullLoad -> (boolean)\n\nSet this attribute to Y to have DMS use a direct path full load. Specify this value to use the direct path protocol in the Oracle Call Interface (OCI). By using this OCI protocol, you can bulk-load Oracle target tables during a full load.\n\nUseLogminerReader -> (boolean)\n\nSet this attribute to Y to capture change data using the Oracle LogMiner utility (the default). Set this attribute to N if you want to access the redo logs as a binary file. When you set UseLogminerReader to N, also set UseBfile to Y. For more information on this setting and using Oracle ASM, see Using Oracle LogMiner or DMS Binary Reader for CDC in the DMS User Guide .\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Oracle endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Oracle endpoint connection details.\n\nSecretsManagerOracleAsmAccessRoleArn -> (string)\n\nRequired only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the SecretsManagerOracleAsmSecret . This SecretsManagerOracleAsmSecret has the secret value that allows access to the Oracle ASM of the endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerOracleAsmSecretId . Or you can specify clear-text values for AsmUserName , AsmPassword , and AsmServerName . You can’t specify both. For more information on creating this SecretsManagerOracleAsmSecret and the SecretsManagerOracleAsmAccessRoleArn and SecretsManagerOracleAsmSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerOracleAsmSecretId -> (string)\n\nRequired only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN, partial ARN, or friendly name of the SecretsManagerOracleAsmSecret that contains the Oracle ASM connection details for the Oracle endpoint.\n\nShorthand Syntax:\n\nAddSupplementalLogging=boolean,ArchivedLogDestId=integer,AdditionalArchivedLogDestId=integer,ExtraArchivedLogDestIds=integer,integer,AllowSelectNestedTables=boolean,ParallelAsmReadThreads=integer,ReadAheadBlocks=integer,AccessAlternateDirectly=boolean,UseAlternateFolderForOnline=boolean,OraclePathPrefix=string,UsePathPrefix=string,ReplacePathPrefix=boolean,EnableHomogenousTablespace=boolean,DirectPathNoLog=boolean,ArchivedLogsOnly=boolean,AsmPassword=string,AsmServer=string,AsmUser=string,CharLengthSemantics=string,DatabaseName=string,DirectPathParallelLoad=boolean,FailTasksOnLobTruncation=boolean,NumberDatatypeScale=integer,Password=string,Port=integer,ReadTableSpaceName=boolean,RetryInterval=integer,SecurityDbEncryption=string,SecurityDbEncryptionName=string,ServerName=string,SpatialDataOptionToGeoJsonFunctionName=string,StandbyDelayTime=integer,Username=string,UseBFile=boolean,UseDirectPathFullLoad=boolean,UseLogminerReader=boolean,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string,SecretsManagerOracleAsmAccessRoleArn=string,SecretsManagerOracleAsmSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"AddSupplementalLogging\": true|false,\n  \"ArchivedLogDestId\": integer,\n  \"AdditionalArchivedLogDestId\": integer,\n  \"ExtraArchivedLogDestIds\": [integer, ...],\n  \"AllowSelectNestedTables\": true|false,\n  \"ParallelAsmReadThreads\": integer,\n  \"ReadAheadBlocks\": integer,\n  \"AccessAlternateDirectly\": true|false,\n  \"UseAlternateFolderForOnline\": true|false,\n  \"OraclePathPrefix\": \"string\",\n  \"UsePathPrefix\": \"string\",\n  \"ReplacePathPrefix\": true|false,\n  \"EnableHomogenousTablespace\": true|false,\n  \"DirectPathNoLog\": true|false,\n  \"ArchivedLogsOnly\": true|false,\n  \"AsmPassword\": \"string\",\n  \"AsmServer\": \"string\",\n  \"AsmUser\": \"string\",\n  \"CharLengthSemantics\": \"default\"|\"char\"|\"byte\",\n  \"DatabaseName\": \"string\",\n  \"DirectPathParallelLoad\": true|false,\n  \"FailTasksOnLobTruncation\": true|false,\n  \"NumberDatatypeScale\": integer,\n  \"Password\": \"string\",\n  \"Port\": integer,\n  \"ReadTableSpaceName\": true|false,\n  \"RetryInterval\": integer,\n  \"SecurityDbEncryption\": \"string\",\n  \"SecurityDbEncryptionName\": \"string\",\n  \"ServerName\": \"string\",\n  \"SpatialDataOptionToGeoJsonFunctionName\": \"string\",\n  \"StandbyDelayTime\": integer,\n  \"Username\": \"string\",\n  \"UseBFile\": true|false,\n  \"UseDirectPathFullLoad\": true|false,\n  \"UseLogminerReader\": true|false,\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\",\n  \"SecretsManagerOracleAsmAccessRoleArn\": \"string\",\n  \"SecretsManagerOracleAsmSecretId\": \"string\"\n}\n\n\n--sybase-settings (structure)\n\nSettings in JSON format for the source and target SAP ASE endpoint. For information about other available settings, see Extra connection attributes when using SAP ASE as a source for DMS and Extra connection attributes when using SAP ASE as a target for DMS in the Database Migration Service User Guide.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the SAP ASE endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the SAP SAE endpoint connection details.\n\nShorthand Syntax:\n\nDatabaseName=string,Password=string,Port=integer,ServerName=string,Username=string,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"DatabaseName\": \"string\",\n  \"Password\": \"string\",\n  \"Port\": integer,\n  \"ServerName\": \"string\",\n  \"Username\": \"string\",\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--microsoft-sql-server-settings (structure)\n\nSettings in JSON format for the source and target Microsoft SQL Server endpoint. For information about other available settings, see Extra connection attributes when using SQL Server as a source for DMS and Extra connection attributes when using SQL Server as a target for DMS in the Database Migration Service User Guide.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nBcpPacketSize -> (integer)\n\nThe maximum size of the packets (in bytes) used to transfer data using BCP.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nControlTablesFileGroup -> (string)\n\nSpecifies a file group for the DMS internal tables. When the replication task starts, all the internal DMS control tables (awsdms_ apply_exception, awsdms_apply, awsdms_changes) are created for the specified file group.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nQuerySingleAlwaysOnNode -> (boolean)\n\nCleans and recreates table metadata information on the replication instance when a mismatch occurs. An example is a situation where running an alter DDL statement on a table might result in different information about the table cached in the replication instance.\n\nReadBackupOnly -> (boolean)\n\nWhen this attribute is set to Y , DMS only reads changes from transaction log backups and doesn’t read from the active transaction log file during ongoing replication. Setting this parameter to Y enables you to control active transaction log file growth during full load and ongoing replication tasks. However, it can add some source latency to ongoing replication.\n\nSafeguardPolicy -> (string)\n\nUse this attribute to minimize the need to access the backup log and enable DMS to prevent truncation using one of the following two methods.\n\nStart transactions in the database: This is the default method. When this method is used, DMS prevents TLOG truncation by mimicking a transaction in the database. As long as such a transaction is open, changes that appear after the transaction started aren’t truncated. If you need Microsoft Replication to be enabled in your database, then you must choose this method.\n\nExclusively use sp_repldone within a single task : When this method is used, DMS reads the changes and then uses sp_repldone to mark the TLOG transactions as ready for truncation. Although this method doesn’t involve any transactional activities, it can only be used when Microsoft Replication isn’t running. Also, when using this method, only one DMS task can access the database at any given time. Therefore, if you need to run parallel DMS tasks against the same database, use the default method.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nUseBcpFullLoad -> (boolean)\n\nUse this to attribute to transfer data for full-load operations using BCP. When the target table contains an identity column that does not exist in the source table, you must disable the use BCP for loading table option.\n\nUseThirdPartyBackupDevice -> (boolean)\n\nWhen this attribute is set to Y , DMS processes third-party transaction log backups if they are created in native format.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the SQL Server endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the SQL Server endpoint connection details.\n\nShorthand Syntax:\n\nPort=integer,BcpPacketSize=integer,DatabaseName=string,ControlTablesFileGroup=string,Password=string,QuerySingleAlwaysOnNode=boolean,ReadBackupOnly=boolean,SafeguardPolicy=string,ServerName=string,Username=string,UseBcpFullLoad=boolean,UseThirdPartyBackupDevice=boolean,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"Port\": integer,\n  \"BcpPacketSize\": integer,\n  \"DatabaseName\": \"string\",\n  \"ControlTablesFileGroup\": \"string\",\n  \"Password\": \"string\",\n  \"QuerySingleAlwaysOnNode\": true|false,\n  \"ReadBackupOnly\": true|false,\n  \"SafeguardPolicy\": \"rely-on-sql-server-replication-agent\"|\"exclusive-automatic-truncation\"|\"shared-automatic-truncation\",\n  \"ServerName\": \"string\",\n  \"Username\": \"string\",\n  \"UseBcpFullLoad\": true|false,\n  \"UseThirdPartyBackupDevice\": true|false,\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--ibm-db2-settings (structure)\n\nSettings in JSON format for the source IBM Db2 LUW endpoint. For information about other available settings, see Extra connection attributes when using Db2 LUW as a source for DMS in the Database Migration Service User Guide.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port. The default value is 50000.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nSetDataCaptureChanges -> (boolean)\n\nEnables ongoing replication (CDC) as a BOOLEAN value. The default is true.\n\nCurrentLsn -> (string)\n\nFor ongoing replication (CDC), use CurrentLSN to specify a log sequence number (LSN) where you want the replication to start.\n\nMaxKBytesPerRead -> (integer)\n\nMaximum number of bytes per read, as a NUMBER value. The default is 64 KB.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Db2 LUW endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Db2 LUW endpoint connection details.\n\nShorthand Syntax:\n\nDatabaseName=string,Password=string,Port=integer,ServerName=string,SetDataCaptureChanges=boolean,CurrentLsn=string,MaxKBytesPerRead=integer,Username=string,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"DatabaseName\": \"string\",\n  \"Password\": \"string\",\n  \"Port\": integer,\n  \"ServerName\": \"string\",\n  \"SetDataCaptureChanges\": true|false,\n  \"CurrentLsn\": \"string\",\n  \"MaxKBytesPerRead\": integer,\n  \"Username\": \"string\",\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--doc-db-settings (structure)\n\nSettings in JSON format for the source DocumentDB endpoint. For more information about the available settings, see the configuration properties section in Using DocumentDB as a Target for Database Migration Service in the Database Migration Service User Guide.\n\nUsername -> (string)\n\nThe user name you use to access the DocumentDB source endpoint.\n\nPassword -> (string)\n\nThe password for the user account you use to access the DocumentDB source endpoint.\n\nServerName -> (string)\n\nThe name of the server on the DocumentDB source endpoint.\n\nPort -> (integer)\n\nThe port value for the DocumentDB source endpoint.\n\nDatabaseName -> (string)\n\nThe database name on the DocumentDB source endpoint.\n\nNestingLevel -> (string)\n\nSpecifies either document or table mode.\n\nDefault value is \"none\" . Specify \"none\" to use document mode. Specify \"one\" to use table mode.\n\nExtractDocId -> (boolean)\n\nSpecifies the document ID. Use this setting when NestingLevel is set to \"none\" .\n\nDefault value is \"false\" .\n\nDocsToInvestigate -> (integer)\n\nIndicates the number of documents to preview to determine the document organization. Use this setting when NestingLevel is set to \"one\" .\n\nMust be a positive value greater than 0 . Default value is 1000 .\n\nKmsKeyId -> (string)\n\nThe KMS key identifier that is used to encrypt the content on the replication instance. If you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key. KMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the DocumentDB endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the DocumentDB endpoint connection details.\n\nShorthand Syntax:\n\nUsername=string,Password=string,ServerName=string,Port=integer,DatabaseName=string,NestingLevel=string,ExtractDocId=boolean,DocsToInvestigate=integer,KmsKeyId=string,SecretsManagerAccessRoleArn=string,SecretsManagerSecretId=string\n\n\nJSON Syntax:\n\n{\n  \"Username\": \"string\",\n  \"Password\": \"string\",\n  \"ServerName\": \"string\",\n  \"Port\": integer,\n  \"DatabaseName\": \"string\",\n  \"NestingLevel\": \"none\"|\"one\",\n  \"ExtractDocId\": true|false,\n  \"DocsToInvestigate\": integer,\n  \"KmsKeyId\": \"string\",\n  \"SecretsManagerAccessRoleArn\": \"string\",\n  \"SecretsManagerSecretId\": \"string\"\n}\n\n\n--redis-settings (structure)\n\nSettings in JSON format for the Redis target endpoint.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nPort -> (integer)\n\nTransmission Control Protocol (TCP) port for the endpoint.\n\nSslSecurityProtocol -> (string)\n\nThe connection to a Redis target endpoint using Transport Layer Security (TLS). Valid values include plaintext and ssl-encryption . The default is ssl-encryption . The ssl-encryption option makes an encrypted connection. Optionally, you can identify an Amazon Resource Name (ARN) for an SSL certificate authority (CA) using the SslCaCertificateArn setting. If an ARN isn’t given for a CA, DMS uses the Amazon root CA.\n\nThe plaintext option doesn’t provide Transport Layer Security (TLS) encryption for traffic between endpoint and database.\n\nAuthType -> (string)\n\nThe type of authentication to perform when connecting to a Redis target. Options include none , auth-token , and auth-role . The auth-token option requires an AuthPassword value to be provided. The auth-role option requires AuthUserName and AuthPassword values to be provided.\n\nAuthUserName -> (string)\n\nThe user name provided with the auth-role option of the AuthType setting for a Redis target endpoint.\n\nAuthPassword -> (string)\n\nThe password provided with the auth-role and auth-token options of the AuthType setting for a Redis target endpoint.\n\nSslCaCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the certificate authority (CA) that DMS uses to connect to your Redis target endpoint.\n\nShorthand Syntax:\n\nServerName=string,Port=integer,SslSecurityProtocol=string,AuthType=string,AuthUserName=string,AuthPassword=string,SslCaCertificateArn=string\n\n\nJSON Syntax:\n\n{\n  \"ServerName\": \"string\",\n  \"Port\": integer,\n  \"SslSecurityProtocol\": \"plaintext\"|\"ssl-encryption\",\n  \"AuthType\": \"none\"|\"auth-role\"|\"auth-token\",\n  \"AuthUserName\": \"string\",\n  \"AuthPassword\": \"string\",\n  \"SslCaCertificateArn\": \"string\"\n}\n\n\n--exact-settings | --no-exact-settings (boolean)\n\nIf this attribute is Y, the current call to ModifyEndpoint replaces all existing endpoint settings with the exact settings that you specify in this call. If this attribute is N, the current call to ModifyEndpoint does two things:\n\nIt replaces any endpoint settings that already exist with new values, for settings with the same names.\n\nIt creates new endpoint settings that you specify in the call, for settings with different names.\n\nFor example, if you call create-endpoint ... --endpoint-settings '{\"a\":1}' ... , the endpoint has the following endpoint settings: '{\"a\":1}' . If you then call modify-endpoint ... --endpoint-settings '{\"b\":2}' ... for the same endpoint, the endpoint has the following settings: '{\"a\":1,\"b\":2}' .\n\nHowever, suppose that you follow this with a call to modify-endpoint ... --endpoint-settings '{\"b\":2}' --exact-settings ... for that same endpoint again. Then the endpoint has the following settings: '{\"b\":2}' . All existing settings are replaced with the exact settings that you specify.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nEndpoint -> (structure)\n\nThe modified endpoint.\n\nEndpointIdentifier -> (string)\n\nThe database endpoint identifier. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen or contain two consecutive hyphens.\n\nEndpointType -> (string)\n\nThe type of endpoint. Valid values are source and target .\n\nEngineName -> (string)\n\nThe database engine name. Valid values, depending on the EndpointType, include \"mysql\" , \"oracle\" , \"postgres\" , \"mariadb\" , \"aurora\" , \"aurora-postgresql\" , \"redshift\" , \"s3\" , \"db2\" , \"azuredb\" , \"sybase\" , \"dynamodb\" , \"mongodb\" , \"kinesis\" , \"kafka\" , \"elasticsearch\" , \"documentdb\" , \"sqlserver\" , and \"neptune\" .\n\nEngineDisplayName -> (string)\n\nThe expanded name for the engine name. For example, if the EngineName parameter is “aurora,” this value would be “Amazon Aurora MySQL.”\n\nUsername -> (string)\n\nThe user name used to connect to the endpoint.\n\nServerName -> (string)\n\nThe name of the server at the endpoint.\n\nPort -> (integer)\n\nThe port value used to access the endpoint.\n\nDatabaseName -> (string)\n\nThe name of the database at the endpoint.\n\nExtraConnectionAttributes -> (string)\n\nAdditional connection attributes used to connect to the endpoint.\n\nStatus -> (string)\n\nThe status of the endpoint.\n\nKmsKeyId -> (string)\n\nAn KMS key identifier that is used to encrypt the connection parameters for the endpoint.\n\nIf you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key.\n\nKMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\nCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) used for SSL connection to the endpoint.\n\nSslMode -> (string)\n\nThe SSL mode used to connect to the endpoint. The default value is none .\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nExternalTableDefinition -> (string)\n\nThe external table definition.\n\nExternalId -> (string)\n\nValue returned by a call to CreateEndpoint that can be used for cross-account validation. Use it on a subsequent call to CreateEndpoint to create the endpoint with a cross-account.\n\nDynamoDbSettings -> (structure)\n\nThe settings for the DynamoDB target endpoint. For more information, see the DynamoDBSettings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nS3Settings -> (structure)\n\nThe settings for the S3 target endpoint. For more information, see the S3Settings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action. It is a required parameter that enables DMS to write and read objects from an S3 bucket.\n\nExternalTableDefinition -> (string)\n\nSpecifies how tables are defined in the S3 source files only.\n\nCsvRowDelimiter -> (string)\n\nThe delimiter used to separate rows in the .csv file for both source and target. The default is a carriage return (\\n ).\n\nCsvDelimiter -> (string)\n\nThe delimiter used to separate columns in the .csv file for both source and target. The default is a comma.\n\nBucketFolder -> (string)\n\nAn optional parameter to set a folder name in the S3 bucket. If provided, tables are created in the path `` bucketFolder /schema_name /table_name /`` . If this parameter isn’t specified, then the path used is `` schema_name /table_name /`` .\n\nBucketName -> (string)\n\nThe name of the S3 bucket.\n\nCompressionType -> (string)\n\nAn optional parameter to use GZIP to compress the target files. Set to GZIP to compress the target files. Either set this parameter to NONE (the default) or don’t use it to leave the files uncompressed. This parameter applies to both .csv and .parquet file formats.\n\nEncryptionMode -> (string)\n\nThe type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either SSE_S3 (the default) or SSE_KMS .\n\nNote\n\nFor the ModifyEndpoint operation, you can change the existing value of the EncryptionMode parameter from SSE_KMS to SSE_S3 . But you can’t change the existing value from SSE_S3 to SSE_KMS .\n\nTo use SSE_S3 , you need an Identity and Access Management (IAM) role with permission to allow \"arn:aws:s3:::dms-*\" to use the following actions:\n\ns3:CreateBucket\n\ns3:ListBucket\n\ns3:DeleteBucket\n\ns3:GetBucketLocation\n\ns3:GetObject\n\ns3:PutObject\n\ns3:DeleteObject\n\ns3:GetObjectVersion\n\ns3:GetBucketPolicy\n\ns3:PutBucketPolicy\n\ns3:DeleteBucketPolicy\n\nServerSideEncryptionKmsKeyId -> (string)\n\nIf you are using SSE_KMS for the EncryptionMode , provide the KMS key ID. The key that you use needs an attached policy that enables Identity and Access Management (IAM) user permissions and allows use of the key.\n\nHere is a CLI example: ``aws dms create-endpoint –endpoint-identifier value –endpoint-type target –engine-name s3 –s3-settings ServiceAccessRoleArn=*value* ,BucketFolder=*value* ,BucketName=*value* ,EncryptionMode=SSE_KMS,ServerSideEncryptionKmsKeyId=*value* ``\n\nDataFormat -> (string)\n\nThe format of the data that you want to use for output. You can choose one of the following:\n\ncsv : This is a row-based file format with comma-separated values (.csv).\n\nparquet : Apache Parquet (.parquet) is a columnar storage file format that features efficient compression and provides faster query response.\n\nEncodingType -> (string)\n\nThe type of encoding you are using:\n\nRLE_DICTIONARY uses a combination of bit-packing and run-length encoding to store repeated values more efficiently. This is the default.\n\nPLAIN doesn’t use encoding at all. Values are stored as they are.\n\nPLAIN_DICTIONARY builds a dictionary of the values encountered in a given column. The dictionary is stored in a dictionary page for each column chunk.\n\nDictPageSizeLimit -> (integer)\n\nThe maximum size of an encoded dictionary page of a column. If the dictionary page exceeds this, this column is stored using an encoding type of PLAIN . This parameter defaults to 1024 * 1024 bytes (1 MiB), the maximum size of a dictionary page before it reverts to PLAIN encoding. This size is used for .parquet file format only.\n\nRowGroupLength -> (integer)\n\nThe number of rows in a row group. A smaller row group size provides faster reads. But as the number of row groups grows, the slower writes become. This parameter defaults to 10,000 rows. This number is used for .parquet file format only.\n\nIf you choose a value larger than the maximum, RowGroupLength is set to the max row group length in bytes (64 * 1024 * 1024).\n\nDataPageSize -> (integer)\n\nThe size of one data page in bytes. This parameter defaults to 1024 * 1024 bytes (1 MiB). This number is used for .parquet file format only.\n\nParquetVersion -> (string)\n\nThe version of the Apache Parquet format that you want to use: parquet_1_0 (the default) or parquet_2_0 .\n\nEnableStatistics -> (boolean)\n\nA value that enables statistics for Parquet pages and row groups. Choose true to enable statistics, false to disable. Statistics include NULL , DISTINCT , MAX , and MIN values. This parameter defaults to true . This value is used for .parquet file format only.\n\nIncludeOpForFullLoad -> (boolean)\n\nA value that enables a full load to write INSERT operations to the comma-separated value (.csv) output files only to indicate how the rows were added to the source database.\n\nNote\n\nDMS supports the IncludeOpForFullLoad parameter in versions 3.1.4 and later.\n\nFor full load, records can only be inserted. By default (the false setting), no information is recorded in these output files for a full load to indicate that the rows were inserted at the source database. If IncludeOpForFullLoad is set to true or y , the INSERT is recorded as an I annotation in the first field of the .csv file. This allows the format of your target records from a full load to be consistent with the target records from a CDC load.\n\nNote\n\nThis setting works together with the CdcInsertsOnly and the CdcInsertsAndUpdates parameters for output to .csv files only. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nCdcInsertsOnly -> (boolean)\n\nA value that enables a change data capture (CDC) load to write only INSERT operations to .csv or columnar storage (.parquet) output files. By default (the false setting), the first field in a .csv or .parquet record contains the letter I (INSERT), U (UPDATE), or D (DELETE). These values indicate whether the row was inserted, updated, or deleted at the source database for a CDC load to the target.\n\nIf CdcInsertsOnly is set to true or y , only INSERTs from the source database are migrated to the .csv or .parquet file. For .csv format only, how these INSERTs are recorded depends on the value of IncludeOpForFullLoad . If IncludeOpForFullLoad is set to true , the first field of every CDC record is set to I to indicate the INSERT operation at the source. If IncludeOpForFullLoad is set to false , every CDC record is written without a first field to indicate the INSERT operation at the source. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nNote\n\nDMS supports the interaction described preceding between the CdcInsertsOnly and IncludeOpForFullLoad parameters in versions 3.1.4 and later.\n\nCdcInsertsOnly and CdcInsertsAndUpdates can’t both be set to true for the same endpoint. Set either CdcInsertsOnly or CdcInsertsAndUpdates to true for the same endpoint, but not both.\n\nTimestampColumnName -> (string)\n\nA value that when nonblank causes DMS to add a column with timestamp information to the endpoint data for an Amazon S3 target.\n\nNote\n\nDMS supports the TimestampColumnName parameter in versions 3.1.4 and later.\n\nDMS includes an additional STRING column in the .csv or .parquet object files of your migrated data when you set TimestampColumnName to a nonblank value.\n\nFor a full load, each row of this timestamp column contains a timestamp for when the data was transferred from the source to the target by DMS.\n\nFor a change data capture (CDC) load, each row of the timestamp column contains the timestamp for the commit of that row in the source database.\n\nThe string format for this timestamp column value is yyyy-MM-dd HH:mm:ss.SSSSSS . By default, the precision of this value is in microseconds. For a CDC load, the rounding of the precision depends on the commit timestamp supported by DMS for the source database.\n\nWhen the AddColumnName parameter is set to true , DMS also includes a name for the timestamp column that you set with TimestampColumnName .\n\nParquetTimestampInMillisecond -> (boolean)\n\nA value that specifies the precision of any TIMESTAMP column values that are written to an Amazon S3 object file in .parquet format.\n\nNote\n\nDMS supports the ParquetTimestampInMillisecond parameter in versions 3.1.4 and later.\n\nWhen ParquetTimestampInMillisecond is set to true or y , DMS writes all TIMESTAMP columns in a .parquet formatted file with millisecond precision. Otherwise, DMS writes them with microsecond precision.\n\nCurrently, Amazon Athena and Glue can handle only millisecond precision for TIMESTAMP values. Set this parameter to true for S3 endpoint object files that are .parquet formatted only if you plan to query or process the data with Athena or Glue.\n\nNote\n\nDMS writes any TIMESTAMP column values written to an S3 file in .csv format with microsecond precision.\n\nSetting ParquetTimestampInMillisecond has no effect on the string format of the timestamp column value that is inserted by setting the TimestampColumnName parameter.\n\nCdcInsertsAndUpdates -> (boolean)\n\nA value that enables a change data capture (CDC) load to write INSERT and UPDATE operations to .csv or .parquet (columnar storage) output files. The default setting is false , but when CdcInsertsAndUpdates is set to true or y , only INSERTs and UPDATEs from the source database are migrated to the .csv or .parquet file.\n\nFor .csv file format only, how these INSERTs and UPDATEs are recorded depends on the value of the IncludeOpForFullLoad parameter. If IncludeOpForFullLoad is set to true , the first field of every CDC record is set to either I or U to indicate INSERT and UPDATE operations at the source. But if IncludeOpForFullLoad is set to false , CDC records are written without an indication of INSERT or UPDATE operations at the source. For more information about how these settings work together, see Indicating Source DB Operations in Migrated S3 Data in the Database Migration Service User Guide. .\n\nNote\n\nDMS supports the use of the CdcInsertsAndUpdates parameter in versions 3.3.1 and later.\n\nCdcInsertsOnly and CdcInsertsAndUpdates can’t both be set to true for the same endpoint. Set either CdcInsertsOnly or CdcInsertsAndUpdates to true for the same endpoint, but not both.\n\nDatePartitionEnabled -> (boolean)\n\nWhen set to true , this parameter partitions S3 bucket folders based on transaction commit dates. The default value is false . For more information about date-based folder partitioning, see Using date-based folder partitioning .\n\nDatePartitionSequence -> (string)\n\nIdentifies the sequence of the date format to use during folder partitioning. The default value is YYYYMMDD . Use this parameter when DatePartitionedEnabled is set to true .\n\nDatePartitionDelimiter -> (string)\n\nSpecifies a date separating delimiter to use during folder partitioning. The default value is SLASH . Use this parameter when DatePartitionedEnabled is set to true .\n\nUseCsvNoSupValue -> (boolean)\n\nThis setting applies if the S3 output files during a change data capture (CDC) load are written in .csv format. If set to true for columns not included in the supplemental log, DMS uses the value specified by ` CsvNoSupValue https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CsvNoSupValue`__ . If not set or set to false , DMS uses the null value for these columns.\n\nNote\n\nThis setting is supported in DMS versions 3.4.1 and later.\n\nCsvNoSupValue -> (string)\n\nThis setting only applies if your Amazon S3 output files during a change data capture (CDC) load are written in .csv format. If ` UseCsvNoSupValue https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-UseCsvNoSupValue`__ is set to true, specify a string value that you want DMS to use for all columns not included in the supplemental log. If you do not specify a string value, DMS uses the null value for these columns regardless of the UseCsvNoSupValue setting.\n\nNote\n\nThis setting is supported in DMS versions 3.4.1 and later.\n\nPreserveTransactions -> (boolean)\n\nIf set to true , DMS saves the transaction order for a change data capture (CDC) load on the Amazon S3 target specified by ` CdcPath https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CdcPath`__ . For more information, see Capturing data changes (CDC) including transaction order on the S3 target .\n\nNote\n\nThis setting is supported in DMS versions 3.4.2 and later.\n\nCdcPath -> (string)\n\nSpecifies the folder path of CDC files. For an S3 source, this setting is required if a task captures change data; otherwise, it’s optional. If CdcPath is set, DMS reads CDC files from this path and replicates the data changes to the target endpoint. For an S3 target if you set ` PreserveTransactions https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-PreserveTransactions`__ to true , DMS verifies that you have set this parameter to a folder path on your S3 target where DMS can save the transaction order for the CDC load. DMS creates this CDC folder path in either your S3 target working directory or the S3 target location specified by ` BucketFolder https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketFolder`__ and ` BucketName https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketName`__ .\n\nFor example, if you specify CdcPath as MyChangedData , and you specify BucketName as MyTargetBucket but do not specify BucketFolder , DMS creates the CDC folder path following: MyTargetBucket/MyChangedData .\n\nIf you specify the same CdcPath , and you specify BucketName as MyTargetBucket and BucketFolder as MyTargetData , DMS creates the CDC folder path following: MyTargetBucket/MyTargetData/MyChangedData .\n\nFor more information on CDC including transaction order on an S3 target, see Capturing data changes (CDC) including transaction order on the S3 target .\n\nNote\n\nThis setting is supported in DMS versions 3.4.2 and later.\n\nCannedAclForObjects -> (string)\n\nA value that enables DMS to specify a predefined (canned) access control list for objects created in an Amazon S3 bucket as .csv or .parquet files. For more information about Amazon S3 canned ACLs, see Canned ACL in the Amazon S3 Developer Guide.\n\nThe default value is NONE. Valid values include NONE, PRIVATE, PUBLIC_READ, PUBLIC_READ_WRITE, AUTHENTICATED_READ, AWS_EXEC_READ, BUCKET_OWNER_READ, and BUCKET_OWNER_FULL_CONTROL.\n\nAddColumnName -> (boolean)\n\nAn optional parameter that, when set to true or y , you can use to add column name information to the .csv output file.\n\nThe default value is false . Valid values are true , false , y , and n .\n\nCdcMaxBatchInterval -> (integer)\n\nMaximum length of the interval, defined in seconds, after which to output a file to Amazon S3.\n\nWhen CdcMaxBatchInterval and CdcMinFileSize are both specified, the file write is triggered by whichever parameter condition is met first within an DMS CloudFormation template.\n\nThe default value is 60 seconds.\n\nCdcMinFileSize -> (integer)\n\nMinimum file size, defined in megabytes, to reach for a file output to Amazon S3.\n\nWhen CdcMinFileSize and CdcMaxBatchInterval are both specified, the file write is triggered by whichever parameter condition is met first within an DMS CloudFormation template.\n\nThe default value is 32 MB.\n\nCsvNullValue -> (string)\n\nAn optional parameter that specifies how DMS treats null values. While handling the null value, you can use this parameter to pass a user-defined string as null when writing to the target. For example, when target columns are not nullable, you can use this option to differentiate between the empty string value and the null value. So, if you set this parameter value to the empty string (“” or ‘’), DMS treats the empty string as the null value instead of NULL .\n\nThe default value is NULL . Valid values include any valid string.\n\nIgnoreHeaderRows -> (integer)\n\nWhen this value is set to 1, DMS ignores the first row header in a .csv file. A value of 1 turns on the feature; a value of 0 turns off the feature.\n\nThe default is 0.\n\nMaxFileSize -> (integer)\n\nA value that specifies the maximum size (in KB) of any .csv file to be created while migrating to an S3 target during full load.\n\nThe default value is 1,048,576 KB (1 GB). Valid values include 1 to 1,048,576.\n\nRfc4180 -> (boolean)\n\nFor an S3 source, when this value is set to true or y , each leading double quotation mark has to be followed by an ending double quotation mark. This formatting complies with RFC 4180. When this value is set to false or n , string literals are copied to the target as is. In this case, a delimiter (row or column) signals the end of the field. Thus, you can’t use a delimiter as part of the string, because it signals the end of the value.\n\nFor an S3 target, an optional parameter used to set behavior to comply with RFC 4180 for data migrated to Amazon S3 using .csv file format only. When this value is set to true or y using Amazon S3 as a target, if the data has quotation marks or newline characters in it, DMS encloses the entire column with an additional pair of double quotation marks (“). Every quotation mark within the data is repeated twice.\n\nThe default value is true . Valid values include true , false , y , and n .\n\nDmsTransferSettings -> (structure)\n\nThe settings in JSON format for the DMS transfer type of source endpoint.\n\nPossible settings include the following:\n\nServiceAccessRoleArn - - The Amazon Resource Name (ARN) used by the service access IAM role. The role must allow the iam:PassRole action.\n\nBucketName - The name of the S3 bucket to use.\n\nShorthand syntax for these settings is as follows: ServiceAccessRoleArn=string,BucketName=string,\n\nJSON syntax for these settings is as follows: { \"ServiceAccessRoleArn\": \"string\", \"BucketName\": \"string\"}\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service access IAM role. The role must allow the iam:PassRole action.\n\nBucketName -> (string)\n\nThe name of the S3 bucket to use.\n\nMongoDbSettings -> (structure)\n\nThe settings for the MongoDB source endpoint. For more information, see the MongoDbSettings structure.\n\nUsername -> (string)\n\nThe user name you use to access the MongoDB source endpoint.\n\nPassword -> (string)\n\nThe password for the user account you use to access the MongoDB source endpoint.\n\nServerName -> (string)\n\nThe name of the server on the MongoDB source endpoint.\n\nPort -> (integer)\n\nThe port value for the MongoDB source endpoint.\n\nDatabaseName -> (string)\n\nThe database name on the MongoDB source endpoint.\n\nAuthType -> (string)\n\nThe authentication type you use to access the MongoDB source endpoint.\n\nWhen when set to \"no\" , user name and password parameters are not used and can be empty.\n\nAuthMechanism -> (string)\n\nThe authentication mechanism you use to access the MongoDB source endpoint.\n\nFor the default value, in MongoDB version 2.x, \"default\" is \"mongodb_cr\" . For MongoDB version 3.x or later, \"default\" is \"scram_sha_1\" . This setting isn’t used when AuthType is set to \"no\" .\n\nNestingLevel -> (string)\n\nSpecifies either document or table mode.\n\nDefault value is \"none\" . Specify \"none\" to use document mode. Specify \"one\" to use table mode.\n\nExtractDocId -> (string)\n\nSpecifies the document ID. Use this setting when NestingLevel is set to \"none\" .\n\nDefault value is \"false\" .\n\nDocsToInvestigate -> (string)\n\nIndicates the number of documents to preview to determine the document organization. Use this setting when NestingLevel is set to \"one\" .\n\nMust be a positive value greater than 0 . Default value is 1000 .\n\nAuthSource -> (string)\n\nThe MongoDB database name. This setting isn’t used when AuthType is set to \"no\" .\n\nThe default is \"admin\" .\n\nKmsKeyId -> (string)\n\nThe KMS key identifier that is used to encrypt the content on the replication instance. If you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key. KMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the MongoDB endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the MongoDB endpoint connection details.\n\nKinesisSettings -> (structure)\n\nThe settings for the Amazon Kinesis target endpoint. For more information, see the KinesisSettings structure.\n\nStreamArn -> (string)\n\nThe Amazon Resource Name (ARN) for the Amazon Kinesis Data Streams endpoint.\n\nMessageFormat -> (string)\n\nThe output format for the records created on the endpoint. The message format is JSON (default) or JSON_UNFORMATTED (a single line with no tab).\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) for the IAM role that DMS uses to write to the Kinesis data stream. The role must allow the iam:PassRole action.\n\nIncludeTransactionDetails -> (boolean)\n\nProvides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for transaction_id , previous transaction_id , and transaction_record_id (the record offset within a transaction). The default is false .\n\nIncludePartitionValue -> (boolean)\n\nShows the partition value within the Kinesis message output, unless the partition type is schema-table-type . The default is false .\n\nPartitionIncludeSchemaTable -> (boolean)\n\nPrefixes schema and table names to partition values, when the partition type is primary-key-type . Doing this increases data distribution among Kinesis shards. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same shard, which causes throttling. The default is false .\n\nIncludeTableAlterOperations -> (boolean)\n\nIncludes any data definition language (DDL) operations that change the table in the control data, such as rename-table , drop-table , add-column , drop-column , and rename-column . The default is false .\n\nIncludeControlDetails -> (boolean)\n\nShows detailed control information for table definition, column definition, and table and column changes in the Kinesis message output. The default is false .\n\nIncludeNullAndEmpty -> (boolean)\n\nInclude NULL and empty columns for records migrated to the endpoint. The default is false .\n\nNoHexPrefix -> (boolean)\n\nSet this optional parameter to true to avoid adding a ‘0x’ prefix to raw data in hexadecimal format. For example, by default, DMS adds a ‘0x’ prefix to the LOB column type in hexadecimal format moving from an Oracle source to an Amazon Kinesis target. Use the NoHexPrefix endpoint setting to enable migration of RAW data type columns without adding the ‘0x’ prefix.\n\nKafkaSettings -> (structure)\n\nThe settings for the Apache Kafka target endpoint. For more information, see the KafkaSettings structure.\n\nBroker -> (string)\n\nA comma-separated list of one or more broker locations in your Kafka cluster that host your Kafka instance. Specify each broker location in the form `` broker-hostname-or-ip :port `` . For example, \"ec2-12-345-678-901.compute-1.amazonaws.com:2345\" . For more information and examples of specifying a list of broker locations, see Using Apache Kafka as a target for Database Migration Service in the Database Migration Service User Guide .\n\nTopic -> (string)\n\nThe topic to which you migrate the data. If you don’t specify a topic, DMS specifies \"kafka-default-topic\" as the migration topic.\n\nMessageFormat -> (string)\n\nThe output format for the records created on the endpoint. The message format is JSON (default) or JSON_UNFORMATTED (a single line with no tab).\n\nIncludeTransactionDetails -> (boolean)\n\nProvides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for transaction_id , previous transaction_id , and transaction_record_id (the record offset within a transaction). The default is false .\n\nIncludePartitionValue -> (boolean)\n\nShows the partition value within the Kafka message output unless the partition type is schema-table-type . The default is false .\n\nPartitionIncludeSchemaTable -> (boolean)\n\nPrefixes schema and table names to partition values, when the partition type is primary-key-type . Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. The default is false .\n\nIncludeTableAlterOperations -> (boolean)\n\nIncludes any data definition language (DDL) operations that change the table in the control data, such as rename-table , drop-table , add-column , drop-column , and rename-column . The default is false .\n\nIncludeControlDetails -> (boolean)\n\nShows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. The default is false .\n\nMessageMaxBytes -> (integer)\n\nThe maximum size in bytes for records created on the endpoint The default is 1,000,000.\n\nIncludeNullAndEmpty -> (boolean)\n\nInclude NULL and empty columns for records migrated to the endpoint. The default is false .\n\nSecurityProtocol -> (string)\n\nSet secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include ssl-encryption , ssl-authentication , and sasl-ssl . sasl-ssl requires SaslUsername and SaslPassword .\n\nSslClientCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) of the client certificate used to securely connect to a Kafka target endpoint.\n\nSslClientKeyArn -> (string)\n\nThe Amazon Resource Name (ARN) for the client private key used to securely connect to a Kafka target endpoint.\n\nSslClientKeyPassword -> (string)\n\nThe password for the client private key used to securely connect to a Kafka target endpoint.\n\nSslCaCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the private certificate authority (CA) cert that DMS uses to securely connect to your Kafka target endpoint.\n\nSaslUsername -> (string)\n\nThe secure user name you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n\nSaslPassword -> (string)\n\nThe secure password you created when you first set up your MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n\nNoHexPrefix -> (boolean)\n\nSet this optional parameter to true to avoid adding a ‘0x’ prefix to raw data in hexadecimal format. For example, by default, DMS adds a ‘0x’ prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the NoHexPrefix endpoint setting to enable migration of RAW data type columns without adding the ‘0x’ prefix.\n\nElasticsearchSettings -> (structure)\n\nThe settings for the Elasticsearch source endpoint. For more information, see the ElasticsearchSettings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the iam:PassRole action.\n\nEndpointUri -> (string)\n\nThe endpoint for the Elasticsearch cluster. DMS uses HTTPS if a transport protocol (http/https) is not specified.\n\nFullLoadErrorPercentage -> (integer)\n\nThe maximum percentage of records that can fail to be written before a full load operation stops.\n\nTo avoid early failure, this counter is only effective after 1000 records are transferred. Elasticsearch also has the concept of error monitoring during the last 10 minutes of an Observation Window. If transfer of all records fail in the last 10 minutes, the full load operation stops.\n\nErrorRetryDuration -> (integer)\n\nThe maximum number of seconds for which DMS retries failed API requests to the Elasticsearch cluster.\n\nNeptuneSettings -> (structure)\n\nThe settings for the Amazon Neptune target endpoint. For more information, see the NeptuneSettings structure.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the service role that you created for the Neptune target endpoint. The role must allow the iam:PassRole action. For more information, see Creating an IAM Service Role for Accessing Amazon Neptune as a Target in the Database Migration Service User Guide.\n\nS3BucketName -> (string)\n\nThe name of the Amazon S3 bucket where DMS can temporarily store migrated graph data in .csv files before bulk-loading it to the Neptune target database. DMS maps the SQL source data to graph data before storing it in these .csv files.\n\nS3BucketFolder -> (string)\n\nA folder path where you want DMS to store migrated graph data in the S3 bucket specified by S3BucketName\n\nErrorRetryDuration -> (integer)\n\nThe number of milliseconds for DMS to wait to retry a bulk-load of migrated graph data to the Neptune target database before raising an error. The default is 250.\n\nMaxFileSize -> (integer)\n\nThe maximum size in kilobytes of migrated graph data stored in a .csv file before DMS bulk-loads the data to the Neptune target database. The default is 1,048,576 KB. If the bulk load is successful, DMS clears the bucket, ready to store the next batch of migrated graph data.\n\nMaxRetryCount -> (integer)\n\nThe number of times for DMS to retry a bulk load of migrated graph data to the Neptune target database before raising an error. The default is 5.\n\nIamAuthEnabled -> (boolean)\n\nIf you want Identity and Access Management (IAM) authorization enabled for this endpoint, set this parameter to true . Then attach the appropriate IAM policy document to your service role specified by ServiceAccessRoleArn . The default is false .\n\nRedshiftSettings -> (structure)\n\nSettings for the Amazon Redshift endpoint.\n\nAcceptAnyDate -> (boolean)\n\nA value that indicates to allow any date format, including invalid formats such as 00/00/00 00:00:00, to be loaded without generating an error. You can choose true or false (the default).\n\nThis parameter applies only to TIMESTAMP and DATE columns. Always use ACCEPTANYDATE with the DATEFORMAT parameter. If the date format for the data doesn’t match the DATEFORMAT specification, Amazon Redshift inserts a NULL value into that field.\n\nAfterConnectScript -> (string)\n\nCode to run after connecting. This parameter should contain the code itself, not the name of a file containing the code.\n\nBucketFolder -> (string)\n\nAn S3 folder where the comma-separated-value (.csv) files are stored before being uploaded to the target Redshift cluster.\n\nFor full load mode, DMS converts source records into .csv files and loads them to the BucketFolder/TableID path. DMS uses the Redshift COPY command to upload the .csv files to the target table. The files are deleted once the COPY operation has finished. For more information, see COPY in the Amazon Redshift Database Developer Guide .\n\nFor change-data-capture (CDC) mode, DMS creates a NetChanges table, and loads the .csv files to this BucketFolder/NetChangesTableID path.\n\nBucketName -> (string)\n\nThe name of the intermediate S3 bucket used to store .csv files before uploading data to Redshift.\n\nCaseSensitiveNames -> (boolean)\n\nIf Amazon Redshift is configured to support case sensitive schema names, set CaseSensitiveNames to true . The default is false .\n\nCompUpdate -> (boolean)\n\nIf you set CompUpdate to true Amazon Redshift applies automatic compression if the table is empty. This applies even if the table columns already have encodings other than RAW . If you set CompUpdate to false , automatic compression is disabled and existing column encodings aren’t changed. The default is true .\n\nConnectionTimeout -> (integer)\n\nA value that sets the amount of time to wait (in milliseconds) before timing out, beginning from when you initially establish a connection.\n\nDatabaseName -> (string)\n\nThe name of the Amazon Redshift data warehouse (service) that you are working with.\n\nDateFormat -> (string)\n\nThe date format that you are using. Valid values are auto (case-sensitive), your date format string enclosed in quotes, or NULL. If this parameter is left unset (NULL), it defaults to a format of ‘YYYY-MM-DD’. Using auto recognizes most strings, even some that aren’t supported when you use a date format string.\n\nIf your date and time values use formats different from each other, set this to auto .\n\nEmptyAsNull -> (boolean)\n\nA value that specifies whether DMS should migrate empty CHAR and VARCHAR fields as NULL. A value of true sets empty CHAR and VARCHAR fields to null. The default is false .\n\nEncryptionMode -> (string)\n\nThe type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either SSE_S3 (the default) or SSE_KMS .\n\nNote\n\nFor the ModifyEndpoint operation, you can change the existing value of the EncryptionMode parameter from SSE_KMS to SSE_S3 . But you can’t change the existing value from SSE_S3 to SSE_KMS .\n\nTo use SSE_S3 , create an Identity and Access Management (IAM) role with a policy that allows \"arn:aws:s3:::*\" to use the following actions: \"s3:PutObject\", \"s3:ListBucket\"\n\nExplicitIds -> (boolean)\n\nThis setting is only valid for a full-load migration task. Set ExplicitIds to true to have tables with IDENTITY columns override their auto-generated values with explicit values loaded from the source data files used to populate the tables. The default is false .\n\nFileTransferUploadStreams -> (integer)\n\nThe number of threads used to upload a single file. This parameter accepts a value from 1 through 64. It defaults to 10.\n\nThe number of parallel streams used to upload a single .csv file to an S3 bucket using S3 Multipart Upload. For more information, see Multipart upload overview .\n\nFileTransferUploadStreams accepts a value from 1 through 64. It defaults to 10.\n\nLoadTimeout -> (integer)\n\nThe amount of time to wait (in milliseconds) before timing out of operations performed by DMS on a Redshift cluster, such as Redshift COPY, INSERT, DELETE, and UPDATE.\n\nMaxFileSize -> (integer)\n\nThe maximum size (in KB) of any .csv file used to load data on an S3 bucket and transfer data to Amazon Redshift. It defaults to 1048576KB (1 GB).\n\nPassword -> (string)\n\nThe password for the user named in the username property.\n\nPort -> (integer)\n\nThe port number for Amazon Redshift. The default value is 5439.\n\nRemoveQuotes -> (boolean)\n\nA value that specifies to remove surrounding quotation marks from strings in the incoming data. All characters within the quotation marks, including delimiters, are retained. Choose true to remove quotation marks. The default is false .\n\nReplaceInvalidChars -> (string)\n\nA list of characters that you want to replace. Use with ReplaceChars .\n\nReplaceChars -> (string)\n\nA value that specifies to replaces the invalid characters specified in ReplaceInvalidChars , substituting the specified characters instead. The default is \"?\" .\n\nServerName -> (string)\n\nThe name of the Amazon Redshift cluster you are using.\n\nServiceAccessRoleArn -> (string)\n\nThe Amazon Resource Name (ARN) of the IAM role that has access to the Amazon Redshift service. The role must allow the iam:PassRole action.\n\nServerSideEncryptionKmsKeyId -> (string)\n\nThe KMS key ID. If you are using SSE_KMS for the EncryptionMode , provide this key ID. The key that you use needs an attached policy that enables IAM user permissions and allows use of the key.\n\nTimeFormat -> (string)\n\nThe time format that you want to use. Valid values are auto (case-sensitive), 'timeformat_string' , 'epochsecs' , or 'epochmillisecs' . It defaults to 10. Using auto recognizes most strings, even some that aren’t supported when you use a time format string.\n\nIf your date and time values use formats different from each other, set this parameter to auto .\n\nTrimBlanks -> (boolean)\n\nA value that specifies to remove the trailing white space characters from a VARCHAR string. This parameter applies only to columns with a VARCHAR data type. Choose true to remove unneeded white space. The default is false .\n\nTruncateColumns -> (boolean)\n\nA value that specifies to truncate data in columns to the appropriate number of characters, so that the data fits in the column. This parameter applies only to columns with a VARCHAR or CHAR data type, and rows with a size of 4 MB or less. Choose true to truncate data. The default is false .\n\nUsername -> (string)\n\nAn Amazon Redshift user name for a registered user.\n\nWriteBufferSize -> (integer)\n\nThe size (in KB) of the in-memory file write buffer used when generating .csv files on the local disk at the DMS replication instance. The default value is 1000 (buffer size is 1000KB).\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Amazon Redshift endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Amazon Redshift endpoint connection details.\n\nPostgreSQLSettings -> (structure)\n\nThe settings for the PostgreSQL source and target endpoint. For more information, see the PostgreSQLSettings structure.\n\nAfterConnectScript -> (string)\n\nFor use with change data capture (CDC) only, this attribute has DMS bypass foreign keys and user triggers to reduce the time it takes to bulk load data.\n\nExample: afterConnectScript=SET session_replication_role='replica'\n\nCaptureDdls -> (boolean)\n\nTo capture DDL events, DMS creates various artifacts in the PostgreSQL database when the task starts. You can later remove these artifacts.\n\nIf this value is set to N , you don’t have to create tables or triggers on the source database.\n\nMaxFileSize -> (integer)\n\nSpecifies the maximum size (in KB) of any .csv file used to transfer data to PostgreSQL.\n\nExample: maxFileSize=512\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nDdlArtifactsSchema -> (string)\n\nThe schema in which the operational DDL database artifacts are created.\n\nExample: ddlArtifactsSchema=xyzddlschema;\n\nExecuteTimeout -> (integer)\n\nSets the client statement timeout for the PostgreSQL instance, in seconds. The default value is 60 seconds.\n\nExample: executeTimeout=100;\n\nFailTasksOnLobTruncation -> (boolean)\n\nWhen set to true , this value causes a task to fail if the actual size of a LOB column is greater than the specified LobMaxSize .\n\nIf task is set to Limited LOB mode and this option is set to true, the task fails instead of truncating the LOB data.\n\nHeartbeatEnable -> (boolean)\n\nThe write-ahead log (WAL) heartbeat feature mimics a dummy transaction. By doing this, it prevents idle logical replication slots from holding onto old WAL logs, which can result in storage full situations on the source. This heartbeat keeps restart_lsn moving and prevents storage full scenarios.\n\nHeartbeatSchema -> (string)\n\nSets the schema in which the heartbeat artifacts are created.\n\nHeartbeatFrequency -> (integer)\n\nSets the WAL heartbeat frequency (in minutes).\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSlotName -> (string)\n\nSets the name of a previously created logical replication slot for a change data capture (CDC) load of the PostgreSQL source instance.\n\nWhen used with the CdcStartPosition request parameter for the DMS API , this attribute also makes it possible to use native CDC start points. DMS verifies that the specified logical replication slot exists before starting the CDC load task. It also verifies that the task was created with a valid setting of CdcStartPosition . If the specified slot doesn’t exist or the task doesn’t have a valid CdcStartPosition setting, DMS raises an error.\n\nFor more information about setting the CdcStartPosition request parameter, see Determining a CDC native start point in the Database Migration Service User Guide . For more information about using CdcStartPosition , see CreateReplicationTask , StartReplicationTask , and ModifyReplicationTask .\n\nPluginName -> (string)\n\nSpecifies the plugin to use to create a replication slot.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the PostgreSQL endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the PostgreSQL endpoint connection details.\n\nMySQLSettings -> (structure)\n\nThe settings for the MySQL source and target endpoint. For more information, see the MySQLSettings structure.\n\nAfterConnectScript -> (string)\n\nSpecifies a script to run immediately after DMS connects to the endpoint. The migration task continues running regardless if the SQL statement succeeds or fails.\n\nFor this parameter, provide the code of the script itself, not the name of a file containing the script.\n\nCleanSourceMetadataOnMismatch -> (boolean)\n\nAdjusts the behavior of DMS when migrating from an SQL Server source database that is hosted as part of an Always On availability group cluster. If you need DMS to poll all the nodes in the Always On cluster for transaction backups, set this attribute to false .\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint. For a MySQL source or target endpoint, don’t explicitly specify the database using the DatabaseName request parameter on either the CreateEndpoint or ModifyEndpoint API call. Specifying DatabaseName when you create or modify a MySQL endpoint replicates all the task tables to this single database. For MySQL endpoints, you specify the database only when you specify the schema in the table-mapping rules of the DMS task.\n\nEventsPollInterval -> (integer)\n\nSpecifies how often to check the binary log for new changes/events when the database is idle.\n\nExample: eventsPollInterval=5;\n\nIn the example, DMS checks for changes in the binary logs every five seconds.\n\nTargetDbType -> (string)\n\nSpecifies where to migrate source tables on the target, either to a single database or multiple databases.\n\nExample: targetDbType=MULTIPLE_DATABASES\n\nMaxFileSize -> (integer)\n\nSpecifies the maximum size (in KB) of any .csv file used to transfer data to a MySQL-compatible database.\n\nExample: maxFileSize=512\n\nParallelLoadThreads -> (integer)\n\nImproves performance when loading data into the MySQL-compatible target database. Specifies how many threads to use to load the data into the MySQL-compatible target database. Setting a large number of threads can have an adverse effect on database performance, because a separate connection is required for each thread.\n\nExample: parallelLoadThreads=1\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nServerTimezone -> (string)\n\nSpecifies the time zone for the source MySQL database.\n\nExample: serverTimezone=US/Pacific;\n\nNote: Do not enclose time zones in single quotes.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the MySQL endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the MySQL endpoint connection details.\n\nOracleSettings -> (structure)\n\nThe settings for the Oracle source and target endpoint. For more information, see the OracleSettings structure.\n\nAddSupplementalLogging -> (boolean)\n\nSet this attribute to set up table-level supplemental logging for the Oracle database. This attribute enables PRIMARY KEY supplemental logging on all tables selected for a migration task.\n\nIf you use this option, you still need to enable database-level supplemental logging.\n\nArchivedLogDestId -> (integer)\n\nSpecifies the ID of the destination for the archived redo logs. This value should be the same as a number in the dest_id column of the v$archived_log view. If you work with an additional redo log destination, use the AdditionalArchivedLogDestId option to specify the additional destination ID. Doing this improves performance by ensuring that the correct logs are accessed from the outset.\n\nAdditionalArchivedLogDestId -> (integer)\n\nSet this attribute with ArchivedLogDestId in a primary/ standby setup. This attribute is useful in the case of a switchover. In this case, DMS needs to know which destination to get archive redo logs from to read changes. This need arises because the previous primary instance is now a standby instance after switchover.\n\nAlthough DMS supports the use of the Oracle RESETLOGS option to open the database, never use RESETLOGS unless necessary. For additional information about RESETLOGS , see RMAN Data Repair Concepts in the Oracle Database Backup and Recovery User’s Guide .\n\nExtraArchivedLogDestIds -> (list)\n\nSpecifies the IDs of one more destinations for one or more archived redo logs. These IDs are the values of the dest_id column in the v$archived_log view. Use this setting with the archivedLogDestId extra connection attribute in a primary-to-single setup or a primary-to-multiple-standby setup.\n\nThis setting is useful in a switchover when you use an Oracle Data Guard database as a source. In this case, DMS needs information about what destination to get archive redo logs from to read changes. DMS needs this because after the switchover the previous primary is a standby instance. For example, in a primary-to-single standby setup you might apply the following settings.\n\narchivedLogDestId=1; ExtraArchivedLogDestIds=[2]\n\nIn a primary-to-multiple-standby setup, you might apply the following settings.\n\narchivedLogDestId=1; ExtraArchivedLogDestIds=[2,3,4]\n\nAlthough DMS supports the use of the Oracle RESETLOGS option to open the database, never use RESETLOGS unless it’s necessary. For more information about RESETLOGS , see RMAN Data Repair Concepts in the Oracle Database Backup and Recovery User’s Guide .\n\n(integer)\n\nAllowSelectNestedTables -> (boolean)\n\nSet this attribute to true to enable replication of Oracle tables containing columns that are nested tables or defined types.\n\nParallelAsmReadThreads -> (integer)\n\nSet this attribute to change the number of threads that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 2 (the default) and 8 (the maximum). Use this attribute together with the readAheadBlocks attribute.\n\nReadAheadBlocks -> (integer)\n\nSet this attribute to change the number of read-ahead blocks that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 1000 (the default) and 200,000 (the maximum).\n\nAccessAlternateDirectly -> (boolean)\n\nSet this attribute to false in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to not access redo logs through any specified path prefix replacement using direct file access.\n\nUseAlternateFolderForOnline -> (boolean)\n\nSet this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to use any specified prefix replacement to access all online redo logs.\n\nOraclePathPrefix -> (string)\n\nSet this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the default Oracle root used to access the redo logs.\n\nUsePathPrefix -> (string)\n\nSet this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the path prefix used to replace the default Oracle root to access the redo logs.\n\nReplacePathPrefix -> (boolean)\n\nSet this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This setting tells DMS instance to replace the default Oracle root with the specified usePathPrefix setting to access the redo logs.\n\nEnableHomogenousTablespace -> (boolean)\n\nSet this attribute to enable homogenous tablespace replication and create existing tables or indexes under the same tablespace on the target.\n\nDirectPathNoLog -> (boolean)\n\nWhen set to true , this attribute helps to increase the commit rate on the Oracle target database by writing directly to tables and not writing a trail to database logs.\n\nArchivedLogsOnly -> (boolean)\n\nWhen this field is set to Y , DMS only accesses the archived redo logs. If the archived redo logs are stored on Oracle ASM only, the DMS user account needs to be granted ASM privileges.\n\nAsmPassword -> (string)\n\nFor an Oracle source endpoint, your Oracle Automatic Storage Management (ASM) password. You can set this value from the `` asm_user_password `` value. You set this value as part of the comma-separated value that you set to the Password request parameter when you create the endpoint to access transaction logs using Binary Reader. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nAsmServer -> (string)\n\nFor an Oracle source endpoint, your ASM server address. You can set this value from the asm_server value. You set asm_server as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nAsmUser -> (string)\n\nFor an Oracle source endpoint, your ASM user name. You can set this value from the asm_user value. You set asm_user as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see Configuration for change data capture (CDC) on an Oracle source database .\n\nCharLengthSemantics -> (string)\n\nSpecifies whether the length of a character column is in bytes or in characters. To indicate that the character column length is in characters, set this attribute to CHAR . Otherwise, the character column length is in bytes.\n\nExample: charLengthSemantics=CHAR;\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nDirectPathParallelLoad -> (boolean)\n\nWhen set to true , this attribute specifies a parallel load when useDirectPathFullLoad is set to Y . This attribute also only applies when you use the DMS parallel load feature. Note that the target table cannot have any constraints or indexes.\n\nFailTasksOnLobTruncation -> (boolean)\n\nWhen set to true , this attribute causes a task to fail if the actual size of an LOB column is greater than the specified LobMaxSize .\n\nIf a task is set to limited LOB mode and this option is set to true , the task fails instead of truncating the LOB data.\n\nNumberDatatypeScale -> (integer)\n\nSpecifies the number scale. You can select a scale up to 38, or you can select FLOAT. By default, the NUMBER data type is converted to precision 38, scale 10.\n\nExample: numberDataTypeScale=12\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nReadTableSpaceName -> (boolean)\n\nWhen set to true , this attribute supports tablespace replication.\n\nRetryInterval -> (integer)\n\nSpecifies the number of seconds that the system waits before resending a query.\n\nExample: retryInterval=6;\n\nSecurityDbEncryption -> (string)\n\nFor an Oracle source endpoint, the transparent data encryption (TDE) password required by AWM DMS to access Oracle redo logs encrypted by TDE using Binary Reader. It is also the `` TDE_Password `` part of the comma-separated value you set to the Password request parameter when you create the endpoint. The SecurityDbEncryptian setting is related to this SecurityDbEncryptionName setting. For more information, see Supported encryption methods for using Oracle as a source for DMS in the Database Migration Service User Guide .\n\nSecurityDbEncryptionName -> (string)\n\nFor an Oracle source endpoint, the name of a key used for the transparent data encryption (TDE) of the columns and tablespaces in an Oracle source database that is encrypted using TDE. The key value is the value of the SecurityDbEncryption setting. For more information on setting the key name value of SecurityDbEncryptionName , see the information and example for setting the securityDbEncryptionName extra connection attribute in Supported encryption methods for using Oracle as a source for DMS in the Database Migration Service User Guide .\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nSpatialDataOptionToGeoJsonFunctionName -> (string)\n\nUse this attribute to convert SDO_GEOMETRY to GEOJSON format. By default, DMS calls the SDO2GEOJSON custom function if present and accessible. Or you can create your own custom function that mimics the operation of SDOGEOJSON and set SpatialDataOptionToGeoJsonFunctionName to call it instead.\n\nStandbyDelayTime -> (integer)\n\nUse this attribute to specify a time in minutes for the delay in standby sync. If the source is an Oracle Active Data Guard standby database, use this attribute to specify the time lag between primary and standby databases.\n\nIn DMS, you can create an Oracle CDC task that uses an Active Data Guard standby instance as a source for replicating ongoing changes. Doing this eliminates the need to connect to an active database that might be in production.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nUseBFile -> (boolean)\n\nSet this attribute to Y to capture change data using the Binary Reader utility. Set UseLogminerReader to N to set this attribute to Y. To use Binary Reader with Amazon RDS for Oracle as the source, you set additional attributes. For more information about using this setting with Oracle Automatic Storage Management (ASM), see Using Oracle LogMiner or DMS Binary Reader for CDC .\n\nUseDirectPathFullLoad -> (boolean)\n\nSet this attribute to Y to have DMS use a direct path full load. Specify this value to use the direct path protocol in the Oracle Call Interface (OCI). By using this OCI protocol, you can bulk-load Oracle target tables during a full load.\n\nUseLogminerReader -> (boolean)\n\nSet this attribute to Y to capture change data using the Oracle LogMiner utility (the default). Set this attribute to N if you want to access the redo logs as a binary file. When you set UseLogminerReader to N, also set UseBfile to Y. For more information on this setting and using Oracle ASM, see Using Oracle LogMiner or DMS Binary Reader for CDC in the DMS User Guide .\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Oracle endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Oracle endpoint connection details.\n\nSecretsManagerOracleAsmAccessRoleArn -> (string)\n\nRequired only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the SecretsManagerOracleAsmSecret . This SecretsManagerOracleAsmSecret has the secret value that allows access to the Oracle ASM of the endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerOracleAsmSecretId . Or you can specify clear-text values for AsmUserName , AsmPassword , and AsmServerName . You can’t specify both. For more information on creating this SecretsManagerOracleAsmSecret and the SecretsManagerOracleAsmAccessRoleArn and SecretsManagerOracleAsmSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerOracleAsmSecretId -> (string)\n\nRequired only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN, partial ARN, or friendly name of the SecretsManagerOracleAsmSecret that contains the Oracle ASM connection details for the Oracle endpoint.\n\nSybaseSettings -> (structure)\n\nThe settings for the SAP ASE source and target endpoint. For more information, see the SybaseSettings structure.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the SAP ASE endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the SAP SAE endpoint connection details.\n\nMicrosoftSQLServerSettings -> (structure)\n\nThe settings for the Microsoft SQL Server source and target endpoint. For more information, see the MicrosoftSQLServerSettings structure.\n\nPort -> (integer)\n\nEndpoint TCP port.\n\nBcpPacketSize -> (integer)\n\nThe maximum size of the packets (in bytes) used to transfer data using BCP.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nControlTablesFileGroup -> (string)\n\nSpecifies a file group for the DMS internal tables. When the replication task starts, all the internal DMS control tables (awsdms_ apply_exception, awsdms_apply, awsdms_changes) are created for the specified file group.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nQuerySingleAlwaysOnNode -> (boolean)\n\nCleans and recreates table metadata information on the replication instance when a mismatch occurs. An example is a situation where running an alter DDL statement on a table might result in different information about the table cached in the replication instance.\n\nReadBackupOnly -> (boolean)\n\nWhen this attribute is set to Y , DMS only reads changes from transaction log backups and doesn’t read from the active transaction log file during ongoing replication. Setting this parameter to Y enables you to control active transaction log file growth during full load and ongoing replication tasks. However, it can add some source latency to ongoing replication.\n\nSafeguardPolicy -> (string)\n\nUse this attribute to minimize the need to access the backup log and enable DMS to prevent truncation using one of the following two methods.\n\nStart transactions in the database: This is the default method. When this method is used, DMS prevents TLOG truncation by mimicking a transaction in the database. As long as such a transaction is open, changes that appear after the transaction started aren’t truncated. If you need Microsoft Replication to be enabled in your database, then you must choose this method.\n\nExclusively use sp_repldone within a single task : When this method is used, DMS reads the changes and then uses sp_repldone to mark the TLOG transactions as ready for truncation. Although this method doesn’t involve any transactional activities, it can only be used when Microsoft Replication isn’t running. Also, when using this method, only one DMS task can access the database at any given time. Therefore, if you need to run parallel DMS tasks against the same database, use the default method.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nUseBcpFullLoad -> (boolean)\n\nUse this to attribute to transfer data for full-load operations using BCP. When the target table contains an identity column that does not exist in the source table, you must disable the use BCP for loading table option.\n\nUseThirdPartyBackupDevice -> (boolean)\n\nWhen this attribute is set to Y , DMS processes third-party transaction log backups if they are created in native format.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the SQL Server endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the SQL Server endpoint connection details.\n\nIBMDb2Settings -> (structure)\n\nThe settings for the IBM Db2 LUW source endpoint. For more information, see the IBMDb2Settings structure.\n\nDatabaseName -> (string)\n\nDatabase name for the endpoint.\n\nPassword -> (string)\n\nEndpoint connection password.\n\nPort -> (integer)\n\nEndpoint TCP port. The default value is 50000.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nSetDataCaptureChanges -> (boolean)\n\nEnables ongoing replication (CDC) as a BOOLEAN value. The default is true.\n\nCurrentLsn -> (string)\n\nFor ongoing replication (CDC), use CurrentLSN to specify a log sequence number (LSN) where you want the replication to start.\n\nMaxKBytesPerRead -> (integer)\n\nMaximum number of bytes per read, as a NUMBER value. The default is 64 KB.\n\nUsername -> (string)\n\nEndpoint connection user name.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the Db2 LUW endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the Db2 LUW endpoint connection details.\n\nDocDbSettings -> (structure)\n\nProvides information that defines a DocumentDB endpoint.\n\nUsername -> (string)\n\nThe user name you use to access the DocumentDB source endpoint.\n\nPassword -> (string)\n\nThe password for the user account you use to access the DocumentDB source endpoint.\n\nServerName -> (string)\n\nThe name of the server on the DocumentDB source endpoint.\n\nPort -> (integer)\n\nThe port value for the DocumentDB source endpoint.\n\nDatabaseName -> (string)\n\nThe database name on the DocumentDB source endpoint.\n\nNestingLevel -> (string)\n\nSpecifies either document or table mode.\n\nDefault value is \"none\" . Specify \"none\" to use document mode. Specify \"one\" to use table mode.\n\nExtractDocId -> (boolean)\n\nSpecifies the document ID. Use this setting when NestingLevel is set to \"none\" .\n\nDefault value is \"false\" .\n\nDocsToInvestigate -> (integer)\n\nIndicates the number of documents to preview to determine the document organization. Use this setting when NestingLevel is set to \"one\" .\n\nMust be a positive value greater than 0 . Default value is 1000 .\n\nKmsKeyId -> (string)\n\nThe KMS key identifier that is used to encrypt the content on the replication instance. If you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key. KMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nSecretsManagerAccessRoleArn -> (string)\n\nThe full Amazon Resource Name (ARN) of the IAM role that specifies DMS as the trusted entity and grants the required permissions to access the value in SecretsManagerSecret . The role must allow the iam:PassRole action. SecretsManagerSecret has the value of the Amazon Web Services Secrets Manager secret that allows access to the DocumentDB endpoint.\n\nNote\n\nYou can specify one of two sets of values for these permissions. You can specify the values for this setting and SecretsManagerSecretId . Or you can specify clear-text values for UserName , Password , ServerName , and Port . You can’t specify both. For more information on creating this SecretsManagerSecret and the SecretsManagerAccessRoleArn and SecretsManagerSecretId required to access it, see Using secrets to access Database Migration Service resources in the Database Migration Service User Guide .\n\nSecretsManagerSecretId -> (string)\n\nThe full ARN, partial ARN, or friendly name of the SecretsManagerSecret that contains the DocumentDB endpoint connection details.\n\nRedisSettings -> (structure)\n\nThe settings for the Redis target endpoint. For more information, see the RedisSettings structure.\n\nServerName -> (string)\n\nFully qualified domain name of the endpoint.\n\nPort -> (integer)\n\nTransmission Control Protocol (TCP) port for the endpoint.\n\nSslSecurityProtocol -> (string)\n\nThe connection to a Redis target endpoint using Transport Layer Security (TLS). Valid values include plaintext and ssl-encryption . The default is ssl-encryption . The ssl-encryption option makes an encrypted connection. Optionally, you can identify an Amazon Resource Name (ARN) for an SSL certificate authority (CA) using the SslCaCertificateArn setting. If an ARN isn’t given for a CA, DMS uses the Amazon root CA.\n\nThe plaintext option doesn’t provide Transport Layer Security (TLS) encryption for traffic between endpoint and database.\n\nAuthType -> (string)\n\nThe type of authentication to perform when connecting to a Redis target. Options include none , auth-token , and auth-role . The auth-token option requires an AuthPassword value to be provided. The auth-role option requires AuthUserName and AuthPassword values to be provided.\n\nAuthUserName -> (string)\n\nThe user name provided with the auth-role option of the AuthType setting for a Redis target endpoint.\n\nAuthPassword -> (string)\n\nThe password provided with the auth-role and auth-token options of the AuthType setting for a Redis target endpoint.\n\nSslCaCertificateArn -> (string)\n\nThe Amazon Resource Name (ARN) for the certificate authority (CA) that DMS uses to connect to your Redis target endpoint.",
      "command_examples": "Examples\n\nTo modify an endpoint\n\nThe following modify-endpoint example adds an extra connection attribute to an endpoint.\n\naws dms modify-endpoint \\\n    --endpoint-arn \"arn:aws:dms:us-east-1:123456789012:endpoint:GUVAFG34EECUOJ6QVZ56DAHT3U\" \\\n    --extra-connection-attributes \"compressionType=GZIP\"\n\n\nOutput:\n\n{\n    \"Endpoint\": {\n        \"EndpointIdentifier\": \"src-endpoint\",\n        \"EndpointType\": \"SOURCE\",\n        \"EngineName\": \"s3\",\n        \"EngineDisplayName\": \"Amazon S3\",\n        \"ExtraConnectionAttributes\": \"compressionType=GZIP;csvDelimiter=,;csvRowDelimiter=\\\\n;\",\n        \"Status\": \"active\",\n        \"EndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:GUVAFG34EECUOJ6QVZ56DAHT3U\",\n        \"SslMode\": \"none\",\n        \"ServiceAccessRoleArn\": \"arn:aws:iam::123456789012:role/my-s3-access-role\",\n        \"S3Settings\": {\n            \"ServiceAccessRoleArn\": \"arn:aws:iam::123456789012:role/my-s3-access-role\",\n            \"CsvRowDelimiter\": \"\\\\n\",\n            \"CsvDelimiter\": \",\",\n            \"BucketFolder\": \"\",\n            \"BucketName\": \"\",\n            \"CompressionType\": \"GZIP\",\n            \"EnableStatistics\": true\n        }\n    }\n}\n\n\nFor more information, see Working with AWS DMS Endpoints <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Endpoints.html>`__ in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "modify-event-subscription",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/modify-event-subscription.html",
      "command_description": "Description\n\nModifies an existing DMS event notification subscription.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  modify-event-subscription\n--subscription-name <value>\n[--sns-topic-arn <value>]\n[--source-type <value>]\n[--event-categories <value>]\n[--enabled | --no-enabled]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--subscription-name <value>",
        "[--sns-topic-arn <value>]",
        "[--source-type <value>]",
        "[--event-categories <value>]",
        "[--enabled | --no-enabled]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--subscription-name (string)\n\nThe name of the DMS event notification subscription to be modified.\n\n--sns-topic-arn (string)\n\nThe Amazon Resource Name (ARN) of the Amazon SNS topic created for event notification. The ARN is created by Amazon SNS when you create a topic and subscribe to it.\n\n--source-type (string)\n\nThe type of DMS resource that generates the events you want to subscribe to.\n\nValid values: replication-instance | replication-task\n\n--event-categories (list)\n\nA list of event categories for a source type that you want to subscribe to. Use the DescribeEventCategories action to see a list of event categories.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--enabled | --no-enabled (boolean)\n\nA Boolean value; set to true to activate the subscription.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nEventSubscription -> (structure)\n\nThe modified event subscription.\n\nCustomerAwsId -> (string)\n\nThe Amazon Web Services customer account associated with the DMS event notification subscription.\n\nCustSubscriptionId -> (string)\n\nThe DMS event notification subscription Id.\n\nSnsTopicArn -> (string)\n\nThe topic ARN of the DMS event notification subscription.\n\nStatus -> (string)\n\nThe status of the DMS event notification subscription.\n\nConstraints:\n\nCan be one of the following: creating | modifying | deleting | active | no-permission | topic-not-exist\n\nThe status “no-permission” indicates that DMS no longer has permission to post to the SNS topic. The status “topic-not-exist” indicates that the topic was deleted after the subscription was created.\n\nSubscriptionCreationTime -> (string)\n\nThe time the DMS event notification subscription was created.\n\nSourceType -> (string)\n\nThe type of DMS resource that generates events.\n\nValid values: replication-instance | replication-server | security-group | replication-task\n\nSourceIdsList -> (list)\n\nA list of source Ids for the event subscription.\n\n(string)\n\nEventCategoriesList -> (list)\n\nA lists of event categories.\n\n(string)\n\nEnabled -> (boolean)\n\nBoolean value that indicates if the event subscription is enabled.",
      "command_examples": "Examples\n\nTo modify an event subscription\n\nThe following modify-event-subscription example changes the source type of an event subscription.\n\naws dms modify-event-subscription \\\n    --subscription-name \"my-dms-events\" \\\n    --source-type replication-task\n\n\nOutput:\n\n{\n    \"EventSubscription\": {\n        \"CustomerAwsId\": \"123456789012\",\n        \"CustSubscriptionId\": \"my-dms-events\",\n        \"SnsTopicArn\": \"arn:aws:sns:us-east-1:123456789012:my-sns-topic\",\n        \"Status\": \"modifying\",\n        \"SubscriptionCreationTime\": \"2020-05-29 17:04:40.262\",\n        \"SourceType\": \"replication-task\",\n        \"Enabled\": true\n    }\n}\n\n\nFor more information, see Working with Events and Notifications in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "modify-replication-instance",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/modify-replication-instance.html",
      "command_description": "Description\n\nModifies the replication instance to apply new settings. You can change one or more parameters by specifying these parameters and the new values in the request.\n\nSome settings are applied during the maintenance window.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  modify-replication-instance\n--replication-instance-arn <value>\n[--allocated-storage <value>]\n[--apply-immediately | --no-apply-immediately]\n[--replication-instance-class <value>]\n[--vpc-security-group-ids <value>]\n[--preferred-maintenance-window <value>]\n[--multi-az | --no-multi-az]\n[--engine-version <value>]\n[--allow-major-version-upgrade | --no-allow-major-version-upgrade]\n[--auto-minor-version-upgrade | --no-auto-minor-version-upgrade]\n[--replication-instance-identifier <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-instance-arn <value>",
        "[--allocated-storage <value>]",
        "[--apply-immediately | --no-apply-immediately]",
        "[--replication-instance-class <value>]",
        "[--vpc-security-group-ids <value>]",
        "[--preferred-maintenance-window <value>]",
        "[--multi-az | --no-multi-az]",
        "[--engine-version <value>]",
        "[--allow-major-version-upgrade | --no-allow-major-version-upgrade]",
        "[--auto-minor-version-upgrade | --no-auto-minor-version-upgrade]",
        "[--replication-instance-identifier <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-instance-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\n--allocated-storage (integer)\n\nThe amount of storage (in gigabytes) to be allocated for the replication instance.\n\n--apply-immediately | --no-apply-immediately (boolean)\n\nIndicates whether the changes should be applied immediately or during the next maintenance window.\n\n--replication-instance-class (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class. For example to specify the instance class dms.c4.large, set this parameter to \"dms.c4.large\" .\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\n--vpc-security-group-ids (list)\n\nSpecifies the VPC security group to be used with the replication instance. The VPC security group must work with the VPC containing the replication instance.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--preferred-maintenance-window (string)\n\nThe weekly time range (in UTC) during which system maintenance can occur, which might result in an outage. Changing this parameter does not result in an outage, except in the following situation, and the change is asynchronously applied as soon as possible. If moving this window to the current time, there must be at least 30 minutes between the current time and end of the window to ensure pending changes are applied.\n\nDefault: Uses existing setting\n\nFormat: ddd:hh24:mi-ddd:hh24:mi\n\nValid Days: Mon | Tue | Wed | Thu | Fri | Sat | Sun\n\nConstraints: Must be at least 30 minutes\n\n--multi-az | --no-multi-az (boolean)\n\nSpecifies whether the replication instance is a Multi-AZ deployment. You can’t set the AvailabilityZone parameter if the Multi-AZ parameter is set to true .\n\n--engine-version (string)\n\nThe engine version number of the replication instance.\n\nWhen modifying a major engine version of an instance, also set AllowMajorVersionUpgrade to true .\n\n--allow-major-version-upgrade | --no-allow-major-version-upgrade (boolean)\n\nIndicates that major version upgrades are allowed. Changing this parameter does not result in an outage, and the change is asynchronously applied as soon as possible.\n\nThis parameter must be set to true when specifying a value for the EngineVersion parameter that is a different major version than the replication instance’s current version.\n\n--auto-minor-version-upgrade | --no-auto-minor-version-upgrade (boolean)\n\nA value that indicates that minor version upgrades are applied automatically to the replication instance during the maintenance window. Changing this parameter doesn’t result in an outage, except in the case described following. The change is asynchronously applied as soon as possible.\n\nAn outage does result if these factors apply:\n\nThis parameter is set to true during the maintenance window.\n\nA newer minor version is available.\n\nDMS has enabled automatic patching for the given engine version.\n\n--replication-instance-identifier (string)\n\nThe replication instance identifier. This parameter is stored as a lowercase string.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationInstance -> (structure)\n\nThe modified replication instance.\n\nReplicationInstanceIdentifier -> (string)\n\nThe replication instance identifier is a required parameter. This parameter is stored as a lowercase string.\n\nConstraints:\n\nMust contain 1-63 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nExample: myrepinstance\n\nReplicationInstanceClass -> (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class. It is a required parameter, although a default value is pre-selected in the DMS console.\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\nReplicationInstanceStatus -> (string)\n\nThe status of the replication instance. The possible return values include:\n\n\"available\"\n\n\"creating\"\n\n\"deleted\"\n\n\"deleting\"\n\n\"failed\"\n\n\"modifying\"\n\n\"upgrading\"\n\n\"rebooting\"\n\n\"resetting-master-credentials\"\n\n\"storage-full\"\n\n\"incompatible-credentials\"\n\n\"incompatible-network\"\n\n\"maintenance\"\n\nAllocatedStorage -> (integer)\n\nThe amount of storage (in gigabytes) that is allocated for the replication instance.\n\nInstanceCreateTime -> (timestamp)\n\nThe time the replication instance was created.\n\nVpcSecurityGroups -> (list)\n\nThe VPC security group for the instance.\n\n(structure)\n\nDescribes the status of a security group associated with the virtual private cloud (VPC) hosting your replication and DB instances.\n\nVpcSecurityGroupId -> (string)\n\nThe VPC security group ID.\n\nStatus -> (string)\n\nThe status of the VPC security group.\n\nAvailabilityZone -> (string)\n\nThe Availability Zone for the instance.\n\nReplicationSubnetGroup -> (structure)\n\nThe subnet group for the replication instance.\n\nReplicationSubnetGroupIdentifier -> (string)\n\nThe identifier of the replication instance subnet group.\n\nReplicationSubnetGroupDescription -> (string)\n\nA description for the replication subnet group.\n\nVpcId -> (string)\n\nThe ID of the VPC.\n\nSubnetGroupStatus -> (string)\n\nThe status of the subnet group.\n\nSubnets -> (list)\n\nThe subnets that are in the subnet group.\n\n(structure)\n\nIn response to a request by the DescribeReplicationSubnetGroups operation, this object identifies a subnet by its given Availability Zone, subnet identifier, and status.\n\nSubnetIdentifier -> (string)\n\nThe subnet identifier.\n\nSubnetAvailabilityZone -> (structure)\n\nThe Availability Zone of the subnet.\n\nName -> (string)\n\nThe name of the Availability Zone.\n\nSubnetStatus -> (string)\n\nThe status of the subnet.\n\nPreferredMaintenanceWindow -> (string)\n\nThe maintenance window times for the replication instance. Any pending upgrades to the replication instance are performed during this time.\n\nPendingModifiedValues -> (structure)\n\nThe pending modification values.\n\nReplicationInstanceClass -> (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class.\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\nAllocatedStorage -> (integer)\n\nThe amount of storage (in gigabytes) that is allocated for the replication instance.\n\nMultiAZ -> (boolean)\n\nSpecifies whether the replication instance is a Multi-AZ deployment. You can’t set the AvailabilityZone parameter if the Multi-AZ parameter is set to true .\n\nEngineVersion -> (string)\n\nThe engine version number of the replication instance.\n\nMultiAZ -> (boolean)\n\nSpecifies whether the replication instance is a Multi-AZ deployment. You can’t set the AvailabilityZone parameter if the Multi-AZ parameter is set to true .\n\nEngineVersion -> (string)\n\nThe engine version number of the replication instance.\n\nIf an engine version number is not specified when a replication instance is created, the default is the latest engine version available.\n\nWhen modifying a major engine version of an instance, also set AllowMajorVersionUpgrade to true .\n\nAutoMinorVersionUpgrade -> (boolean)\n\nBoolean value indicating if minor version upgrades will be automatically applied to the instance.\n\nKmsKeyId -> (string)\n\nAn KMS key identifier that is used to encrypt the data on the replication instance.\n\nIf you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key.\n\nKMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nReplicationInstanceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\nReplicationInstancePublicIpAddress -> (string)\n\nThe public IP address of the replication instance.\n\nReplicationInstancePrivateIpAddress -> (string)\n\nThe private IP address of the replication instance.\n\nReplicationInstancePublicIpAddresses -> (list)\n\nOne or more public IP addresses for the replication instance.\n\n(string)\n\nReplicationInstancePrivateIpAddresses -> (list)\n\nOne or more private IP addresses for the replication instance.\n\n(string)\n\nPubliclyAccessible -> (boolean)\n\nSpecifies the accessibility options for the replication instance. A value of true represents an instance with a public IP address. A value of false represents an instance with a private IP address. The default value is true .\n\nSecondaryAvailabilityZone -> (string)\n\nThe Availability Zone of the standby replication instance in a Multi-AZ deployment.\n\nFreeUntil -> (timestamp)\n\nThe expiration date of the free replication instance that is part of the Free DMS program.\n\nDnsNameServers -> (string)\n\nThe DNS name servers supported for the replication instance to access your on-premise source or target database.",
      "command_examples": "Examples\n\nTo modify a replication instance\n\nThe following modify-replication-instance example modifies a replication instance so that it uses a Multi-AZ deployment.\n\naws dms modify-replication-instance \\\n     --replication-instance-arn arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE \\\n     --multi-az\n\n\nOutput:\n\n{\n    \"ReplicationInstance\": {\n        \"ReplicationInstanceIdentifier\": \"my-repl-instance\",\n        \"ReplicationInstanceClass\": \"dms.t2.micro\",\n        \"ReplicationInstanceStatus\": \"available\",\n        \"AllocatedStorage\": 5,\n        \"InstanceCreateTime\": 1590011235.952,\n\n        ...output omitted...\n\n        \"PendingModifiedValues\": {\n            \"MultiAZ\": true\n        },\n        \"MultiAZ\": false,\n        \"EngineVersion\": \"3.3.2\",\n        \"AutoMinorVersionUpgrade\": true,\n        \"KmsKeyId\": \"arn:aws:kms:us-east-1:123456789012:key/f7bc0f8e-1a3a-4ace-9faa-e8494fa3921a\",\n\n        ...output omitted...\n\n    }\n}\n\n\nFor more information, see Working with an AWS DMS Replication Instance in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "modify-replication-subnet-group",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/modify-replication-subnet-group.html",
      "command_description": "Description\n\nModifies the settings for the specified replication subnet group.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  modify-replication-subnet-group\n--replication-subnet-group-identifier <value>\n[--replication-subnet-group-description <value>]\n--subnet-ids <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-subnet-group-identifier <value>",
        "[--replication-subnet-group-description <value>]",
        "--subnet-ids <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-subnet-group-identifier (string)\n\nThe name of the replication instance subnet group.\n\n--replication-subnet-group-description (string)\n\nA description for the replication instance subnet group.\n\n--subnet-ids (list)\n\nA list of subnet IDs.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationSubnetGroup -> (structure)\n\nThe modified replication subnet group.\n\nReplicationSubnetGroupIdentifier -> (string)\n\nThe identifier of the replication instance subnet group.\n\nReplicationSubnetGroupDescription -> (string)\n\nA description for the replication subnet group.\n\nVpcId -> (string)\n\nThe ID of the VPC.\n\nSubnetGroupStatus -> (string)\n\nThe status of the subnet group.\n\nSubnets -> (list)\n\nThe subnets that are in the subnet group.\n\n(structure)\n\nIn response to a request by the DescribeReplicationSubnetGroups operation, this object identifies a subnet by its given Availability Zone, subnet identifier, and status.\n\nSubnetIdentifier -> (string)\n\nThe subnet identifier.\n\nSubnetAvailabilityZone -> (structure)\n\nThe Availability Zone of the subnet.\n\nName -> (string)\n\nThe name of the Availability Zone.\n\nSubnetStatus -> (string)\n\nThe status of the subnet.",
      "command_examples": "Examples\n\nTo modify a subnet group\n\nThe following modify-replication-subnet-group example changes the lists of subnets associated with a subnet group.\n\naws dms modify-replication-subnet-group \\\n    --replication-subnet-group-identifier my-subnet-group \\\n    --subnet-id subnet-da327bf6 subnet-bac383e0\n\n\nOutput:\n\n{\n    \"ReplicationSubnetGroup\": {\n        \"ReplicationSubnetGroupIdentifier\": \"my-subnet-group\",\n        \"ReplicationSubnetGroupDescription\": \"my subnet group\",\n        \"VpcId\": \"vpc-136a4c6a\",\n        \"SubnetGroupStatus\": \"Complete\",\n        \"Subnets\": [\n            {\n                \"SubnetIdentifier\": \"subnet-da327bf6\",\n                \"SubnetAvailabilityZone\": {\n                    \"Name\": \"us-east-1a\"\n                },\n                \"SubnetStatus\": \"Active\"\n            },\n            {\n                \"SubnetIdentifier\": \"subnet-bac383e0\",\n                \"SubnetAvailabilityZone\": {\n                    \"Name\": \"us-east-1c\"\n                },\n                \"SubnetStatus\": \"Active\"\n            }\n        ]\n    }\n}\n\n\nFor more information, see Setting Up a Network for a Replication Instance in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "modify-replication-task",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/modify-replication-task.html",
      "command_description": "Description\n\nModifies the specified replication task.\n\nYou can’t modify the task endpoints. The task must be stopped before you can modify it.\n\nFor more information about DMS tasks, see Working with Migration Tasks in the Database Migration Service User Guide .\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  modify-replication-task\n--replication-task-arn <value>\n[--replication-task-identifier <value>]\n[--migration-type <value>]\n[--table-mappings <value>]\n[--replication-task-settings <value>]\n[--cdc-start-time <value>]\n[--cdc-start-position <value>]\n[--cdc-stop-position <value>]\n[--task-data <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-task-arn <value>",
        "[--replication-task-identifier <value>]",
        "[--migration-type <value>]",
        "[--table-mappings <value>]",
        "[--replication-task-settings <value>]",
        "[--cdc-start-time <value>]",
        "[--cdc-start-position <value>]",
        "[--cdc-stop-position <value>]",
        "[--task-data <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\n--replication-task-identifier (string)\n\nThe replication task identifier.\n\nConstraints:\n\nMust contain 1-255 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\n--migration-type (string)\n\nThe migration type. Valid values: full-load | cdc | full-load-and-cdc\n\nPossible values:\n\nfull-load\n\ncdc\n\nfull-load-and-cdc\n\n--table-mappings (string)\n\nWhen using the CLI or boto3, provide the path of the JSON file that contains the table mappings. Precede the path with file:// . For example, --table-mappings file://mappingfile.json . When working with the DMS API, provide the JSON as the parameter value.\n\n--replication-task-settings (string)\n\nJSON file that contains settings for the task, such as task metadata settings.\n\n--cdc-start-time (timestamp)\n\nIndicates the start time for a change data capture (CDC) operation. Use either CdcStartTime or CdcStartPosition to specify when you want a CDC operation to start. Specifying both values results in an error.\n\nTimestamp Example: –cdc-start-time “2018-03-08T12:12:12”\n\n--cdc-start-position (string)\n\nIndicates when you want a change data capture (CDC) operation to start. Use either CdcStartPosition or CdcStartTime to specify when you want a CDC operation to start. Specifying both values results in an error.\n\nThe value can be in date, checkpoint, or LSN/SCN format.\n\nDate Example: –cdc-start-position “2018-03-08T12:12:12”\n\nCheckpoint Example: –cdc-start-position “checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93”\n\nLSN Example: –cdc-start-position “mysql-bin-changelog.000024:373”\n\nNote\n\nWhen you use this task setting with a source PostgreSQL database, a logical replication slot should already be created and associated with the source endpoint. You can verify this by setting the slotName extra connection attribute to the name of this logical replication slot. For more information, see Extra Connection Attributes When Using PostgreSQL as a Source for DMS .\n\n--cdc-stop-position (string)\n\nIndicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time.\n\nServer time example: –cdc-stop-position “server_time:2018-02-09T12:12:12”\n\nCommit time example: –cdc-stop-position “commit_time: 2018-02-09T12:12:12 “\n\n--task-data (string)\n\nSupplemental information that the task requires to migrate the data for certain source and target endpoints. For more information, see Specifying Supplemental Data for Task Settings in the Database Migration Service User Guide.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationTask -> (structure)\n\nThe replication task that was modified.\n\nReplicationTaskIdentifier -> (string)\n\nThe user-assigned replication task identifier or name.\n\nConstraints:\n\nMust contain 1-255 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nSourceEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) that uniquely identifies the endpoint.\n\nTargetEndpointArn -> (string)\n\nThe ARN that uniquely identifies the endpoint.\n\nReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance.\n\nMigrationType -> (string)\n\nThe type of migration.\n\nTableMappings -> (string)\n\nTable mappings specified in the task.\n\nReplicationTaskSettings -> (string)\n\nThe settings for the replication task.\n\nStatus -> (string)\n\nThe status of the replication task. This response parameter can return one of the following values:\n\n\"moving\" – The task is being moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"creating\" – The task is being created in response to running the ` CreateReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_CreateReplicationTask.html`__ operation.\n\n\"deleting\" – The task is being deleted in response to running the ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ operation.\n\n\"failed\" – The task failed to successfully complete the database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"failed-move\" – The task failed to move in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"modifying\" – The task definition is being modified in response to running the ` ModifyReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_ModifyReplicationTask.html`__ operation.\n\n\"ready\" – The task is in a ready state where it can respond to other task operations, such as ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ or ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ .\n\n\"running\" – The task is performing a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"starting\" – The task is preparing to perform a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"stopped\" – The task has stopped in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"stopping\" – The task is preparing to stop in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"testing\" – The database migration specified for this task is being tested in response to running either the ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ or the ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation.\n\nNote\n\n` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ is an improved premigration task assessment operation. The ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation assesses data type compatibility only between the source and target database of a given migration task. In contrast, ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ enables you to specify a variety of premigration task assessments in addition to data type compatibility. These assessments include ones for the validity of primary key definitions and likely issues with database migration performance, among others.\n\nLastFailureMessage -> (string)\n\nThe last error (failure) message generated for the replication task.\n\nStopReason -> (string)\n\nThe reason the replication task was stopped. This response parameter can return one of the following values:\n\n\"STOP_REASON_FULL_LOAD_COMPLETED\" – Full-load migration completed.\n\n\"STOP_REASON_CACHED_CHANGES_APPLIED\" – Change data capture (CDC) load completed.\n\n\"STOP_REASON_CACHED_CHANGES_NOT_APPLIED\" – In a full-load and CDC migration, the full load stopped as specified before starting the CDC migration.\n\n\"STOP_REASON_SERVER_TIME\" – The migration stopped at the specified server time.\n\nReplicationTaskCreationDate -> (timestamp)\n\nThe date the replication task was created.\n\nReplicationTaskStartDate -> (timestamp)\n\nThe date the replication task is scheduled to start.\n\nCdcStartPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to start. Use either CdcStartPosition or CdcStartTime to specify when you want the CDC operation to start. Specifying both values results in an error.\n\nThe value can be in date, checkpoint, or LSN/SCN format.\n\nDate Example: –cdc-start-position “2018-03-08T12:12:12”\n\nCheckpoint Example: –cdc-start-position “checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93”\n\nLSN Example: –cdc-start-position “mysql-bin-changelog.000024:373”\n\nCdcStopPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time.\n\nServer time example: –cdc-stop-position “server_time:2018-02-09T12:12:12”\n\nCommit time example: –cdc-stop-position “commit_time: 2018-02-09T12:12:12 “\n\nRecoveryCheckpoint -> (string)\n\nIndicates the last checkpoint that occurred during a change data capture (CDC) operation. You can provide this value to the CdcStartPosition parameter to start a CDC operation that begins at that checkpoint.\n\nReplicationTaskArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\nReplicationTaskStats -> (structure)\n\nThe statistics for the task, including elapsed time, tables loaded, and table errors.\n\nFullLoadProgressPercent -> (integer)\n\nThe percent complete for the full load migration task.\n\nElapsedTimeMillis -> (long)\n\nThe elapsed time of the task, in milliseconds.\n\nTablesLoaded -> (integer)\n\nThe number of tables loaded for this task.\n\nTablesLoading -> (integer)\n\nThe number of tables currently loading for this task.\n\nTablesQueued -> (integer)\n\nThe number of tables queued for this task.\n\nTablesErrored -> (integer)\n\nThe number of errors that have occurred during this task.\n\nFreshStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a target reload.\n\nStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a resume. For more information, see StartReplicationTaskType .\n\nStopDate -> (timestamp)\n\nThe date the replication task was stopped.\n\nFullLoadStartDate -> (timestamp)\n\nThe date the replication task full load was started.\n\nFullLoadFinishDate -> (timestamp)\n\nThe date the replication task full load was completed.\n\nTaskData -> (string)\n\nSupplemental information that the task requires to migrate the data for certain source and target endpoints. For more information, see Specifying Supplemental Data for Task Settings in the Database Migration Service User Guide.\n\nTargetReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance to which this task is moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation. Otherwise, this response parameter isn’t a member of the ReplicationTask object.",
      "command_examples": "Examples\n\nTo modify a replication task\n\nThe following modify-replication-task example changes the table mappings for a task.\n\naws dms modify-replication-task \\\n    --replication-task-arn \"arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\" \\\n    --table-mappings file://table-mappings.json\n\n\nContents of table-mappings.json:\n\n{\n    \"rules\": [\n        {\n            \"rule-type\": \"selection\",\n            \"rule-id\": \"1\",\n            \"rule-name\": \"1\",\n            \"object-locator\": {\n                \"schema-name\": \"prodrep\",\n                \"table-name\": \"ACCT_%\"\n            },\n            \"rule-action\": \"include\",\n            \"filters\": []\n        }\n    ]\n}\n\n\nOutput:\n\n{\n    \"ReplicationTask\": {\n        \"ReplicationTaskIdentifier\": \"moveit2\",\n        \"SourceEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\",\n        \"TargetEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:EOM4SFKCZEYHZBFGAGZT3QEC5U\",\n        \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n        \"MigrationType\": \"full-load\",\n        \"TableMappings\": ...output omitted...,\n        \"ReplicationTaskSettings\": ...output omitted...,\n        \"Status\": \"modifying\",\n        \"StopReason\": \"Stop Reason FULL_LOAD_ONLY_FINISHED\",\n        \"ReplicationTaskCreationDate\": 1590524772.505,\n        \"ReplicationTaskStartDate\": 1590789424.653,\n        \"ReplicationTaskArn\": \"arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\"\n    }\n}\n\n\nFor more information, see Working with AWS DMS Tasks in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "move-replication-task",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/move-replication-task.html",
      "command_description": "Description\n\nMoves a replication task from its current replication instance to a different target replication instance using the specified parameters. The target replication instance must be created with the same or later DMS version as the current replication instance.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  move-replication-task\n--replication-task-arn <value>\n--target-replication-instance-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-task-arn <value>",
        "--target-replication-instance-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-arn (string)\n\nThe Amazon Resource Name (ARN) of the task that you want to move.\n\n--target-replication-instance-arn (string)\n\nThe ARN of the replication instance where you want to move the task to.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationTask -> (structure)\n\nThe replication task that was moved.\n\nReplicationTaskIdentifier -> (string)\n\nThe user-assigned replication task identifier or name.\n\nConstraints:\n\nMust contain 1-255 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nSourceEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) that uniquely identifies the endpoint.\n\nTargetEndpointArn -> (string)\n\nThe ARN that uniquely identifies the endpoint.\n\nReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance.\n\nMigrationType -> (string)\n\nThe type of migration.\n\nTableMappings -> (string)\n\nTable mappings specified in the task.\n\nReplicationTaskSettings -> (string)\n\nThe settings for the replication task.\n\nStatus -> (string)\n\nThe status of the replication task. This response parameter can return one of the following values:\n\n\"moving\" – The task is being moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"creating\" – The task is being created in response to running the ` CreateReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_CreateReplicationTask.html`__ operation.\n\n\"deleting\" – The task is being deleted in response to running the ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ operation.\n\n\"failed\" – The task failed to successfully complete the database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"failed-move\" – The task failed to move in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"modifying\" – The task definition is being modified in response to running the ` ModifyReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_ModifyReplicationTask.html`__ operation.\n\n\"ready\" – The task is in a ready state where it can respond to other task operations, such as ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ or ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ .\n\n\"running\" – The task is performing a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"starting\" – The task is preparing to perform a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"stopped\" – The task has stopped in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"stopping\" – The task is preparing to stop in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"testing\" – The database migration specified for this task is being tested in response to running either the ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ or the ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation.\n\nNote\n\n` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ is an improved premigration task assessment operation. The ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation assesses data type compatibility only between the source and target database of a given migration task. In contrast, ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ enables you to specify a variety of premigration task assessments in addition to data type compatibility. These assessments include ones for the validity of primary key definitions and likely issues with database migration performance, among others.\n\nLastFailureMessage -> (string)\n\nThe last error (failure) message generated for the replication task.\n\nStopReason -> (string)\n\nThe reason the replication task was stopped. This response parameter can return one of the following values:\n\n\"STOP_REASON_FULL_LOAD_COMPLETED\" – Full-load migration completed.\n\n\"STOP_REASON_CACHED_CHANGES_APPLIED\" – Change data capture (CDC) load completed.\n\n\"STOP_REASON_CACHED_CHANGES_NOT_APPLIED\" – In a full-load and CDC migration, the full load stopped as specified before starting the CDC migration.\n\n\"STOP_REASON_SERVER_TIME\" – The migration stopped at the specified server time.\n\nReplicationTaskCreationDate -> (timestamp)\n\nThe date the replication task was created.\n\nReplicationTaskStartDate -> (timestamp)\n\nThe date the replication task is scheduled to start.\n\nCdcStartPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to start. Use either CdcStartPosition or CdcStartTime to specify when you want the CDC operation to start. Specifying both values results in an error.\n\nThe value can be in date, checkpoint, or LSN/SCN format.\n\nDate Example: –cdc-start-position “2018-03-08T12:12:12”\n\nCheckpoint Example: –cdc-start-position “checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93”\n\nLSN Example: –cdc-start-position “mysql-bin-changelog.000024:373”\n\nCdcStopPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time.\n\nServer time example: –cdc-stop-position “server_time:2018-02-09T12:12:12”\n\nCommit time example: –cdc-stop-position “commit_time: 2018-02-09T12:12:12 “\n\nRecoveryCheckpoint -> (string)\n\nIndicates the last checkpoint that occurred during a change data capture (CDC) operation. You can provide this value to the CdcStartPosition parameter to start a CDC operation that begins at that checkpoint.\n\nReplicationTaskArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\nReplicationTaskStats -> (structure)\n\nThe statistics for the task, including elapsed time, tables loaded, and table errors.\n\nFullLoadProgressPercent -> (integer)\n\nThe percent complete for the full load migration task.\n\nElapsedTimeMillis -> (long)\n\nThe elapsed time of the task, in milliseconds.\n\nTablesLoaded -> (integer)\n\nThe number of tables loaded for this task.\n\nTablesLoading -> (integer)\n\nThe number of tables currently loading for this task.\n\nTablesQueued -> (integer)\n\nThe number of tables queued for this task.\n\nTablesErrored -> (integer)\n\nThe number of errors that have occurred during this task.\n\nFreshStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a target reload.\n\nStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a resume. For more information, see StartReplicationTaskType .\n\nStopDate -> (timestamp)\n\nThe date the replication task was stopped.\n\nFullLoadStartDate -> (timestamp)\n\nThe date the replication task full load was started.\n\nFullLoadFinishDate -> (timestamp)\n\nThe date the replication task full load was completed.\n\nTaskData -> (string)\n\nSupplemental information that the task requires to migrate the data for certain source and target endpoints. For more information, see Specifying Supplemental Data for Task Settings in the Database Migration Service User Guide.\n\nTargetReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance to which this task is moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation. Otherwise, this response parameter isn’t a member of the ReplicationTask object."
    },
    {
      "command_name": "reboot-replication-instance",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/reboot-replication-instance.html",
      "command_description": "Description\n\nReboots a replication instance. Rebooting results in a momentary outage, until the replication instance becomes available again.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  reboot-replication-instance\n--replication-instance-arn <value>\n[--force-failover | --no-force-failover]\n[--force-planned-failover | --no-force-planned-failover]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-instance-arn <value>",
        "[--force-failover | --no-force-failover]",
        "[--force-planned-failover | --no-force-planned-failover]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-instance-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\n--force-failover | --no-force-failover (boolean)\n\nIf this parameter is true , the reboot is conducted through a Multi-AZ failover. If the instance isn’t configured for Multi-AZ, then you can’t specify true . ( --force-planned-failover and --force-failover can’t both be set to true .)\n\n--force-planned-failover | --no-force-planned-failover (boolean)\n\nIf this parameter is true , the reboot is conducted through a planned Multi-AZ failover where resources are released and cleaned up prior to conducting the failover. If the instance isn’’t configured for Multi-AZ, then you can’t specify true . ( --force-planned-failover and --force-failover can’t both be set to true .)\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationInstance -> (structure)\n\nThe replication instance that is being rebooted.\n\nReplicationInstanceIdentifier -> (string)\n\nThe replication instance identifier is a required parameter. This parameter is stored as a lowercase string.\n\nConstraints:\n\nMust contain 1-63 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nExample: myrepinstance\n\nReplicationInstanceClass -> (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class. It is a required parameter, although a default value is pre-selected in the DMS console.\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\nReplicationInstanceStatus -> (string)\n\nThe status of the replication instance. The possible return values include:\n\n\"available\"\n\n\"creating\"\n\n\"deleted\"\n\n\"deleting\"\n\n\"failed\"\n\n\"modifying\"\n\n\"upgrading\"\n\n\"rebooting\"\n\n\"resetting-master-credentials\"\n\n\"storage-full\"\n\n\"incompatible-credentials\"\n\n\"incompatible-network\"\n\n\"maintenance\"\n\nAllocatedStorage -> (integer)\n\nThe amount of storage (in gigabytes) that is allocated for the replication instance.\n\nInstanceCreateTime -> (timestamp)\n\nThe time the replication instance was created.\n\nVpcSecurityGroups -> (list)\n\nThe VPC security group for the instance.\n\n(structure)\n\nDescribes the status of a security group associated with the virtual private cloud (VPC) hosting your replication and DB instances.\n\nVpcSecurityGroupId -> (string)\n\nThe VPC security group ID.\n\nStatus -> (string)\n\nThe status of the VPC security group.\n\nAvailabilityZone -> (string)\n\nThe Availability Zone for the instance.\n\nReplicationSubnetGroup -> (structure)\n\nThe subnet group for the replication instance.\n\nReplicationSubnetGroupIdentifier -> (string)\n\nThe identifier of the replication instance subnet group.\n\nReplicationSubnetGroupDescription -> (string)\n\nA description for the replication subnet group.\n\nVpcId -> (string)\n\nThe ID of the VPC.\n\nSubnetGroupStatus -> (string)\n\nThe status of the subnet group.\n\nSubnets -> (list)\n\nThe subnets that are in the subnet group.\n\n(structure)\n\nIn response to a request by the DescribeReplicationSubnetGroups operation, this object identifies a subnet by its given Availability Zone, subnet identifier, and status.\n\nSubnetIdentifier -> (string)\n\nThe subnet identifier.\n\nSubnetAvailabilityZone -> (structure)\n\nThe Availability Zone of the subnet.\n\nName -> (string)\n\nThe name of the Availability Zone.\n\nSubnetStatus -> (string)\n\nThe status of the subnet.\n\nPreferredMaintenanceWindow -> (string)\n\nThe maintenance window times for the replication instance. Any pending upgrades to the replication instance are performed during this time.\n\nPendingModifiedValues -> (structure)\n\nThe pending modification values.\n\nReplicationInstanceClass -> (string)\n\nThe compute and memory capacity of the replication instance as defined for the specified replication instance class.\n\nFor more information on the settings and capacities for the available replication instance classes, see Selecting the right DMS replication instance for your migration .\n\nAllocatedStorage -> (integer)\n\nThe amount of storage (in gigabytes) that is allocated for the replication instance.\n\nMultiAZ -> (boolean)\n\nSpecifies whether the replication instance is a Multi-AZ deployment. You can’t set the AvailabilityZone parameter if the Multi-AZ parameter is set to true .\n\nEngineVersion -> (string)\n\nThe engine version number of the replication instance.\n\nMultiAZ -> (boolean)\n\nSpecifies whether the replication instance is a Multi-AZ deployment. You can’t set the AvailabilityZone parameter if the Multi-AZ parameter is set to true .\n\nEngineVersion -> (string)\n\nThe engine version number of the replication instance.\n\nIf an engine version number is not specified when a replication instance is created, the default is the latest engine version available.\n\nWhen modifying a major engine version of an instance, also set AllowMajorVersionUpgrade to true .\n\nAutoMinorVersionUpgrade -> (boolean)\n\nBoolean value indicating if minor version upgrades will be automatically applied to the instance.\n\nKmsKeyId -> (string)\n\nAn KMS key identifier that is used to encrypt the data on the replication instance.\n\nIf you don’t specify a value for the KmsKeyId parameter, then DMS uses your default encryption key.\n\nKMS creates the default encryption key for your Amazon Web Services account. Your Amazon Web Services account has a different default encryption key for each Amazon Web Services Region.\n\nReplicationInstanceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\nReplicationInstancePublicIpAddress -> (string)\n\nThe public IP address of the replication instance.\n\nReplicationInstancePrivateIpAddress -> (string)\n\nThe private IP address of the replication instance.\n\nReplicationInstancePublicIpAddresses -> (list)\n\nOne or more public IP addresses for the replication instance.\n\n(string)\n\nReplicationInstancePrivateIpAddresses -> (list)\n\nOne or more private IP addresses for the replication instance.\n\n(string)\n\nPubliclyAccessible -> (boolean)\n\nSpecifies the accessibility options for the replication instance. A value of true represents an instance with a public IP address. A value of false represents an instance with a private IP address. The default value is true .\n\nSecondaryAvailabilityZone -> (string)\n\nThe Availability Zone of the standby replication instance in a Multi-AZ deployment.\n\nFreeUntil -> (timestamp)\n\nThe expiration date of the free replication instance that is part of the Free DMS program.\n\nDnsNameServers -> (string)\n\nThe DNS name servers supported for the replication instance to access your on-premise source or target database.",
      "command_examples": "Examples\n\nTo reboot a replication instance\n\nThe following reboot-replication-instance example reboots a replication instance.\n\naws dms reboot-replication-instance \\\n    --replication-instance-arn arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\n\n\nOutput:\n\n{\n    \"ReplicationInstance\": {\n        \"ReplicationInstanceIdentifier\": \"my-repl-instance\",\n        \"ReplicationInstanceClass\": \"dms.t2.micro\",\n        \"ReplicationInstanceStatus\": \"rebooting\",\n        \"AllocatedStorage\": 5,\n        \"InstanceCreateTime\": 1590011235.952,\n    ... output omitted ...\n    }\n}\n\n\nFor more information, see Working with an AWS DMS Replication Instance in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "refresh-schemas",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/refresh-schemas.html",
      "command_description": "Description\n\nPopulates the schema for the specified endpoint. This is an asynchronous operation and can take several minutes. You can check the status of this operation by calling the DescribeRefreshSchemasStatus operation.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  refresh-schemas\n--endpoint-arn <value>\n--replication-instance-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--endpoint-arn <value>",
        "--replication-instance-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--endpoint-arn (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\n--replication-instance-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nRefreshSchemasStatus -> (structure)\n\nThe status of the refreshed schema.\n\nEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\nReplicationInstanceArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\nStatus -> (string)\n\nThe status of the schema.\n\nLastRefreshDate -> (timestamp)\n\nThe date the schema was last refreshed.\n\nLastFailureMessage -> (string)\n\nThe last failure message for the schema.",
      "command_examples": "Examples\n\nTo refresh database schemas\n\nThe following refresh-schemas example requests that AWS DMS refresh the list of schemas at an endpoint.\n\naws dms refresh-schemas \\\n    --replication-instance-arn arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE \\\n    --endpoint-arn \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\"\n\n\nOutput:\n\n{\n    \"RefreshSchemasStatus\": {\n        \"EndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\",\n        \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n        \"Status\": \"refreshing\",\n        \"LastRefreshDate\": 1590019949.103\n    }\n}\n"
    },
    {
      "command_name": "reload-tables",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/reload-tables.html",
      "command_description": "Description\n\nReloads the target database table with the source data.\n\nYou can only use this operation with a task in the RUNNING state, otherwise the service will throw an InvalidResourceStateFault exception.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  reload-tables\n--replication-task-arn <value>\n--tables-to-reload <value>\n[--reload-option <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-task-arn <value>",
        "--tables-to-reload <value>",
        "[--reload-option <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\n--tables-to-reload (list)\n\nThe name and schema of the table to be reloaded.\n\n(structure)\n\nProvides the name of the schema and table to be reloaded.\n\nSchemaName -> (string)\n\nThe schema name of the table to be reloaded.\n\nTableName -> (string)\n\nThe table name of the table to be reloaded.\n\nShorthand Syntax:\n\nSchemaName=string,TableName=string ...\n\n\nJSON Syntax:\n\n[\n  {\n    \"SchemaName\": \"string\",\n    \"TableName\": \"string\"\n  }\n  ...\n]\n\n\n--reload-option (string)\n\nOptions for reload. Specify data-reload to reload the data and re-validate it if validation is enabled. Specify validate-only to re-validate the table. This option applies only when validation is enabled for the task.\n\nValid values: data-reload, validate-only\n\nDefault value is data-reload.\n\nPossible values:\n\ndata-reload\n\nvalidate-only\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationTaskArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication task.",
      "command_examples": "Examples\n\nTo refresh the list of tables available at an endpoint\n\nThe following reload-tables example reloads the list of available tables at an endpoint.\n\naws dms reload-tables \\\n    --replication-task-arn \"arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\" \\\n    --tables-to-reload \"SchemaName=prodrep,TableName=ACCT_BAL\"\n\n\nOutput:\n\n{\n    \"ReplicationTaskArn\": \"arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\"\n}\n"
    },
    {
      "command_name": "remove-tags-from-resource",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/remove-tags-from-resource.html",
      "command_description": "Description\n\nRemoves metadata tags from an DMS resource, including replication instance, endpoint, security group, and migration task. For more information, see ` Tag https://docs.aws.amazon.com/dms/latest/APIReference/API_Tag.html`__ data type description.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  remove-tags-from-resource\n--resource-arn <value>\n--tag-keys <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--resource-arn <value>",
        "--tag-keys <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--resource-arn (string)\n\nAn DMS resource from which you want to remove tag(s). The value for this parameter is an Amazon Resource Name (ARN).\n\n--tag-keys (list)\n\nThe tag key (name) of the tag to be removed.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nNone",
      "command_examples": "Examples\n\nTo remove tags from a replication instance\n\nThe following remove-tags-from-resource example removes tags from a replication instance.\n\naws dms remove-tags-from-resource \\\n    --resource-arn arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE \\\n    --tag-keys Environment Project\n\n\nThis command produces no output.\n\nFor more information, see Tagging Resources in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "start-replication-task",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/start-replication-task.html",
      "command_description": "Description\n\nStarts the replication task.\n\nFor more information about DMS tasks, see Working with Migration Tasks in the Database Migration Service User Guide.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  start-replication-task\n--replication-task-arn <value>\n--start-replication-task-type <value>\n[--cdc-start-time <value>]\n[--cdc-start-position <value>]\n[--cdc-stop-position <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-task-arn <value>",
        "--start-replication-task-type <value>",
        "[--cdc-start-time <value>]",
        "[--cdc-start-position <value>]",
        "[--cdc-stop-position <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication task to be started.\n\n--start-replication-task-type (string)\n\nA type of replication task.\n\nPossible values:\n\nstart-replication\n\nresume-processing\n\nreload-target\n\n--cdc-start-time (timestamp)\n\nIndicates the start time for a change data capture (CDC) operation. Use either CdcStartTime or CdcStartPosition to specify when you want a CDC operation to start. Specifying both values results in an error.\n\nTimestamp Example: –cdc-start-time “2018-03-08T12:12:12”\n\n--cdc-start-position (string)\n\nIndicates when you want a change data capture (CDC) operation to start. Use either CdcStartPosition or CdcStartTime to specify when you want a CDC operation to start. Specifying both values results in an error.\n\nThe value can be in date, checkpoint, or LSN/SCN format.\n\nDate Example: –cdc-start-position “2018-03-08T12:12:12”\n\nCheckpoint Example: –cdc-start-position “checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93”\n\nLSN Example: –cdc-start-position “mysql-bin-changelog.000024:373”\n\nNote\n\nWhen you use this task setting with a source PostgreSQL database, a logical replication slot should already be created and associated with the source endpoint. You can verify this by setting the slotName extra connection attribute to the name of this logical replication slot. For more information, see Extra Connection Attributes When Using PostgreSQL as a Source for DMS .\n\n--cdc-stop-position (string)\n\nIndicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time.\n\nServer time example: –cdc-stop-position “server_time:2018-02-09T12:12:12”\n\nCommit time example: –cdc-stop-position “commit_time: 2018-02-09T12:12:12 “\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationTask -> (structure)\n\nThe replication task started.\n\nReplicationTaskIdentifier -> (string)\n\nThe user-assigned replication task identifier or name.\n\nConstraints:\n\nMust contain 1-255 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nSourceEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) that uniquely identifies the endpoint.\n\nTargetEndpointArn -> (string)\n\nThe ARN that uniquely identifies the endpoint.\n\nReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance.\n\nMigrationType -> (string)\n\nThe type of migration.\n\nTableMappings -> (string)\n\nTable mappings specified in the task.\n\nReplicationTaskSettings -> (string)\n\nThe settings for the replication task.\n\nStatus -> (string)\n\nThe status of the replication task. This response parameter can return one of the following values:\n\n\"moving\" – The task is being moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"creating\" – The task is being created in response to running the ` CreateReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_CreateReplicationTask.html`__ operation.\n\n\"deleting\" – The task is being deleted in response to running the ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ operation.\n\n\"failed\" – The task failed to successfully complete the database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"failed-move\" – The task failed to move in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"modifying\" – The task definition is being modified in response to running the ` ModifyReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_ModifyReplicationTask.html`__ operation.\n\n\"ready\" – The task is in a ready state where it can respond to other task operations, such as ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ or ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ .\n\n\"running\" – The task is performing a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"starting\" – The task is preparing to perform a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"stopped\" – The task has stopped in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"stopping\" – The task is preparing to stop in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"testing\" – The database migration specified for this task is being tested in response to running either the ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ or the ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation.\n\nNote\n\n` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ is an improved premigration task assessment operation. The ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation assesses data type compatibility only between the source and target database of a given migration task. In contrast, ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ enables you to specify a variety of premigration task assessments in addition to data type compatibility. These assessments include ones for the validity of primary key definitions and likely issues with database migration performance, among others.\n\nLastFailureMessage -> (string)\n\nThe last error (failure) message generated for the replication task.\n\nStopReason -> (string)\n\nThe reason the replication task was stopped. This response parameter can return one of the following values:\n\n\"STOP_REASON_FULL_LOAD_COMPLETED\" – Full-load migration completed.\n\n\"STOP_REASON_CACHED_CHANGES_APPLIED\" – Change data capture (CDC) load completed.\n\n\"STOP_REASON_CACHED_CHANGES_NOT_APPLIED\" – In a full-load and CDC migration, the full load stopped as specified before starting the CDC migration.\n\n\"STOP_REASON_SERVER_TIME\" – The migration stopped at the specified server time.\n\nReplicationTaskCreationDate -> (timestamp)\n\nThe date the replication task was created.\n\nReplicationTaskStartDate -> (timestamp)\n\nThe date the replication task is scheduled to start.\n\nCdcStartPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to start. Use either CdcStartPosition or CdcStartTime to specify when you want the CDC operation to start. Specifying both values results in an error.\n\nThe value can be in date, checkpoint, or LSN/SCN format.\n\nDate Example: –cdc-start-position “2018-03-08T12:12:12”\n\nCheckpoint Example: –cdc-start-position “checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93”\n\nLSN Example: –cdc-start-position “mysql-bin-changelog.000024:373”\n\nCdcStopPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time.\n\nServer time example: –cdc-stop-position “server_time:2018-02-09T12:12:12”\n\nCommit time example: –cdc-stop-position “commit_time: 2018-02-09T12:12:12 “\n\nRecoveryCheckpoint -> (string)\n\nIndicates the last checkpoint that occurred during a change data capture (CDC) operation. You can provide this value to the CdcStartPosition parameter to start a CDC operation that begins at that checkpoint.\n\nReplicationTaskArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\nReplicationTaskStats -> (structure)\n\nThe statistics for the task, including elapsed time, tables loaded, and table errors.\n\nFullLoadProgressPercent -> (integer)\n\nThe percent complete for the full load migration task.\n\nElapsedTimeMillis -> (long)\n\nThe elapsed time of the task, in milliseconds.\n\nTablesLoaded -> (integer)\n\nThe number of tables loaded for this task.\n\nTablesLoading -> (integer)\n\nThe number of tables currently loading for this task.\n\nTablesQueued -> (integer)\n\nThe number of tables queued for this task.\n\nTablesErrored -> (integer)\n\nThe number of errors that have occurred during this task.\n\nFreshStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a target reload.\n\nStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a resume. For more information, see StartReplicationTaskType .\n\nStopDate -> (timestamp)\n\nThe date the replication task was stopped.\n\nFullLoadStartDate -> (timestamp)\n\nThe date the replication task full load was started.\n\nFullLoadFinishDate -> (timestamp)\n\nThe date the replication task full load was completed.\n\nTaskData -> (string)\n\nSupplemental information that the task requires to migrate the data for certain source and target endpoints. For more information, see Specifying Supplemental Data for Task Settings in the Database Migration Service User Guide.\n\nTargetReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance to which this task is moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation. Otherwise, this response parameter isn’t a member of the ReplicationTask object.",
      "command_examples": "Examples\n\nTo start a replication task\n\nThe following command-name example lists the available widgets in your AWS account.\n\naws dms start-replication-task \\\n    --replication-task-arn arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII \\\n    --start-replication-task-type reload-target\n\n\nOutput:\n\n{\n    \"ReplicationTask\": {\n        \"ReplicationTaskIdentifier\": \"moveit2\",\n        \"SourceEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\",\n        \"TargetEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:EOM4SFKCZEYHZBFGAGZT3QEC5U\",\n        \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n        \"MigrationType\": \"full-load\",\n        \"TableMappings\": ...output omitted... ,\n        \"ReplicationTaskSettings\": ...output omitted... ,\n        \"Status\": \"starting\",\n        \"ReplicationTaskCreationDate\": 1590524772.505,\n        \"ReplicationTaskStartDate\": 1590619805.212,\n        \"ReplicationTaskArn\": \"arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\"\n    }\n}\n\n\nFor more information, see Working with AWS DMS Tasks in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "start-replication-task-assessment",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/start-replication-task-assessment.html",
      "command_description": "Description\n\nStarts the replication task assessment for unsupported data types in the source database.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  start-replication-task-assessment\n--replication-task-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-task-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationTask -> (structure)\n\nThe assessed replication task.\n\nReplicationTaskIdentifier -> (string)\n\nThe user-assigned replication task identifier or name.\n\nConstraints:\n\nMust contain 1-255 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nSourceEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) that uniquely identifies the endpoint.\n\nTargetEndpointArn -> (string)\n\nThe ARN that uniquely identifies the endpoint.\n\nReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance.\n\nMigrationType -> (string)\n\nThe type of migration.\n\nTableMappings -> (string)\n\nTable mappings specified in the task.\n\nReplicationTaskSettings -> (string)\n\nThe settings for the replication task.\n\nStatus -> (string)\n\nThe status of the replication task. This response parameter can return one of the following values:\n\n\"moving\" – The task is being moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"creating\" – The task is being created in response to running the ` CreateReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_CreateReplicationTask.html`__ operation.\n\n\"deleting\" – The task is being deleted in response to running the ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ operation.\n\n\"failed\" – The task failed to successfully complete the database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"failed-move\" – The task failed to move in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"modifying\" – The task definition is being modified in response to running the ` ModifyReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_ModifyReplicationTask.html`__ operation.\n\n\"ready\" – The task is in a ready state where it can respond to other task operations, such as ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ or ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ .\n\n\"running\" – The task is performing a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"starting\" – The task is preparing to perform a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"stopped\" – The task has stopped in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"stopping\" – The task is preparing to stop in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"testing\" – The database migration specified for this task is being tested in response to running either the ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ or the ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation.\n\nNote\n\n` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ is an improved premigration task assessment operation. The ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation assesses data type compatibility only between the source and target database of a given migration task. In contrast, ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ enables you to specify a variety of premigration task assessments in addition to data type compatibility. These assessments include ones for the validity of primary key definitions and likely issues with database migration performance, among others.\n\nLastFailureMessage -> (string)\n\nThe last error (failure) message generated for the replication task.\n\nStopReason -> (string)\n\nThe reason the replication task was stopped. This response parameter can return one of the following values:\n\n\"STOP_REASON_FULL_LOAD_COMPLETED\" – Full-load migration completed.\n\n\"STOP_REASON_CACHED_CHANGES_APPLIED\" – Change data capture (CDC) load completed.\n\n\"STOP_REASON_CACHED_CHANGES_NOT_APPLIED\" – In a full-load and CDC migration, the full load stopped as specified before starting the CDC migration.\n\n\"STOP_REASON_SERVER_TIME\" – The migration stopped at the specified server time.\n\nReplicationTaskCreationDate -> (timestamp)\n\nThe date the replication task was created.\n\nReplicationTaskStartDate -> (timestamp)\n\nThe date the replication task is scheduled to start.\n\nCdcStartPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to start. Use either CdcStartPosition or CdcStartTime to specify when you want the CDC operation to start. Specifying both values results in an error.\n\nThe value can be in date, checkpoint, or LSN/SCN format.\n\nDate Example: –cdc-start-position “2018-03-08T12:12:12”\n\nCheckpoint Example: –cdc-start-position “checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93”\n\nLSN Example: –cdc-start-position “mysql-bin-changelog.000024:373”\n\nCdcStopPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time.\n\nServer time example: –cdc-stop-position “server_time:2018-02-09T12:12:12”\n\nCommit time example: –cdc-stop-position “commit_time: 2018-02-09T12:12:12 “\n\nRecoveryCheckpoint -> (string)\n\nIndicates the last checkpoint that occurred during a change data capture (CDC) operation. You can provide this value to the CdcStartPosition parameter to start a CDC operation that begins at that checkpoint.\n\nReplicationTaskArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\nReplicationTaskStats -> (structure)\n\nThe statistics for the task, including elapsed time, tables loaded, and table errors.\n\nFullLoadProgressPercent -> (integer)\n\nThe percent complete for the full load migration task.\n\nElapsedTimeMillis -> (long)\n\nThe elapsed time of the task, in milliseconds.\n\nTablesLoaded -> (integer)\n\nThe number of tables loaded for this task.\n\nTablesLoading -> (integer)\n\nThe number of tables currently loading for this task.\n\nTablesQueued -> (integer)\n\nThe number of tables queued for this task.\n\nTablesErrored -> (integer)\n\nThe number of errors that have occurred during this task.\n\nFreshStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a target reload.\n\nStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a resume. For more information, see StartReplicationTaskType .\n\nStopDate -> (timestamp)\n\nThe date the replication task was stopped.\n\nFullLoadStartDate -> (timestamp)\n\nThe date the replication task full load was started.\n\nFullLoadFinishDate -> (timestamp)\n\nThe date the replication task full load was completed.\n\nTaskData -> (string)\n\nSupplemental information that the task requires to migrate the data for certain source and target endpoints. For more information, see Specifying Supplemental Data for Task Settings in the Database Migration Service User Guide.\n\nTargetReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance to which this task is moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation. Otherwise, this response parameter isn’t a member of the ReplicationTask object.",
      "command_examples": "Examples\n\nTo start a task assessment\n\nThe following start-replication-task-assessment example starts a replication task assessment.\n\naws dms start-replication-task-assessment \\\n    --replication-task-arn arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\n\n\nOutput:\n\n{\n    \"ReplicationTask\": {\n        \"ReplicationTaskIdentifier\": \"moveit2\",\n        \"SourceEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\",\n        \"TargetEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:EOM4SFKCZEYHZBFGAGZT3QEC5U\",\n        \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n        \"MigrationType\": \"full-load\",\n        \"TableMappings\": ...output omitted...,\n        \"ReplicationTaskSettings\": ...output omitted...,\n        \"Status\": \"testing\",\n        \"StopReason\": \"Stop Reason FULL_LOAD_ONLY_FINISHED\",\n        \"ReplicationTaskCreationDate\": 1590524772.505,\n        \"ReplicationTaskStartDate\": 1590789988.677,\n        \"ReplicationTaskArn\": \"arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\"\n    }\n}\n\n\nFor more information, see Creating a Task Assessment Report in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "start-replication-task-assessment-run",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/start-replication-task-assessment-run.html",
      "command_description": "Description\n\nStarts a new premigration assessment run for one or more individual assessments of a migration task.\n\nThe assessments that you can specify depend on the source and target database engine and the migration type defined for the given task. To run this operation, your migration task must already be created. After you run this operation, you can review the status of each individual assessment. You can also run the migration task manually after the assessment run and its individual assessments complete.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  start-replication-task-assessment-run\n--replication-task-arn <value>\n--service-access-role-arn <value>\n--result-location-bucket <value>\n[--result-location-folder <value>]\n[--result-encryption-mode <value>]\n[--result-kms-key-arn <value>]\n--assessment-run-name <value>\n[--include-only <value>]\n[--exclude <value>]\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-task-arn <value>",
        "--service-access-role-arn <value>",
        "--result-location-bucket <value>",
        "[--result-location-folder <value>]",
        "[--result-encryption-mode <value>]",
        "[--result-kms-key-arn <value>]",
        "--assessment-run-name <value>",
        "[--include-only <value>]",
        "[--exclude <value>]",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-arn (string)\n\nAmazon Resource Name (ARN) of the migration task associated with the premigration assessment run that you want to start.\n\n--service-access-role-arn (string)\n\nARN of the service role needed to start the assessment run. The role must allow the iam:PassRole action.\n\n--result-location-bucket (string)\n\nAmazon S3 bucket where you want DMS to store the results of this assessment run.\n\n--result-location-folder (string)\n\nFolder within an Amazon S3 bucket where you want DMS to store the results of this assessment run.\n\n--result-encryption-mode (string)\n\nEncryption mode that you can specify to encrypt the results of this assessment run. If you don’t specify this request parameter, DMS stores the assessment run results without encryption. You can specify one of the options following:\n\n\"SSE_S3\" – The server-side encryption provided as a default by Amazon S3.\n\n\"SSE_KMS\" – Key Management Service (KMS) encryption. This encryption can use either a custom KMS encryption key that you specify or the default KMS encryption key that DMS provides.\n\n--result-kms-key-arn (string)\n\nARN of a custom KMS encryption key that you specify when you set ResultEncryptionMode to \"SSE_KMS “.\n\n--assessment-run-name (string)\n\nUnique name to identify the assessment run.\n\n--include-only (list)\n\nSpace-separated list of names for specific individual assessments that you want to include. These names come from the default list of individual assessments that DMS supports for the associated migration task. This task is specified by ReplicationTaskArn .\n\nNote\n\nYou can’t set a value for IncludeOnly if you also set a value for Exclude in the API operation.\n\nTo identify the names of the default individual assessments that DMS supports for the associated migration task, run the DescribeApplicableIndividualAssessments operation using its own ReplicationTaskArn request parameter.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--exclude (list)\n\nSpace-separated list of names for specific individual assessments that you want to exclude. These names come from the default list of individual assessments that DMS supports for the associated migration task. This task is specified by ReplicationTaskArn .\n\nNote\n\nYou can’t set a value for Exclude if you also set a value for IncludeOnly in the API operation.\n\nTo identify the names of the default individual assessments that DMS supports for the associated migration task, run the DescribeApplicableIndividualAssessments operation using its own ReplicationTaskArn request parameter.\n\n(string)\n\nSyntax:\n\n\"string\" \"string\" ...\n\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationTaskAssessmentRun -> (structure)\n\nThe premigration assessment run that was started.\n\nReplicationTaskAssessmentRunArn -> (string)\n\nAmazon Resource Name (ARN) of this assessment run.\n\nReplicationTaskArn -> (string)\n\nARN of the migration task associated with this premigration assessment run.\n\nStatus -> (string)\n\nAssessment run status.\n\nThis status can have one of the following values:\n\n\"cancelling\" – The assessment run was canceled by the CancelReplicationTaskAssessmentRun operation.\n\n\"deleting\" – The assessment run was deleted by the DeleteReplicationTaskAssessmentRun operation.\n\n\"failed\" – At least one individual assessment completed with a failed status.\n\n\"error-provisioning\" – An internal error occurred while resources were provisioned (during provisioning status).\n\n\"error-executing\" – An internal error occurred while individual assessments ran (during running status).\n\n\"invalid state\" – The assessment run is in an unknown state.\n\n\"passed\" – All individual assessments have completed, and none has a failed status.\n\n\"provisioning\" – Resources required to run individual assessments are being provisioned.\n\n\"running\" – Individual assessments are being run.\n\n\"starting\" – The assessment run is starting, but resources are not yet being provisioned for individual assessments.\n\nReplicationTaskAssessmentRunCreationDate -> (timestamp)\n\nDate on which the assessment run was created using the StartReplicationTaskAssessmentRun operation.\n\nAssessmentProgress -> (structure)\n\nIndication of the completion progress for the individual assessments specified to run.\n\nIndividualAssessmentCount -> (integer)\n\nThe number of individual assessments that are specified to run.\n\nIndividualAssessmentCompletedCount -> (integer)\n\nThe number of individual assessments that have completed, successfully or not.\n\nLastFailureMessage -> (string)\n\nLast message generated by an individual assessment failure.\n\nServiceAccessRoleArn -> (string)\n\nARN of the service role used to start the assessment run using the StartReplicationTaskAssessmentRun operation. The role must allow the iam:PassRole action.\n\nResultLocationBucket -> (string)\n\nAmazon S3 bucket where DMS stores the results of this assessment run.\n\nResultLocationFolder -> (string)\n\nFolder in an Amazon S3 bucket where DMS stores the results of this assessment run.\n\nResultEncryptionMode -> (string)\n\nEncryption mode used to encrypt the assessment run results.\n\nResultKmsKeyArn -> (string)\n\nARN of the KMS encryption key used to encrypt the assessment run results.\n\nAssessmentRunName -> (string)\n\nUnique name of the assessment run."
    },
    {
      "command_name": "stop-replication-task",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/stop-replication-task.html",
      "command_description": "Description\n\nStops the replication task.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  stop-replication-task\n--replication-task-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-task-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-task-arn (string)\n\nThe Amazon Resource Name(ARN) of the replication task to be stopped.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nReplicationTask -> (structure)\n\nThe replication task stopped.\n\nReplicationTaskIdentifier -> (string)\n\nThe user-assigned replication task identifier or name.\n\nConstraints:\n\nMust contain 1-255 alphanumeric characters or hyphens.\n\nFirst character must be a letter.\n\nCannot end with a hyphen or contain two consecutive hyphens.\n\nSourceEndpointArn -> (string)\n\nThe Amazon Resource Name (ARN) that uniquely identifies the endpoint.\n\nTargetEndpointArn -> (string)\n\nThe ARN that uniquely identifies the endpoint.\n\nReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance.\n\nMigrationType -> (string)\n\nThe type of migration.\n\nTableMappings -> (string)\n\nTable mappings specified in the task.\n\nReplicationTaskSettings -> (string)\n\nThe settings for the replication task.\n\nStatus -> (string)\n\nThe status of the replication task. This response parameter can return one of the following values:\n\n\"moving\" – The task is being moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"creating\" – The task is being created in response to running the ` CreateReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_CreateReplicationTask.html`__ operation.\n\n\"deleting\" – The task is being deleted in response to running the ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ operation.\n\n\"failed\" – The task failed to successfully complete the database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"failed-move\" – The task failed to move in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation.\n\n\"modifying\" – The task definition is being modified in response to running the ` ModifyReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_ModifyReplicationTask.html`__ operation.\n\n\"ready\" – The task is in a ready state where it can respond to other task operations, such as ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ or ` DeleteReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_DeleteReplicationTask.html`__ .\n\n\"running\" – The task is performing a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"starting\" – The task is preparing to perform a database migration in response to running the ` StartReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html`__ operation.\n\n\"stopped\" – The task has stopped in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"stopping\" – The task is preparing to stop in response to running the ` StopReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_StopReplicationTask.html`__ operation.\n\n\"testing\" – The database migration specified for this task is being tested in response to running either the ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ or the ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation.\n\nNote\n\n` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ is an improved premigration task assessment operation. The ` StartReplicationTaskAssessment https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessment.html`__ operation assesses data type compatibility only between the source and target database of a given migration task. In contrast, ` StartReplicationTaskAssessmentRun https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTaskAssessmentRun.html`__ enables you to specify a variety of premigration task assessments in addition to data type compatibility. These assessments include ones for the validity of primary key definitions and likely issues with database migration performance, among others.\n\nLastFailureMessage -> (string)\n\nThe last error (failure) message generated for the replication task.\n\nStopReason -> (string)\n\nThe reason the replication task was stopped. This response parameter can return one of the following values:\n\n\"STOP_REASON_FULL_LOAD_COMPLETED\" – Full-load migration completed.\n\n\"STOP_REASON_CACHED_CHANGES_APPLIED\" – Change data capture (CDC) load completed.\n\n\"STOP_REASON_CACHED_CHANGES_NOT_APPLIED\" – In a full-load and CDC migration, the full load stopped as specified before starting the CDC migration.\n\n\"STOP_REASON_SERVER_TIME\" – The migration stopped at the specified server time.\n\nReplicationTaskCreationDate -> (timestamp)\n\nThe date the replication task was created.\n\nReplicationTaskStartDate -> (timestamp)\n\nThe date the replication task is scheduled to start.\n\nCdcStartPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to start. Use either CdcStartPosition or CdcStartTime to specify when you want the CDC operation to start. Specifying both values results in an error.\n\nThe value can be in date, checkpoint, or LSN/SCN format.\n\nDate Example: –cdc-start-position “2018-03-08T12:12:12”\n\nCheckpoint Example: –cdc-start-position “checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93”\n\nLSN Example: –cdc-start-position “mysql-bin-changelog.000024:373”\n\nCdcStopPosition -> (string)\n\nIndicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time.\n\nServer time example: –cdc-stop-position “server_time:2018-02-09T12:12:12”\n\nCommit time example: –cdc-stop-position “commit_time: 2018-02-09T12:12:12 “\n\nRecoveryCheckpoint -> (string)\n\nIndicates the last checkpoint that occurred during a change data capture (CDC) operation. You can provide this value to the CdcStartPosition parameter to start a CDC operation that begins at that checkpoint.\n\nReplicationTaskArn -> (string)\n\nThe Amazon Resource Name (ARN) of the replication task.\n\nReplicationTaskStats -> (structure)\n\nThe statistics for the task, including elapsed time, tables loaded, and table errors.\n\nFullLoadProgressPercent -> (integer)\n\nThe percent complete for the full load migration task.\n\nElapsedTimeMillis -> (long)\n\nThe elapsed time of the task, in milliseconds.\n\nTablesLoaded -> (integer)\n\nThe number of tables loaded for this task.\n\nTablesLoading -> (integer)\n\nThe number of tables currently loading for this task.\n\nTablesQueued -> (integer)\n\nThe number of tables queued for this task.\n\nTablesErrored -> (integer)\n\nThe number of errors that have occurred during this task.\n\nFreshStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a target reload.\n\nStartDate -> (timestamp)\n\nThe date the replication task was started either with a fresh start or a resume. For more information, see StartReplicationTaskType .\n\nStopDate -> (timestamp)\n\nThe date the replication task was stopped.\n\nFullLoadStartDate -> (timestamp)\n\nThe date the replication task full load was started.\n\nFullLoadFinishDate -> (timestamp)\n\nThe date the replication task full load was completed.\n\nTaskData -> (string)\n\nSupplemental information that the task requires to migrate the data for certain source and target endpoints. For more information, see Specifying Supplemental Data for Task Settings in the Database Migration Service User Guide.\n\nTargetReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance to which this task is moved in response to running the ` MoveReplicationTask https://docs.aws.amazon.com/dms/latest/APIReference/API_MoveReplicationTask.html`__ operation. Otherwise, this response parameter isn’t a member of the ReplicationTask object.",
      "command_examples": "Examples\n\nTo stop a task\n\nThe following stop-replication-task example stops a task.\n\naws dms stop-replication-task \\\n    --replication-task-arn arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\n\n\nOutput:\n\n{\n    \"ReplicationTask\": {\n        \"ReplicationTaskIdentifier\": \"moveit2\",\n        \"SourceEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\",\n        \"TargetEndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:EOM4SFKCZEYHZBFGAGZT3QEC5U\",\n        \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n        \"MigrationType\": \"full-load\",\n        \"TableMappings\": ...output omitted...,\n        \"ReplicationTaskSettings\": ...output omitted...,\n        \"Status\": \"stopping\",\n        \"ReplicationTaskCreationDate\": 1590524772.505,\n        \"ReplicationTaskStartDate\": 1590789424.653,\n        \"ReplicationTaskArn\": \"arn:aws:dms:us-east-1:123456789012:task:K55IUCGBASJS5VHZJIINA45FII\"\n    }\n}\n\n\nFor more information, see Working with AWS DMS Tasks in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "test-connection",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/test-connection.html",
      "command_description": "Description\n\nTests the connection between the replication instance and the endpoint.\n\nSee also: AWS API Documentation\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_synopsis": "Synopsis\n  test-connection\n--replication-instance-arn <value>\n--endpoint-arn <value>\n[--cli-input-json | --cli-input-yaml]\n[--generate-cli-skeleton <value>]\n",
      "command_options": [
        "--replication-instance-arn <value>",
        "--endpoint-arn <value>",
        "[--cli-input-json | --cli-input-yaml]",
        "[--generate-cli-skeleton <value>]"
      ],
      "command_options_description": "Options\n\n--replication-instance-arn (string)\n\nThe Amazon Resource Name (ARN) of the replication instance.\n\n--endpoint-arn (string)\n\nThe Amazon Resource Name (ARN) string that uniquely identifies the endpoint.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_output": "Output\n\nConnection -> (structure)\n\nThe connection tested.\n\nReplicationInstanceArn -> (string)\n\nThe ARN of the replication instance.\n\nEndpointArn -> (string)\n\nThe ARN string that uniquely identifies the endpoint.\n\nStatus -> (string)\n\nThe connection status. This parameter can return one of the following values:\n\n\"successful\"\n\n\"testing\"\n\n\"failed\"\n\n\"deleting\"\n\nLastFailureMessage -> (string)\n\nThe error message when the connection last failed.\n\nEndpointIdentifier -> (string)\n\nThe identifier of the endpoint. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can’t end with a hyphen or contain two consecutive hyphens.\n\nReplicationInstanceIdentifier -> (string)\n\nThe replication instance identifier. This parameter is stored as a lowercase string.",
      "command_examples": "Examples\n\nTo test a connection to an endpoint\n\nThe following test-connection example tests whether an endpoint can be accessed from a replication instance.\n\naws dms test-connection \\\n    --replication-instance-arn arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE \\\n    --endpoint-arn arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\n\n\nOutput:\n\n{\n    \"Connection\": {\n        \"ReplicationInstanceArn\": \"arn:aws:dms:us-east-1:123456789012:rep:T3OM7OUB5NM2LCVZF7JPGJRNUE\",\n        \"EndpointArn\": \"arn:aws:dms:us-east-1:123456789012:endpoint:6GGI6YPWWGAYUVLKIB732KEVWA\",\n        \"Status\": \"testing\",\n        \"EndpointIdentifier\": \"src-database-1\",\n        \"ReplicationInstanceIdentifier\": \"my-repl-instance\"\n    }\n}\n\n\nFor more information, see Creating source and target endpoints in the AWS Database Migration Service User Guide."
    },
    {
      "command_name": "wait",
      "command_url": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/dms/wait/index.html",
      "command_description": "Description\n\nWait until a particular condition is satisfied. Each subcommand polls an API until the listed requirement is met.\n\nSee ‘aws help’ for descriptions of global parameters.",
      "command_options": []
    }
  ],
  "service_description": "Description\n\nDatabase Migration Service (DMS) can migrate your data to and from the most widely used commercial and open-source databases such as Oracle, PostgreSQL, Microsoft SQL Server, Amazon Redshift, MariaDB, Amazon Aurora, MySQL, and SAP Adaptive Server Enterprise (ASE). The service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle to MySQL or SQL Server to PostgreSQL.\n\nFor more information about DMS, see What Is Database Migration Service? in the Database Migration Service User Guide."
}